[{"categories":["Machine Learning","Presentations"],"contents":"Explore distributed training techniques through this interactive presentation. Navigate through the slides using arrow keys or the navigation controls.\nTopics Covered Back to Basics: Understanding neural network fundamentals Why Distributed Training: Memory constraints and scaling challenges DDP (Data Distributed Parallel): Replicating models across GPUs Pipeline Parallelism: Splitting models across devices FSDP (Fully Sharded Data Parallel): Advanced sharding techniques Slides Use the arrow keys (← →) or click the navigation arrows to move between slides. Some slides include animations that you can step through using the animation controls at the bottom.\nDistributed training presentation Open presentation in new tab Toggle fullscreen for presentation The visualizations and images on MNIST examples in the \u0026ldquo;Back to Basics\u0026rdquo; section are from the educational content at 3Blue1Brown. Key Takeaways Data Distributed Parallel (DDP) Best for: Models that fit in single GPU memory How it works: Full model replica on each GPU Trade-off: High memory usage but simple implementation Pipeline Parallelism Best for: Very deep sequential models How it works: Different layers on different GPUs Trade-off: Requires careful batch sizing to minimize idle time FSDP (Fully Sharded Data Parallel) Best for: Very large models (100B+ parameters) How it works: Shards model parameters, gradients, and optimizer states Trade-off: More complex but enables training of massive models For a detailed written guide on these techniques, check out my Distributed Training blog post.\n","date":"2025-12-03T00:00:00Z","permalink":"https://deepakbaby.in/posts/distributed-training-presentation/","tags":["deep learning","distributed training","DDP","FSDP","pipeline parallelism","PyTorch","presentation"],"title":"Slides: Distributed Training for ML"},{"categories":["Machine Learning"],"contents":"Modern deep learning models have grown exponentially in size and complexity. GPT-4 has over a trillion parameters, and even \u0026ldquo;smaller\u0026rdquo; models like LLaMA-70B require substantial computational resources. Training or fine-tuning such models on a single GPU is often impossible; not just because of time constraints, but because the model itself may not fit in the memory of a single device. This is where distributed training becomes essential.\nWhy Do We Need Distributed Training? The Memory Wall Problem A modern GPU like the NVIDIA A100 has 80GB of memory. Sounds like a lot? Let\u0026rsquo;s do some math:\nA model with 7 billion parameters in FP32 requires: $7B \\times 4 \\text{ bytes} = 28\\text{GB}$ But during training, we also need: Gradients: another 28GB Optimizer states (Adam has 2 momentum terms): 56GB more Activations for backpropagation: varies, but often substantial A 7B parameter model can easily require 150GB+ during training, far exceeding what a single GPU can handle.\nThe Time Constraint Even if a model fits in memory, training on a single GPU can take prohibitively long. Consider:\nTraining GPT-3 on a single V100 GPU would take approximately 355 years With distributed training across thousands of GPUs, this was reduced to weeks Types of Parallelism Distributed training employs different parallelism strategies:\nStrategy What\u0026rsquo;s Parallelized When to Use Data Parallelism Training data across replicas Large datasets, model fits in single GPU Model/Tensor Parallelism Model layers across devices Very large layers (e.g., attention in transformers) Pipeline Parallelism Model stages across devices Deep models with many sequential layers Hybrid (3D Parallelism) Combination of above Extremely large models (100B+ parameters) This post dives deep into three fundamental approaches: DDP, Pipeline Parallelism, and FSDP.\nDistributed Data Parallel (DDP) DDP is the most straightforward and commonly used approach for distributed training. The core idea is simple: replicate the entire model on each GPU, split the data batch across GPUs, and synchronize gradients.\nHow DDP Works Click the Play button to see a visualization of how DDP works.\nDDP visualization Open DDP visualization in new tab Toggle fullscreen for DDP visualization Model Replication: Each GPU gets a complete copy of the model with identical initial weights Data Sharding: The training batch is split equally among all GPUs Forward Pass: Each GPU processes its data shard independently Backward Pass: Each GPU computes gradients for its local data Gradient Synchronization: All GPUs synchronize their gradients using AllReduce Weight Update: Each GPU applies the synchronized gradients to update its local model The magic happens in the AllReduce operation, which efficiently computes the average of gradients across all GPUs and distributes the result back to each GPU.\nAllReduce: The Heart of DDP AllReduce is a collective communication operation that:\nTakes input tensors from all processes Applies a reduction operation (typically sum or average) Distributes the result to all processes Pros of DDP Advantage Description ✅ Simple Implementation Minimal code changes required; wrap model in DistributedDataParallel ✅ Linear Scaling Near-linear speedup with more GPUs for communication-bound scenarios ✅ No Model Changes Works with any model architecture without modifications ✅ Fault Tolerance Easy to checkpoint and resume training ✅ Overlapping Communication Gradient sync overlaps with backward computation Cons of DDP Disadvantage Description ❌ Memory Redundancy Full model replicated on each GPU ❌ Model Size Limit Model must fit entirely in single GPU memory ❌ Communication Overhead AllReduce scales with model size ❌ Synchronization Barrier All GPUs must wait for slowest one (stragglers) When to Use DDP DDP is ideal when:\nYour model fits comfortably in a single GPU You want simple, robust distributed training You\u0026rsquo;re scaling across multiple machines with fast interconnects Pipeline Parallelism When models are too large to fit on a single GPU, we need to partition them across devices. Pipeline Parallelism splits the model into stages, where each stage runs on a different GPU.\nHow Pipeline Parallelism Works Click the Play button.\nPipeline parallelism Open pipeline parallelism visualization in new tab Toggle fullscreen for pipeline parallelism visualization Model Partitioning: Split model into N sequential stages Stage Assignment: Each GPU handles one or more stages Micro-batching: Split input batch into smaller micro-batches Pipeline Execution: Process micro-batches in a pipelined fashion The Bubble Problem Naive pipeline parallelism has a significant issue, pipeline bubbles. Notice the Device Utilization Timeline in the above animation as training progresses. It shows \u0026ldquo;bubbles\u0026rdquo; where GPUs sit idle, waiting for data from previous stages, leading to wasted compute resources.\nReducing Bubbles with Micro-batching The key optimization is to use many micro-batches:\n$$\\text{Bubble Fraction} = \\frac{p - 1}{m}$$\nWhere $p$ is the number of pipeline stages and $m$ is the number of micro-batches. With more micro-batches, the bubble overhead becomes negligible.\nPipeline Schedules Different scheduling strategies minimize bubbles:\nSchedule Description Memory Bubble Ratio GPipe All forward, then all backward High (stores activations) $(p-1)/m$ 1F1B Alternates forward/backward Lower $(p-1)/m$ Interleaved 1F1B Virtual stages Lowest $(p-1)/(m \\cdot v)$ Pros of Pipeline Parallelism Advantage Description ✅ Scales Model Size Train models larger than single GPU memory ✅ Lower Communication Only activations transferred between stages ✅ Works with Sequential Models Natural fit for transformer layers ✅ Memory Efficient Each GPU only holds subset of model Cons of Pipeline Parallelism Disadvantage Description ❌ Pipeline Bubbles Idle time reduces GPU utilization ❌ Complex Implementation Requires careful model partitioning ❌ Load Balancing Stages must have similar compute cost ❌ Increased Latency Forward pass must traverse all stages ❌ Gradient Staleness Some schedules have delayed gradient updates When to Use Pipeline Parallelism Pipeline parallelism shines when:\nModel doesn\u0026rsquo;t fit on a single GPU but isn\u0026rsquo;t excessively large Model has clear sequential structure (e.g., transformer blocks) You have limited inter-GPU bandwidth Combined with data parallelism for better scaling Fully Sharded Data Parallel (FSDP) FSDP represents a paradigm shift in distributed training. Instead of replicating the entire model on each GPU (like DDP), FSDP shards the model parameters, gradients, and optimizer states across all GPUs.\nHow FSDP Works FSDP visualization Open FSDP visualization in new tab Toggle fullscreen for FSDP visualization FSDP follows a gather-compute-scatter pattern:\nSharding: Model parameters are partitioned across all GPUs AllGather: Before forward pass, gather full parameters for current layer Forward Compute: Execute layer with full parameters Discard: After forward, discard non-local parameters to save memory Repeat for backward pass: AllGather → Compute gradients → ReduceScatter ReduceScatter: Distribute and reduce gradients back to shards Memory Savings The memory savings with FSDP are dramatic:\nComponent DDP Memory FSDP Memory Parameters $\\Phi$ per GPU $\\Phi / N$ per GPU Gradients $\\Phi$ per GPU $\\Phi / N$ per GPU Optimizer States $2\\Phi$ per GPU (Adam) $2\\Phi / N$ per GPU Total $4\\Phi$ $4\\Phi / N$ Where $\\Phi$ is model size and $N$ is number of GPUs.\nFor a 7B model on 8 GPUs:\nDDP: 28GB × 4 = 112GB per GPU (doesn\u0026rsquo;t fit on 80GB A100!) FSDP: 112GB / 8 = 14GB per GPU ✓ Sharding Strategies FSDP offers flexible sharding strategies:\nStrategy What\u0026rsquo;s Sharded Memory Communication FULL_SHARD Params, Grads, Optimizer Minimum Maximum SHARD_GRAD_OP Grads, Optimizer Medium Medium NO_SHARD Nothing (like DDP) Maximum Minimum Pros of FSDP Advantage Description ✅ Massive Memory Savings Linear reduction in memory with GPU count ✅ Train Huge Models Enable training of models that don\u0026rsquo;t fit on single GPU ✅ Flexible Sharding Choose tradeoff between memory and communication ✅ Native PyTorch Well-integrated into PyTorch ecosystem ✅ Mixed Precision Works seamlessly with AMP/BF16 ✅ Activation Checkpointing Combines well with gradient checkpointing Cons of FSDP Disadvantage Description ❌ Communication Overhead More collective operations than DDP ❌ Complexity More configuration options to tune ❌ Debugging Difficulty Harder to debug distributed sharded state ❌ Checkpoint Complexity Saving/loading requires special handling ❌ Latency AllGather adds latency before each layer When to Use FSDP FSDP is the right choice when:\nModel doesn\u0026rsquo;t fit on single GPU even with mixed precision You need to train models with billions of parameters You have fast GPU interconnects (NVLink, InfiniBand) Memory is the primary bottleneck Comparison Summary Aspect DDP Pipeline Parallel FSDP Memory per GPU Full model Model / stages Model / GPUs Communication AllReduce Point-to-point AllGather + ReduceScatter Complexity Low Medium Medium-High Model Size Limit Single GPU Total GPU memory Total GPU memory GPU Utilization High Medium (bubbles) High Best For Small-Medium models Sequential models Large models Choosing the Right Strategy graph LR; A[Model] --\u003e B{Fit on single GPU?} B --\u003e |YES| G[Use DDP] B --\u003e |NO| C{Do you have fast interconnect?} C --\u003e |YES| E[Use FSDP] C --\u003e |NO| F[Use Pipeline Parallel] Practical Recommendations Start with DDP if your model fits on a single GPU. DDP is the simplest and most efficient.\nSwitch to FSDP when memory becomes the bottleneck. Start with SHARD_GRAD_OP for less communication overhead.\nAdd Pipeline Parallelism for very deep models, especially when combined with FSDP for each pipeline stage.\nUse 3D Parallelism (Data + Pipeline + Tensor) for even bigger models (100B+ parameters).\nProfile and measure: Use tools like PyTorch Profiler to identify bottlenecks.\nConclusion Distributed training is essential for working with state-of-the-art neural network models. Understanding these three fundamental approaches gives you the tools to train models of any size.\nDDP for simplicity and efficiency with smaller models Pipeline Parallelism for scaling deep sequential models FSDP for massive models that exceed single-GPU memory Further Reading PyTorch DDP Documentation PyTorch FSDP Tutorial GPipe Paper ZeRO: Memory Optimization Toward Training Trillion Parameter Models Megatron-LM: Training Multi-Billion Parameter Language Models ","date":"2025-12-01T00:00:00Z","permalink":"https://deepakbaby.in/posts/distributed-training/","tags":["deep learning","distributed training","DDP","FSDP","pipeline parallelism","PyTorch"],"title":"Distributed Training Techniques: DDP, Pipeline Parallelism, and FSDP"},{"categories":[],"contents":"Keras implementation can be found here.\nFlow-based deep generative models have not gained much attention in the research community when compared to GANs or VAEs. This post discusses a flow-based model called NICE, its advantages over the other generative models and finally an implementation in Keras.\nWhile VAEs use an encoder that finds only an approximation of the latent variable corresponding to a datapoint, GANs doesnt even have an encoder to infer latents. In flow-based models, the latent variables can be infered exactly without any approximation. Flow-based models make use of reversible architecture (which will be explained below) which enables accurate inference, in addition to providing optimization over the exact log-likelihood of the data instead of a lower bound of it.\nFlow-based generative models In flow-based approaches the generative process for a datapoint $ \\mathbf{x} $ is defined as:\n$$\\begin{equation} \\mathbf{z} \\sim p (\\mathbf{z}) \\\\\\ \\mathbf{x} = \\mathbf{g}(\\mathbf{z}) \\end{equation}$$\nwhere $\\mathbf{z}$ is the latent variable with some tractable density $p(\\mathbf{z})$ such as standard normal: $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}; \\mathbf{0}, \\mathbf{I})$. The key aspect in flow-based approaches is that the function $\\mathbf{g}$ is an invertible function (also called bijective) such that for every datapoint $\\mathbf{x}$, the latent variable can be inferred by $\\mathbf{z} = \\mathbf{f}(\\mathbf{x})=\\mathbf{g}^{-1}(\\mathbf{x})$.\nThe function $\\mathbf{f}$ which maps the datapoint to the corresponding latent-variable is realised using a deep neural network. In addition, notice that $\\mathbf{g}$ is the inverse of $\\mathbf{f}$, therefore the neural network architecture should be carefully defined such that it is reversible or invertible. Therefore, we focus on functions where $\\mathbf{f}$ (and $\\mathbf {g}$) is composed of a sequence of invertible transformations: $\\mathbf{f} = \\mathbf{f_1} \\circ \\mathbf{f_2} \\circ \\dots \\circ \\mathbf{f_K}$, such that the mapping from $\\mathbf{x}$ to $\\mathbf{z}$ can be written as: $$\\begin{equation} \\mathbf{x}\\stackrel{\\mathbf{f_1}}{\\longleftrightarrow} \\mathbf{h_1} \\stackrel{\\mathrm{\\mathbf{f_1}}}{\\longleftrightarrow} \\mathbf{h_2} \\dots \\stackrel{\\mathrm{\\mathbf{f_K}}}{\\longleftrightarrow} \\mathbf{z}. \\end{equation}$$ Such a sequence of invertible transformations is also called normalizing flow.\nGiven an observed data variable $\\mathbf{x} \\in \\mathcal{X}$, a simple probability distribution $p(\\mathbf{z})$ with $\\mathbf{z} \\in \\mathcal{Z}$, and a bijective function $\\mathbf{f} : \\mathcal{X} \\rightarrow \\mathcal{Z}$ (with $\\mathbf{g} = \\mathbf{f}^{-1}$), the change of variable formula defines a model distribution on $\\mathcal{X}$ by $$\\begin{align} p(\\mathbf{x}) \u0026amp;= p(\\mathbf{z})\\left\\vert \\det \\left( \\dfrac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} \\right) \\right\\vert \\\\\\ \\log p(\\mathbf{x}) \u0026amp;= \\log p(\\mathbf{z}) + \\log \\left\\vert \\det \\left( \\dfrac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} \\right) \\right\\vert \\\\\\ \u0026amp;= \\log p(\\mathbf{z}) + \\sum_{i=1, j=i-1}^{K} \\log \\left\\vert \\det \\left( \\dfrac{{\\partial \\mathbf{h_i}}}{{\\partial \\mathbf{h_j}}} \\right) \\right\\vert \\end{align}$$ with $\\mathbf{h_0} = \\mathbf{x}$ and $\\mathbf{h_K} = \\mathbf{z}$.\nThe determinant of the Jacobian matrix $\\partial \\mathbf{h_i} / \\partial \\mathbf{h_j}$ is the change in density when $\\mathbf{h_j}$ is transformed to $\\mathbf{h_i}$ under the transformation $\\mathbf{f_i}$. Thus, flow-based models require to compute this determinant as well. The second key aspect is to design the functions $\\mathbf{f_i}$ such that the determinant of its Jacobian is easy to compute.\nThus, flow-based models require two important design choices on $\\mathbf{f_i}$:\nHave reversible architecture Design transformations whose determinant of Jacobians are easy to compute To satisfy these two requirements, the trick is to choose the transformations whose Jacobian is a triangular matrix, such that their determinnant can be simply computed as the product of its diagonal elements. Thus, $$\\begin{equation} \\log \\left\\vert \\det \\left( \\dfrac{{\\partial \\mathbf{h_i}}}{{\\partial \\mathbf{h_j}}} \\right) \\right\\vert = \\sum \\log \\left\\vert \\text{diag}~ \\left( \\dfrac{{\\partial \\mathbf{h_i}}}{{\\partial \\mathbf{h_j}}} \\right) \\right\\vert \\end{equation}$$ where, $\\text{diag}(\\cdot)$ takes the diagonal of the Jacobian matrix.\nThese models are trained (i.e., training the neural nets $\\mathbf{f}$) such that the negative log-likelihood of $\\mathbf{z}$ is minimized with respect to some prior distribution (more on this below).\nTo generate data, we can sample from the prior distribution $p(\\mathbf{z})$ and do the inverse operation.\nNICE: Non-linear Independent Components Estimation As mentioned before, the two key main aspects of flow-based approaches is easy determinant of the Jacobian and easy inverse. In NICE, the input data is split into two blocks $\\mathbf{x} \\rightarrow (\\mathbf{x_1}, \\mathbf{x_2})$ (say, even and odd indices). Then apply block transformation from $(\\mathbf{x_1}, \\mathbf{x_2})$ to $(\\mathbf{h_1^1}, \\mathbf{h_1^2})$ of the form: $$\\begin{align} \\mathbf{h_1^1} \u0026amp;= \\mathbf{x_1} \\\\\\ \\mathbf{h_1^2} \u0026amp;= \\mathbf{x_2} + \\mathbf{m}(\\mathbf{x_1}) \\end{align}$$ where, $\\mathbf{m}$ is an arbitrarily complex function (neural net). They call this transformation as an Affine Coupling layer. This transformation satisfies the two design choices:\nThe inverse can be easily computed as: $$\\begin{align} \\mathbf{x_1} \u0026amp;= \\mathbf{h_1^1} \\\\\\ \\mathbf{x_2} \u0026amp;= \\mathbf{h_1^2} - \\mathbf{m}(\\mathbf{h_1^1}) \\end{align}$$\nThe transformation $\\mathbf{f}$ is (where $d$ and $e$ are the dimensions of $ \\mathbf{x_1}$ and $\\mathbf{x_2}$) $$\\begin{align} \\mathbf{h_1} = \\begin{bmatrix} \\mathbf{h_1^1} \\\\\\ \\mathbf{h_1^{2}} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{I_d} \u0026amp; 0 \\\\\\ \\mathbf{m}(\\cdot) \u0026amp; \\mathbf{I_e} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x_1} \\\\\\ \\mathbf{x_2} \\end{bmatrix} \\end{align}$$ resulting in a Jacobian matrix $$\\begin{align} \\dfrac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}} \u0026amp;= \\begin{bmatrix} \\dfrac{\\partial \\mathbf{h_1^1}}{\\partial \\mathbf{x_1}} \u0026amp; \\dfrac{\\partial \\mathbf{h_1^1}}{\\partial \\mathbf{x_2}} \\\\\\ \\dfrac{\\partial \\mathbf{h_1^2}}{\\partial \\mathbf{x_1}} \u0026amp; \\dfrac{\\partial \\mathbf{h_1^2}}{\\partial \\mathbf{x_2}} \\end{bmatrix} \\\\\\ \u0026amp;= \\begin{bmatrix} \\mathbf{I_d} \u0026amp; 0 \\\\\\ \\dfrac{\\partial \\mathbf{m}(\\cdot)}{\\partial \\mathbf{x_1}} \u0026amp; \\mathbf{I_e} \\end{bmatrix} \\end{align}$$ whose determinant is unity. Notice that such a design not only enables easy compuatation of the determinant, but also lets us choose arbitrarily complex $\\mathbf{m}(\\cdot)$ since we dont have to compute its derivative to obtain the determinant.\nSimilarly, inverse operation from $\\mathbf{z}$ to $\\mathbf{x}$ also results in a unit Jacobian determinant. Thus generating data also is easy with the NICE model.\nIn the NICE model, since all the transformations are volume preserving (unit Jacobian determinant), the resulting transformation will have equal weight over all dimensions, which is not desirable in practical applications. To address this, NICE also includes a scaling layer at the output that scales every dimension by a trainable weight $S_i$. This allows the model to give more weight on some dimensions and less on others.\nThus the nice criterion becomes maximizing the log-likelihood of the data distribution: $$\\begin{align} \\log p(\\mathbf{x}) \u0026amp;= \\log p(\\mathbf{z}) + \\log \\left\\vert \\det \\left( \\dfrac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} \\right) \\right\\vert \\\\\\ \u0026amp;= \\log p(\\mathbf{z}) +\\sum_{i=1}^D \\log (\\vert S_i \\vert) \\end{align}$$\nFurther, NICE model assumes that the prior distribution is factorial: $p(\\mathbf{z}) = \\prod_{i=1}^{D} p(\\mathbf{z_i})$. The training criterion for NICE is to maximize its log-likelihood or minimize the negative log-likelihood: $\\mathcal{L} = - \\log p(\\mathbf{x})$\nFor standard gaussian: $\\mathcal{L} = \\sum_{i=1}^D 0.5 \\cdot (\\mathbf{z_i}^2 + \\log 2\\pi) - \\log (\\vert S_i \\vert)$ For standard logistic: $\\mathcal{L} =\\sum_{i=1}^D \\log\\left(1+\\exp(\\mathbf{z_i})\\right) + \\log(1+\\exp(\\mathbf{-z_i})) - \\log (\\vert S_i \\vert) $ As we stack multiple affine coupling layers to obtain more complex transformations. Since the transformation leaves one part of the data leaves unchanged, we can alternate the role of each part in subsequent coupling layers. Typically, 4 coupling layers are used so that all dimensions influence the one another. The scaling layer is paramterised exponentially $\\exp(S_i)$ to have positive scaling.\nIn NICE the forward model maps the datapoint to the latent space and is trained to minimize the negative log-likelihood with respect to some prior distribution. And for inference, we sample from the prior distribution to get $\\mathbf{h}$ and new datapoints $\\mathbf{x}$ can be generated using the inverse flow. For a 4 layer architecture, the forward and inverse flow equations are,\nForward Flow Inverse Flow $$\\begin{align}\\mathbf{h_1^1} \u0026amp;= \\mathbf{x_1} \\\\\\ \\mathbf{h_1^2} \u0026amp;= \\mathbf{x_2} + \\mathbf{m_1} ( \\mathbf{x_1}) \\end{align}$$ $$\\begin{align} \\mathbf{h_4} = \\exp(-S) \\odot \\mathbf{h}\\end{align}$$ $$\\begin{align}\\mathbf{h_2^2} \u0026amp;= \\mathbf{h_1^2} \\\\\\ \\mathbf{h_2^1} \u0026amp;= \\mathbf{h_1^1} + \\mathbf{m_2} ( \\mathbf{h_1^2}) \\end{align}$$ $$\\begin{align} \\mathbf{h_3^2} \u0026amp;= \\mathbf{h_4^2} \\\\\\ \\mathbf{h_3^1} \u0026amp;= \\mathbf{h_4^1} - \\mathbf{m_4} ( \\mathbf{h_4^2}) \\end{align}$$ $$\\begin{align}\\mathbf{h_3^1} \u0026amp;= \\mathbf{h_2^1} \\\\\\ \\mathbf{h_3^2} \u0026amp;= \\mathbf{h_2^2} + \\mathbf{m_3} ( \\mathbf{h_2^1}) \\end{align}$$ $$\\begin{align} \\mathbf{h_2^1} \u0026amp;= \\mathbf{h_3^1} \\\\\\ \\mathbf{h_2^2} \u0026amp;= \\mathbf{h_3^2} - \\mathbf{m_3} ( \\mathbf{h_3^1}) \\end{align}$$ $$\\begin{align}\\mathbf{h_4^2} \u0026amp;= \\mathbf{h_3^2} \\\\\\ \\mathbf{h_4^1} \u0026amp;= \\mathbf{h_3^1} + \\mathbf{m_4} ( \\mathbf{h_3^2}) \\end{align}$$ $$\\begin{align} \\mathbf{h_1^2} \u0026amp;= \\mathbf{h_2^2} \\\\\\ \\mathbf{h_1^1} \u0026amp;= \\mathbf{h_2^1} - \\mathbf{m_2} ( \\mathbf{h_2^2}) \\end{align}$$ $$\\begin{align} \\mathbf{h} = \\exp(S) \\odot \\mathbf{h_4}\\end{align}$$ $$\\begin{align} \\mathbf{x_1} \u0026amp;= \\mathbf{h_1^1} \\\\\\ \\mathbf{x_2} \u0026amp;= \\mathbf{h_1^2} - \\mathbf{m_1} ( \\mathbf{h_1^1}) \\end{align}$$ Implementation Notes As mentioned above, the NICE model is trained to minimize the negative log-likelihood with respect to a standard Gaussian or logistic distribution.\nGetting the models correct: The main design challenge was to get the inverse model correct with its weights tied to the corresponding forward model. We can verify if the inverse model is correct even before training the model. Just take a test image from MNIST and pass it through the forward model. Then use the resulting output as input to the inverse model. If the inverse model is created with correct weights, it should yield the test image.\nGetting NaNs: For implementing the logistic loss $\\log\\left(1+\\exp(\\mathbf{z_i})\\right) + \\log(1+\\exp(\\mathbf{-z_i}))$, I initially used Keras backend as:\nimport keras.backend as K logistic_negloglikelihood = K.sum ( K.log (1 + K.exp(z)) + K.log(1 + K.exp(-z)), axis=1 ) However, it was resulting in NaN after a few epochs. Then this was replaced with softplus function which computes $\\log (1 + \\exp(x))$ and the NaN issue was (magically!) gone.\nlogistic_negloglikelihood = K.sum ( K.softplus(z) + K.softplus(-z), axis=1 ) Initialization: Initially, I was using the default glorot_uniform initialization of keras which was not resulting in the log-likelihoods given in the paper. Dinh\u0026rsquo;s original implementation in Theano used an initialization from a uniform distribution in $[-0.01,0.01]$. Using the same initialization for the Dense layers resulted in better log-likelihoods.\nBatch-size: Dinh used Adam optimizer with a learning rate $0.001$ and batch size of 256. In my experiments, I observed that increasing the batch size from $256$ to $2048$ consistently yielded better log-likelihoods.\nClip the outputs during generating data: MNIST data is rescaled to the range $[0, 1]$ for training. But during inference, the output may not be always in $[0, 1]$. Therefore, we apply clipping on the generated outputs to bring them to the desired range.\n","date":"2019-12-03T12:55:51+01:00","permalink":"https://deepakbaby.in/posts/nice-keras/","tags":["deep learning","keras","generative models","flow-based models","nice","normalizing flows"],"title":"NICE- Non-linear Independent Components Estimation: Insights and Implementation in Keras"},{"categories":[],"contents":"This post is a summary of some of the main hurdles I encountered in implementing a VAE on a custom dataset and the tricks I used to solve them. The keras code snippets are also provided. Understanding VAEs and its basic implementation in Keras can be found in the previous post.\nPosterior collapse in VAEs The Goal of VAE is to train a generative model $\\mathbb{P}(\\mathbf{X}, z)$ to maximize the marginal likelihood $\\mathbb{\\mathbf{X}}$ of the dataset. The cost function used in training a VAE is comprised of a reconstruction loss and a KL loss as given below.\n$$\\begin{equation} \\mathcal{L} = -\\mathbb{E}\\left[ \\log\\mathbb{P}(\\mathbf{X} \\vert z) \\right] + D\\left( \\mathbb{Q}(z \\vert \\mathbf{X}) \\mathrel{\\Vert} \\mathbb{P}(z) \\right) \\end{equation}$$\nThe main implementation issue in this case is that the two losses are kind of opposing each other. The problem of mode collapse is that the second loss term $D\\left( \\mathbb{Q}(z \\vert \\mathbf{X}) \\mathrel{\\Vert} \\mathbb{P}(z) \\right)$ reduces to $0$. i.e., the approximate posterior $\\mathbb{Q}(z \\vert \\mathbf{X})$ becomes equal to the prior $\\mathbb{P}(z)$. Thus the latent variable do not carry any information about the input $\\mathbf{X}$.\nIn addition, there is always a mismatch between the dimensions of the data and the latent space. If our data is $N$ dimensional and the latent space has a dimension of $D$, the first cost term involves summation over $N$ values and the KL loss is a summation over $D$ values. This scaling difference introduces additional weightage on one loss term over the other and it converges faster than the other.\nIf the reconstruction loss converges faster, it leads to the latent space not learning any meaningful representations. On the other hand, if the KL loss converges faster, it leads decoder generating meaniningless outputs. So there is always this problem of balancing these two losses. After some reading, I came across the following three approaches to mitigate this cost balancing problem. The solutions are given in Keras terms.\nimport keras.backend as K Using K.sum instead of K.mean Many standard implementations (for example, Keras VAE tutorial) either use K.sum instead of K.mean or if you are using a standard loss term such as mse scale it by $N$ which is the data dimension.\nimport keras.backend as K from keras.losses import mse N=x_train.shape[1] # dimension of the data def vae_loss(y_true, y_pred): # mse loss reconstruction_loss = mse(y_true, y_pred) reconstruction_loss *= N # kl loss kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 return K.mean(reconstruction_loss + kl_loss) OR\nimport keras.backend as K def vae_loss(y_true, y_pred): # mse loss reconstruction_loss = K.sum(K.square(y_true - y_pred), axis=-1) # kl loss kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 return K.mean(reconstruction_loss + kl_loss) However, this does not always solve the problem. There will be scaling mismatches introduced by $N$ and $D$, and what happened in my experiment was the KL loss converged faster.\nKL annealing This is a more popular approach and it worked in my case too. This approach is to first train the VAE using the reconstruction loss only for a few epochs and then slowly introduce the KL loss term. This approach works better due to the fact that the KL cost is initially very large and the optimizer will focus on the KL loss only which leads to a local minimum.\nSo what I did was to train the VAE with reconstruction loss only by scaling the KL loss by 0 for a few epochs and then gradually increase the scaling on KL loss from 0 to 1 over the next few epochs and let it train using the actual VAE loss for the remaining epochs. I used a callback for updating the weight on KL loss. Assuming the VAE model in keras is compiled as vae.\nIn the following snippet the VAE is trained only on reconstruction loss for the first 40 epochs and then the KL loss scale is increased from 0 to 1 linearly over the next 20 epochs.\nimport keras.backend as K from keras.callbacks import Callback # total number of epochs n_epochs = 500 # The number of epochs at which KL loss should be included klstart = 40 # number of epochs over which KL scaling is increased from 0 to 1 kl_annealtime = 20 class AnnealingCallback(Callback): def __init__(self, weight): self.weight = weight def on_epoch_end (self, epoch, logs={}): if epoch \u0026gt; klstart : new_weight = min(K.get_value(self.weight) + (1./ annealtime), 1.) K.set_value(self.weight, new_weight) print (\u0026#34;Current KL Weight is \u0026#34; + str(K.get_value(self.weight))) # the starting value of weight is 0 # define it as a keras backend variable weight = K.variable(0.) # wrap the loss as a function of weight def vae_loss(weight): def loss (y_true, y_pred): # mse loss reconstruction_loss = K.sum(K.square(y_true - y_pred), axis=-1) # kl loss kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 return reconstruction_loss + (weight * kl_loss) return loss # compile vae with the weighted vae loss vae.compile(optimizer=\u0026#39;adam\u0026#39;, loss=vae_loss(weight)) # train VAE with annealing callback vae.fit(X_train, X_train, epochs=n_epochs, callbacks=[AnnealingCallback(weight)]) This will train the VAE with the new weight scheduling on the KL loss. This lets the network to first learn to reconstruct the data and gradually learn how the latent space is distributed.\n","date":"2019-07-03T15:53:58+02:00","permalink":"https://deepakbaby.in/posts/vae-insights/","tags":[],"title":"Implementing Variational Autoencoders: Insights and some tricks"},{"categories":[],"contents":"Variational Autoencoders (VAEs)[Kingma, et.al (2013)] let us design complex generative models of data that can be trained on large datasets. This post is about understanding the VAE concepts, its loss functions and how we can implement it in keras.\nGenerating data from a latent space VAEs, in terms of probabilistic terms, assume that the data-points in a large dataset are generated from a latent space. For e.g., let us assume we want to generate the image of an animal. First we imagine that it has four legs, a head and a tail. This is analogous to the latent space and from this set of characteristics that are defined in the latent space, the model will learn to generate the image of an animal.\nBefore we dive into the math and intuitions, let us define some notations:\n$\\mathbf{X}$: The type of data we want to generate (say, a large dataset containing images of animals) $z$: The latent variable, the set of characteristics we want in the image $\\mathbb{P}(\\mathbf{X})$: probability distribution of the data $\\mathbb{P}(z)$: probability distribution of the latent space $\\mathbb{P}(\\mathbf{X} \\vert z)$: probability distribution of generating data from the latent variable We assume that every data-point $x$ is a random sample from the unknown underlying process whose true distribution $\\mathbb{P}(\\mathbf{X})$ is unknown. VAEs make use of a specific probability model that captures the joint probability between the data $\\mathbf{X}$ and latent variables $z$. This joint probability can be written as $\\mathbb{P}(\\mathbf{X}, z) = \\mathbb{P}(\\mathbf{X} \\vert z) \\cdot \\mathbf{P}(z)$. The generative model assumed in VAE can be described as:\nDraw one latent variable $ z_{i} \\sim \\mathbb{P}(z) $: similar to defining a set of characteristics that defines an animal Generate the data-point such that $x \\sim \\mathbb{P}(\\mathbf{X} \\vert z) $: similar to generating the image of an animal that satisfies the characteristics specified in the latent variable VAE formulation and cost function From the probability model perspective, the latent variables are drawn from a prior $\\mathbb{P}(z)$ and the generated data $x$ has a likelihood of $\\mathbb{P}(\\mathbf{X} \\vert z)$ that is conditioned on the latent variables $z$. The objective here is to model the data distribution $\\mathbb{P}(\\mathbf{X})$ by marginalizing out the latent variable $z$ from the joint-distribution $\\mathbb{P}(\\mathbf{X}, z)$.\n$$\\begin{equation} \\mathbb{P}(\\mathbf{X}) = \\int_{z} \\mathbb{P}(\\mathbf{X} \\vert z) \\mathbb{P}(z) ~ dz \\end{equation}$$\nHowever, this integral is very difficult to compute as it requires to be computed over all possibilities of the latent variable $z$. In order to overcome this, VAEs first try to infer the distribution $\\mathbb{P}(z)$ from the data using $\\mathbb{P}(z \\vert \\mathbf{X})$. i.e., rather looking at all possibilities of $z$, we want to infer the distribution of $z$ that describes our data reasonably well. For example, if we want to generate an animal, we only need to specify the characteristics that describe an animal. We do not need to include things like glass, table, \u0026hellip; as it is unlikely that those characteristics contribute to generating the image of an animal. Thus, this inference phase $\\mathbb{P}(z \\vert \\mathbf{X})$ limits our imagination space to focus on characteristics that are required for generating the image of animal.\nAgain, we do not know the best set of characteristics $\\mathbb{P}(z \\vert \\mathbf{X})$ yet. VAEs make use of variational inference to infer $\\mathbb{P}(z \\vert \\mathbf{X})$. Variational inference approximate the true distribuation $\\mathbb{P}(z \\vert \\mathbf{X})$ using a simpler distribution that is easy to evaluate. A popular choice is Gaussian distribution.\nFurther, a parametric inference model $\\mathbb{Q}(z \\vert \\mathbf{X})$ that maps the data to the underlying latent space and the difference between $\\mathbb{P}(z \\vert \\mathbf{X})$ and $\\mathbb{Q}(z \\vert \\mathbf{X})$ is quantified using Kullback-Leibler divergence between them.\n$$\\begin{align} D_{KL} \\left( \\mathbb{Q}(z \\vert \\mathbf{X}) \\mathrel{\\Vert} \\mathbb{P}(z \\vert \\mathbf{X}) \\right) \u0026amp;= \\sum \\mathbb{Q}(z \\vert \\mathbf{X}) \\log \\dfrac{ \\mathbb{Q}(z \\vert \\mathbf{X})}{ \\mathbb{P}(z \\vert \\mathbf{X})} \\\\ \u0026amp;=\\mathbb{E}\\left[ \\log \\dfrac{ \\mathbb{Q}(z \\vert \\mathbf{X})}{ \\mathbb{P}(z \\vert \\mathbf{X})} \\right] \\\\ \u0026amp;= \\mathbb{E}\\left[ \\log\\mathbb{Q}(z \\vert \\mathbf{X}) - \\log \\mathbb{P}(z \\vert \\mathbf{X}) \\right] \\end{align}$$\nwhere, $\\mathbb{E}$ is the expectation with respect to $\\mathbb{Q}(z \\vert \\mathbf{X})$. Using $\\mathbb{P}(z \\vert \\mathbf{X}) = \\dfrac{\\mathbb{P}(\\mathbf{X} \\vert z) \\mathbb{P}(z)}{\\mathbb{P}(\\mathbf{X})}$ we can rewrite the above expression as:\n$$\\begin{align} D_{KL} \\left( \\mathbb{Q}(z \\vert \\mathbf{X}) \\mathrel{\\Vert} \\mathbb{P}(z \\vert \\mathbf{X}) \\right) \u0026amp;= \\mathbb{E}\\left[ \\log\\mathbb{Q}(z \\vert \\mathbf{X}) - \\log \\dfrac{\\mathbb{P}(\\mathbf{X} \\vert z) \\mathbb{P}(z)}{\\mathbb{P}(\\mathbf{X})} \\right] \\\\ \u0026amp;= \\mathbb{E}\\left[ \\log\\mathbb{Q}(z \\vert \\mathbf{X}) - \\log\\mathbb{P}(\\mathbf{X} \\vert z) - \\log \\mathbb{P}(z) + \\log \\mathbb{P}(\\mathbf{X}) \\right] \\end{align}$$\nNotice that $\\mathbb{P}(\\mathbf{X})$ does not depend on $z$ and hence it can be taken outside the expectation operation over $z$. We will denote $D_{KL}$ as $D$.\n$$\\begin{align} D \\left( \\mathbb{Q}(z \\vert \\mathbf{X}) \\mathrel{\\Vert} \\mathbb{P}(z \\vert \\mathbf{X}) \\right) \u0026amp;= \\mathbb{E}\\left[ \\log\\mathbb{Q}(z \\vert \\mathbf{X}) - \\log\\mathbb{P}(\\mathbf{X} \\vert z) - \\log \\mathbb{P}(z) \\right] + \\log \\mathbb{P}(\\mathbf{X}) \\\\ \\implies \\quad \\log \\mathbb{P}(\\mathbf{X}) - D \\left( \\mathbb{Q}(z \\vert \\mathbf{X}) \\mathrel{\\Vert} \\mathbb{P}(z \\vert \\mathbf{X}) \\right) \u0026amp;= \\mathbb{E}\\left[ \\log\\mathbb{P}(\\mathbf{X} \\vert z) \\right] - \\mathbb{E}\\left[ \\log\\mathbb{Q}(z \\vert \\mathbf{X}) - \\mathbb{P}(z) \\right] \\\\ \u0026amp;= \\mathbb{E}\\left[ \\log\\mathbb{P}(\\mathbf{X} \\vert z) \\right] - D\\left( \\mathbb{Q}(z \\vert \\mathbf{X}) \\mathrel{\\Vert} \\mathbb{P}(z) \\right) \\end{align}$$\nThe right-hand side of the above equation is the objective function used by VAEs. What it says is that, we are trying to model our data which is described by $\\log \\mathbb{P}(\\mathbf{X})$ with some error $D \\left( \\mathbb{Q}(z \\vert \\mathbf{X}) \\mathrel{\\Vert} \\mathbb{P}(z \\vert \\mathbf{X}) \\right)$. Since $D_{KL}$ is always positive, we can write the above equation as:\n$$\\begin{equation} \\log \\mathbb{P}(\\mathbf{X}) \\geq \\mathbb{E}\\left[ \\log\\mathbb{P}(\\mathbf{X} \\vert z) \\right] - D\\left( \\mathbb{Q}(z \\vert \\mathbf{X}) \\mathrel{\\Vert} \\mathbb{P}(z) \\right) \\end{equation}$$\nThus, the right-hand side (RHS) of the above inequality is the lower bound for $\\log \\mathbb{P}(\\mathbf{X})$ which we are trying to maximize. This is known as the evidence lower bound (ELBO). Maximizing the RHS is also the same as minimizing its negative. The negative of the RHS is therefore used as a cost function to be minimized while training VAEs.\nAt this point, what we have is:\n$\\mathbb{P}(\\mathbf{X} \\vert z)$: Generating data from the given latent variable (the decoder) $\\mathbb{Q}(z \\vert \\mathbf{X})$: Infering the latent code given the data (the encoder) $D\\left(\\mathbb{Q}(z \\vert \\mathbf{X}) \\mathrel{\\Vert} \\mathbb{P}(z) \\right)$: Making sure that the encoded representation resembles a simpler, tractable distribuation (e.g., Gaussian). Thus a VAE first encodes the data into some latent space (mapping $x$ to $z$) and then generates (decodes: mapping $z$ to $x$) data based on samples from that latent space, and hence called variational autoencoder.\nVAE cost function and neural networks The VAE cost function can be seen as adding an additional cost term on the traditional autoencoders. The first term is the reconstruction loss at the output, which is the same as used in an autoencoder. The second term forces the encoder to map the input data to a pre-defined tractable distribution.\nWhy do we need $\\mathbb{P}(z)$ to be a simple distribution? Since VAE is a generative model, we would like to generate new data-points by sampling $\\mathbb{P}(z)$. The easiest choice for this is a standard normal distribution $\\mathcal{N}(0,1)$.\nThe mappings $\\mathbb{P}(\\mathbf{X} \\vert z)$ and $\\mathbb{Q}(z \\vert \\mathbf{X})$ are realized using deep neural networks (DNNs). Thus VAEs are designed using two DNNs: an encoder and a decoder. The cost function is to minimize the negative of the ELBO obtained above.\nImplementing VAE cost in keras As detailed before, the first term of the cost function is the reconstruction loss. We can use any popular loss, say mean-squared error, for this purpose. Computing the KL divergence cost term requires assuming $\\mathbb{Q}(z \\vert \\mathbf{X})$ to be also Gaussian with parameters $\\mu (\\mathbf{X})$ and $\\Sigma (\\mathbf{X})$. This assumption enables us to compute the KL divergence between $\\mathbb{Q}(z \\vert \\mathbf{X}) = \\mathcal{N}(\\mu (\\mathbf{X}), \\Sigma (\\mathbf{X}))$ and $\\mathbb{P}(z) = \\mathcal{N}(0,1)$ in closed form as:\n$$\\begin{align} D\\left[ \\mathcal{N}(\\mu (\\mathbf{X}), \\Sigma (\\mathbf{X}))~\\Vert ~ \\mathcal{N}(0,1) \\right] = \\dfrac{1}{2} \\left[ tr\\left( \\Sigma (\\mathbf{X}) \\right) + \\mu (\\mathbf{X})^{T} \\mu (\\mathbf{X}) - k - \\log det\\left( \\Sigma (\\mathbf{X}) \\right) \\right] \\end{align}$$ where, $tr$ and $det$ are the trace and determinant of the covariance matrix $\\Sigma (\\mathbf{X})$ and $k$ is the dimension of the Gaussian distribution. For details on the calculation of the above divergence, refer to this page. We also assume that the covariance matrix is diagonal, we can compute the determinant by simpy multiplying its diagonal elements. In addition, we can also implement $\\Sigma (\\mathbf{X})$ as a vector since it is a diagonal matrix.\n$$\\begin{align} D\\left[ \\mathcal{N}(\\mu (\\mathbf{X}), \\Sigma (\\mathbf{X}))~\\Vert ~ \\mathcal{N}(0,1) \\right] \u0026amp;= \\dfrac{1}{2} \\left[ \\sum \\Sigma (\\mathbf{X}) + \\sum \\mu^{2} (\\mathbf{X}) - \\sum 1 - \\log \\prod \\Sigma (\\mathbf{X}) \\right] \\\\\\ \u0026amp;= \\dfrac{1}{2} \\left[ \\sum \\Sigma (\\mathbf{X}) + \\sum \\mu^{2} (\\mathbf{X}) - \\sum 1 - \\sum \\log \\Sigma (\\mathbf{X}) \\right] \\\\\\ \u0026amp;= \\dfrac{1}{2} \\sum \\left[ \\Sigma (\\mathbf{X}) + \\mu^{2} (\\mathbf{X}) - 1 - \\log \\Sigma (\\mathbf{X}) \\right] \\end{align}$$\nIn addition, typically we model the logarithm of $\\Sigma (\\mathbf{X})$ for numerical stability. Thus the final loss term becomes:\n$$\\begin{equation} D\\left[ \\mathcal{N}(\\mu (\\mathbf{X}), \\Sigma (\\mathbf{X}))~\\Vert ~ \\mathcal{N}(0,1) \\right] = \\dfrac{1}{2} \\sum \\left[ \\exp(\\Sigma (\\mathbf{X})) + \\mu^{2} (\\mathbf{X}) - 1 - \\Sigma (\\mathbf{X}) \\right] \\end{equation}$$\nKeras implementation This is mostly a copy of the example provided in Keras VAE example, but with some edits and added comments. This post does not discuss the reparameterization trick involved in training a VAE as it is discussed in many other pages.\nEven though the example below works really well, in practice, we will need to somehow adjust the reconstruction loss and the KL loss. The insights I gained and the tricks I used to overcome the issues will be described in the upcoming post.\nImplementing Variational Autoencoders: Some insights and tricks\nfrom keras.layers import Lambda, Input, Dense from keras.models import Model from keras.datasets import mnist from keras.losses import mse, binary_crossentropy from keras.utils import plot_model from keras import backend as K import numpy as np import matplotlib.pyplot as plt import os # reparameterization trick # instead of sampling from Q(z|X), sample eps = N(0,I) # z = z_mean + sqrt(var)*eps def sampling(args): z_mean, z_log_var = args batch = K.shape(z_mean)[0] dim = K.int_shape(z_mean)[1] # by default, random_normal has mean=0 and std=1.0 epsilon = K.random_normal(shape=(batch, dim)) return z_mean + K.exp(0.5 * z_log_var) * epsilon def plot_results(models, data, batch_size=128, model_name=\u0026#34;vae_mnist\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plots labels and MNIST digits as function of 2-dim latent vector # Arguments models (tuple): encoder and decoder models data (tuple): test data and label batch_size (int): prediction batch size model_name (string): which model is using this function \u0026#34;\u0026#34;\u0026#34; encoder, decoder = models x_test, y_test = data os.makedirs(model_name, exist_ok=True) filename = os.path.join(model_name, \u0026#34;vae_mean.png\u0026#34;) # display a 2D plot of the digit classes in the latent space z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(12, 10)) plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test) plt.colorbar() plt.xlabel(\u0026#34;z[0]\u0026#34;) plt.ylabel(\u0026#34;z[1]\u0026#34;) plt.savefig(filename) plt.show() filename = os.path.join(model_name, \u0026#34;digits_over_latent.png\u0026#34;) # display a 30x30 2D manifold of digits n = 30 digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = np.linspace(-4, 4, n) grid_y = np.linspace(-4, 4, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) start_range = digit_size // 2 end_range = n * digit_size + start_range + 1 pixel_range = np.arange(start_range, end_range, digit_size) sample_range_x = np.round(grid_x, 1) sample_range_y = np.round(grid_y, 1) plt.xticks(pixel_range, sample_range_x) plt.yticks(pixel_range, sample_range_y) plt.xlabel(\u0026#34;z[0]\u0026#34;) plt.ylabel(\u0026#34;z[1]\u0026#34;) plt.imshow(figure, cmap=\u0026#39;Greys_r\u0026#39;) plt.savefig(filename) plt.show() # MNIST dataset (x_train, y_train), (x_test, y_test) = mnist.load_data() image_size = x_train.shape[1] original_dim = image_size * image_size x_train = np.reshape(x_train, [-1, original_dim]) x_test = np.reshape(x_test, [-1, original_dim]) x_train = x_train.astype(\u0026#39;float32\u0026#39;) / 255 x_test = x_test.astype(\u0026#39;float32\u0026#39;) / 255 # network parameters input_shape = (original_dim, ) intermediate_dim = 512 batch_size = 128 latent_dim = 2 epochs = 50 # VAE model = encoder + decoder # build encoder model inputs = Input(shape=input_shape, name=\u0026#39;encoder_input\u0026#39;) x = Dense(intermediate_dim, activation=\u0026#39;relu\u0026#39;)(inputs) z_mean = Dense(latent_dim, name=\u0026#39;z_mean\u0026#39;)(x) z_log_var = Dense(latent_dim, name=\u0026#39;z_log_var\u0026#39;)(x) # use reparameterization trick to push the sampling out as input # note that \u0026#34;output_shape\u0026#34; isn\u0026#39;t necessary with the TensorFlow backend z = Lambda(sampling, output_shape=(latent_dim,), name=\u0026#39;z\u0026#39;)([z_mean, z_log_var]) # instantiate encoder model encoder = Model(inputs, [z_mean, z_log_var, z], name=\u0026#39;encoder\u0026#39;) encoder.summary() plot_model(encoder, to_file=\u0026#39;vae_mlp_encoder.png\u0026#39;, show_shapes=True) # build decoder model latent_inputs = Input(shape=(latent_dim,), name=\u0026#39;z_sampling\u0026#39;) x = Dense(intermediate_dim, activation=\u0026#39;relu\u0026#39;)(latent_inputs) outputs = Dense(original_dim, activation=\u0026#39;sigmoid\u0026#39;)(x) # instantiate decoder model decoder = Model(latent_inputs, outputs, name=\u0026#39;decoder\u0026#39;) decoder.summary() plot_model(decoder, to_file=\u0026#39;vae_mlp_decoder.png\u0026#39;, show_shapes=True) # instantiate VAE model outputs = decoder(encoder(inputs)[2]) vae = Model(inputs, outputs, name=\u0026#39;vae_mlp\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: models = (encoder, decoder) data = (x_test, y_test) def vae_loss(y_true, y_pred): reconstruction_loss = mse(y_true, y_pred) reconstruction_loss *= original_dim z_mean = vae.get_layer(\u0026#39;encoder\u0026#39;).get_layer(\u0026#39;z_mean\u0026#39;).output z_log_var = vae.get_layer(\u0026#39;encoder\u0026#39;).get_layer(\u0026#39;z_log_var\u0026#39;).output kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 return K.mean(reconstruction_loss + kl_loss) vae.compile(optimizer=\u0026#39;adam\u0026#39;, loss=vae_loss) vae.summary() plot_model(vae, to_file=\u0026#39;vae_mlp.png\u0026#39;, show_shapes=True) # train the autoencoder vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) vae.save_weights(\u0026#39;vae_mlp_mnist.h5\u0026#39;) plot_results(models, data, batch_size=batch_size, model_name=\u0026#34;vae_mlp\u0026#34;) ","date":"2019-07-02T16:44:25+02:00","permalink":"https://deepakbaby.in/posts/vae-keras/","tags":["deep learning","vae","variational autoencoder","keras"],"title":"Understanding Variational Autoencoders and Implementation in Keras"},{"categories":[],"contents":"Kaldi has recently switched to Intel Math Kernel Libraries (MKL) for linear algebra operations (as of April 2019). However, installing MKL (by running tools/extras/install_mkl.sh) requires root access. This post details how kaldi (with MKL) can be installed without root access.\nDownload Kaldi Download the MKL standalone installer from here. Extract the contents and launch the installer by running install.sh. When asked for the path to install, specify a location where you have write access (e.g., /home/\u0026lt;username\u0026gt;/intel) Complete the installation of MKL libraries Navigate to the kaldi folder kaldi/tools Typically the first step is to run extras/check_dependencies.sh. This will complain about the missing MKL libraries. This is because the script expects the MKL libraries to be located under /opt/intel directory. As of now (May 2019), there is no option to pass the mkl-root directory to this script. Therefore we will edit the extras/check_dependencies.sh script by changing /opt/intel/mkl/include/mkl.h to /home/\u0026lt;username\u0026gt;/intel/mkl/include/mkl.h. Then running extras/check_dependencies.sh should work fine without any MKL related warnings. Then run make -j \u0026lt;numcpu\u0026gt; to install the tools required by kaldi Navigate to the kaldi/src folder. Run ./configure with the --mkl-root option. ./configure --shared --mkl-root=/home/\u0026lt;username\u0026gt;/intel/mkl Then install kaldi using the usual steps make depend -j \u0026lt;numcpu\u0026gt; make -j \u0026lt;numcpu\u0026gt; This will install Kaldi with MKL support without requiring any root privileges.\n","date":"2019-05-21T13:51:12+02:00","permalink":"https://deepakbaby.in/posts/kaldi-mkl/","tags":["kaldi","mkl"],"title":"Installing Kaldi with MKL support without root access"},{"categories":[],"contents":"Kaldi Speech Recognition Toolkit is a freely available toolkit that offers several tools for conducting research on automatic speech recognition (ASR). It lets us train an ASR system from scratch all the way from the feature extraction (MFCC,FBANK, ivector, FMLLR,\u0026hellip;), GMM and DNN acoustic model training, to the decoding using advanced language models, and produce state-of-the-art results.\nWhile kaldi offers so much flexibilty at every stage, sometimes we also need to play with features that are not offered by the kaldi repository. Kaldi makes use of ark format to store the features. If we want to perform experiments with customized features, they must be converted to the ark format first. The goal of this post is to explain how we can extract and store the custom features in the ark format using matlab and python.\nFirst steps Custom features using MATLAB Custom features using Python First Steps In this tutorial, we will be starting with a data folder that is generated by kaldi, in order to avoid directory validation errors (which is performed by kaldi before training any model). So we we will run the kaldi routine to prepare the data folders (typically done by local/\u0026lt;datasetname\u0026gt;_data_prep.sh script). This will generate all the required files such as text containing the transcriptions, utt2spk and spk2utt for CMVN and ivector computation, and wav.scp for reading the audio files from the disk. Then we will run the feature extraction pipeline in the default kaldi way, say FBANK features. Typically, the data-fbank folder contains the train, test and dev folder along with the text, utt2spk, spk2utt and wav.scp files.\nRunning the make_fbank_feats.sh will typically store the features in the ark format to the fbank folder and the corresponding feats.scp files (used for reading the ark files) will be placed in the respective subfolder in data-fbank. In order to run the feature extraction in parallel, kaldi typically splits the data into 10 sets and extract them in parallel. Thus the directory structure will typically be:\nfbank ├── raw_fbank_dev.1.ark ├── raw_fbank_dev.1.scp ├── ... ├── ... ├── raw_fbank_dev.10.ark ├── raw_fbank_dev.10.scp ├── raw_fbank_test.1.ark ├── raw_fbank_test.1.scp ├── ... ├── ... ├── raw_fbank_test.10.ark ├── raw_fbank_test.10.scp ├── raw_fbank_train.1.ark ├── raw_fbank_train.1.scp ├── ... ├── ... ├── raw_fbank_train.10.ark └── raw_fbank_train.10.scp data-fbank ├── dev │ ├── feats.scp │ ├── spk2gender │ ├── spk2utt │ ├── stm │ ├── text │ ├── utt2spk │ ├── wav.scp │ └── split10 | ├── test │ ├── feats.scp │ ├── spk2gender │ ├── spk2utt │ ├── text │ ├── utt2spk │ ├── wav.scp │ └─ split10 | ├── train ├── feats.scp ├── spk2gender ├── spk2utt ├── split10 ├── text ├── train ├── utt2spk └── wav.scp To summarize the steps so far:\n# run the data preparation bash local/\u0026lt;database\u0026gt;_data_prep.sh # extract fbank features mkdir fbank data-fbank for x in train test dev ; do cp -r data/$x data-fbank/ steps/make_fbank.sh --nj 10 --fbank-config conf/fbank.sh \\ data-fbank/$x exp/make_fbank/$x fbank || exit 1; done Notice that feats.scp is a concatenation of all the .scp files belonging to the subset (train, dev or test). .scp files will have two columns: the first column contains the unique id for the utterance and the second column says where the corresponding features are located in the ark file. This tutorial will manipulate the ark files and scp files in order that they are to be used by kaldi for training.\nThe the next step is to create myfeats and data-myfeats directories. These directories will follow the same structure as used by the fbank and data-fbank directories, but contains our custom features inside. Noice that the data folder will have the same files except the feats.scp. feats.scp has to be modified such that it tells kaldi to read features from the myfeats folder. The steps are summarized below:\n$ mkdir myfeats data-myfeats $ cp -r data-fbank/* data-myfeats $ rm data-myfeats/*/feats.scp # we will need to generate new feats.scp files for the custom feats Now the pre-requisites are done. Next, we can use either MATLAB or python to prepare the custom features and generate the correspoding ark and scp files.\nClick here to jump to the python approach\nWarning: Notice that if you want to use the alignments obtained using say MFCC features, the number of frames in the new features should match that of the MFCC features. i.e., the custom feature extraction must use the same window size and window shift. Otherwise, we will end up with wrong alignments and the (DNN or GMM) training will fail.\nCustom features using MATLAB Since kaldi makes use of a unique ID to read the ark files, we will need to follow the same format. As mentioned before, we will use the already prepared FBANK features as a reference file to generate our custom features. In order to do this with MATLAB, we will need to read and write the ark files using MATLAB.\nThe research page of Hynek Boril offers matlab routines for reading and writing ark files. The codes can be found here. It offers three MATLAB files:\narkread.m : For reading ark files arkwrite.m : For writing ark files ark2scp.m : For generating scp files from ark files So the general procedure is follows: Use arkread.m to read the ark files which will return a header mat and a feature matrix. The header mat contains the unique id of the utterance and the feature size related information. feature mat is a huge matrix that contains all the feature vectors. The idea is to change the feature mat with our custom features, change header mat accordingly and to write it using arkwrite.m. Then we will use ark2scp.m to generate the corresponding .scp file.\nA sample MATLAB code is given below.\nsetlist = {\u0026#39;test\u0026#39;, \u0026#39;train\u0026#39;, \u0026#39;dev\u0026#39;} myfeatname = \u0026#39;myfeats\u0026#39;; splits = 10; % the feature extraction is typically split into 10, change this otherwise fbankdatadir = fullfile(codedir, \u0026#39;data-fbank\u0026#39;); fbankfeatdir = fullfile(codedir, \u0026#39;fbank\u0026#39;); datadir = fullfile(codedir, [\u0026#39;data-\u0026#39;, myfeatname]); featdir = fullfile(codedir, myfeatname); % create dirs system([\u0026#39;mkdir -p \u0026#39; datadir \u0026#39; \u0026#39; featdir]) for setnum = 1:length(setlist) setname = setlist{setnum}; % copy files system([\u0026#39;cp -r \u0026#39; fbankdatadir \u0026#39;/\u0026#39; setname \u0026#39; \u0026#39; datadir \u0026#39;/\u0026#39;]) filelist = fullfile(datadir, setname, \u0026#39;wav.scp\u0026#39;); fid = fopen(filelist, \u0026#39;r\u0026#39;); filenames = textscan (fid, \u0026#39;%s %s\u0026#39;); fclose(fid); numfiles = length(filenames{1}); fileindex = 1; for splitindex = 1: splits disp ([\u0026#39;Processing \u0026#39; num2str(splitindex) \u0026#39; of \u0026#39; num2str(splits) \u0026#39; split data.\u0026#39;]); srcark = strcat(\u0026#39;raw_fbank_\u0026#39;, dtag, \u0026#39;.\u0026#39;, num2str(splitindex), \u0026#39;.ark\u0026#39;); srcark_filename = fullfile(fbankfeatdir, srcark); destark = strcat(\u0026#39;raw_\u0026#39;, myfeatname, \u0026#39;_\u0026#39;, setname, \u0026#39;.\u0026#39;, num2str(splitindex), \u0026#39;.ark\u0026#39;); destscp = strcat(\u0026#39;raw_\u0026#39;, myfeatname, \u0026#39;_\u0026#39;, setname, \u0026#39;.\u0026#39;, num2str(splitindex), \u0026#39;.scp\u0026#39;); destark_filename = fullfile(featdir, destark); destscp_filename = fullfile(featdir, destscp); [HEADER_MAT, FEATURE_MAT] = arkread(srcark_filename); HEADER_MAT_NEW = cell(size(HEADER_MAT,1), 5); FEATURE_MAT_NEW = zeros(size(FEATURE_MAT,1), featdim); % expects same frame dimension as FBANK framestart = 1; if lt(splitindex, splitstart) fileindex = fileindex + size(HEADER_MAT, 1); continue; end for filenum = 1 : size(HEADER_MAT, 1) disp([\u0026#39;File index is \u0026#39; num2str(fileindex) \u0026#39; of split set \u0026#39; num2str(splitindex)]) % verify the id if ~strcmp(HEADER_MAT{filenum,1}, filenames{1}(fileindex)) error (\u0026#39;ID mismatch!\u0026#39;) end % read the audio file, the filename is typically the second column in wav.scp % else, find the right column index in wav.scp and change the cell column below [x, fs] = audioread(char(filenames{2}(fileindex))); % Extract custom features here newfeat = extract_my_feats(x); % expected shape of newfeat is (numframes x featdim) % check if it has the same number frames as in FBANK % else, it is an error if we need to reuse alignments % if you dont need to use existing alignmnents % comment out this check if ne(HEADER_MAT{filenum,2}, size(newfeat,1)); error (\u0026#39;Dimension mismtach!\u0026#39;) end frameend = framestart + size(newfeat,1) - 1; % fill in header mat details HEADER_MAT_NEW{filenum,1} = HEADER_MAT{filenum,1}; HEADER_MAT_NEW{filenum,2} = size(newfeat,1); HEADER_MAT_NEW{filenum, 3} = featdim; HEADER_MAT_NEW{filenum, 4} = framestart; HEADER_MAT_NEW{filenum, 5} = frameend; % add features to featuremat FEATURE_MAT_NEW (framestart:frameend,:) = feat; fileindex = fileindex + 1; framestart = frameend + 1; end % write the ark file disp (\u0026#39;Writing ark files...\u0026#39;) arkwrite(destark_filename, HEADER_MAT_NEW, FEATURE_MAT_NEW); ark2scp(destark_filename); end % splitindex end % set num Now the features successfully stored to ark files and the corresponding scp files are generated. We will next have to generate the feats.scp file by concatenating the scp files which are split into 10 subsets. You can do it in a terminal using:\nnj=10 for x in test train dev; do rm -f data-myfeats/$x/feats.scp for n in $(seq $nj); do cat $feat/raw_myfeat_${x}.$n.scp || exit 1; done \u0026gt; data-myfeats/$name/feats.scp done Now you can change the data directory in training scripts to data-myfeats to train and decode using kaldi using your custom features.\nCustom features using Python As mentioned in the above section, snce kaldi makes use of a unique ID to read the ark files, we will need to follow the same format. For this we will use the already prepared FBANK features as a reference file to generate our custom features. In order to do this with python, we will need python routines to read and write ark files.\nThe kaldiio is a pure python module for reading and writing kaldi ark files. You can install it to your machine/environment using pip.\npip install kaldiio We will make use of the load_ark and save_ark commands. So the general procedure is follows: Use load_ark to read the ark files which will return the unique id and the corresponding feature matrix. The idea is to change the feature matrix with our custom features, use it with the unique id and to write it using save_ark. We will also use save_ark to generate the corresponding .scp file.\nA sample python code is given below.\nimport kaldiio import os import numpy as np import shutil import scipy as sp import scipy.signal as sp_sig def copy_and_overwrite(from_path, to_path): if os.path.exists(to_path): shutil.rmtree(to_path) shutil.copytree(from_path, to_path) basedir = os.getcwd() setlist = [\u0026#39;test\u0026#39;, \u0026#39;train\u0026#39;, \u0026#39;dev\u0026#39;] featdim = 64 # insert your feature dim here tags = \u0026#39;myfeats\u0026#39; splits = 10 fbankdatadir = os.path.join(basedir, \u0026#39;data-fbank\u0026#39;) fbankfeatdir = os.path.join (basedir, \u0026#39;fbank\u0026#39;) datadir = os.path.join(basedir, \u0026#39;data-\u0026#39;+ tags) featdir = os.path.join(basedir, tags); # create dirs if they dont exist if not os.path.isdir(datadir): os.makedirs(datadir) if not os.path.isdir(featdir): os.makedirs(featdir) for setnum in range(len(setlist)): setname = setlist[setnum]; print (\u0026#39;Processing \u0026#39; + setname + \u0026#39; data.\u0026#39;) # copy files src_dir = os.path.join(fbankdatadir, setname) dst_dir = os.path.join(datadir, setname) copy_and_overwrite(src_dir, dst_dir) filelist = os.path.join(datadir, setname, \u0026#39;wav.scp\u0026#39;) dtag = setname filenames = [] keys_orig = [] for line in open(filelist): filenames.append(line.split()[4]) # wavenames is in the 5th column keys_orig.append(line.split()[0]) # key or id is in the first column numfiles = len(filenames) fileindex = 0 for splitindex in range(splits): print (\u0026#39;Processing \u0026#39; + str(splitindex) + \u0026#39; of \u0026#39; + str(splits) + \u0026#39; split data.\u0026#39;) srcark = \u0026#39;raw_fbank_\u0026#39; + dtag + \u0026#39;.\u0026#39; + str(splitindex + 1) + \u0026#39;.ark\u0026#39; srcscp = \u0026#39;raw_fbank_\u0026#39; + dtag + \u0026#39;.\u0026#39; + str(splitindex + 1) + \u0026#39;.scp\u0026#39; destark = \u0026#39;raw_\u0026#39; + tags + \u0026#39;_\u0026#39; + dtag + \u0026#39;.\u0026#39; + str(splitindex + 1) + \u0026#39;.ark\u0026#39; destscp = \u0026#39;raw_\u0026#39; + tags + \u0026#39;_\u0026#39; + dtag + \u0026#39;.\u0026#39; + str(splitindex + 1) + \u0026#39;.scp\u0026#39; srcark_filename = os.path.join(fbankfeatdir, srcark) destark_filename = os.path.join(featdir, destark) srcscp_filename = os.path.join(fbankfeatdir, srcscp) destscp_filename = os.path.join(featdir, destscp) d = kaldiio.load_ark(srcark_filename) write_dict={} # kaldiio uses features in the form of a dict for key, array in d: # check if keys match if keys_orig[fileindex] != key : raise ValueError(\u0026#39;ID Mismatch!\u0026#39;) fname = filenames[fileindex] # read audio, assuming wav file # change this if the file is of some other type fs, sig = sp.io.wavfile.read(fname) # Do your feature extraction below: # expected shape is (numframes x featdim) newfeat = extract_my_feats(sig) # do the numframes check, if we are reusing existing alignments # else, comment out this check if array.shape[0] != newfeat.shape[0] : raise ValueError(\u0026#39;Dimension mismatch!\u0026#39;) # append the features into the dictionary write_dict [key] = newfeat.astype(np.float32) # let us store it as float32 fileindex += 1 # write features to disk print (\u0026#34;Writing to \u0026#34; + destark_filename) kaldiio.save_ark(destark_filename, write_dict, scp=destscp_filename) Notice that kaldiio.save_ark with the scp variable set to a destination name will also generate the scp file together with the ark write. We will next have to generate the feats.scp file by concatenating the scp files which are split into 10 subsets. You can do it in a terminal using:\nnj=10 for x in test train dev; do rm -f data-myfeats/$x/feats.scp for n in $(seq $nj); do cat $feat/raw_myfeat_${x}.$n.scp || exit 1; done \u0026gt; data-myfeats/$name/feats.scp done Now you can change the data directory in training scripts to data-myfeats to train and decode using kaldi using your custom features.\n","date":"2019-03-06T12:30:39+01:00","permalink":"https://deepakbaby.in/posts/kaldi-custom-features/","tags":["kaldi","matlab","python"],"title":"Training kaldi models with custom features"},{"categories":[],"contents":"Often we deal with networks that are optimized for multiple losses (e.g., VAE). In such scenarios, it is useful to keep track of each loss independently, for fine-tuning its contribution to the overall loss. This post details an example on how to do this with keras.\nLet us look at an example model which needs to trained to minimize the sum of two losses, say mean square error (MSE) and mean absolute error (MAE). Let $\\lambda_{mse}$ be the hyperparameter that controls the contribution of MSE to the toal loss. i.e., the total loss is MAE + $\\lambda_{mse}$ * MSE. This loss can be implemented using:\nimport keras.backend as K lambda_mse = 10 # hyperparameter to be adjusted def joint_loss (y_true, y_pred): # mse mse_loss = K.mean(K.square(y_true - y_pred)) # mae mae_loss = K.mean(K.abs(y_true - y_pred)) return mae_loss + (lambda_mse * mse_loss) with the model compiled as:\nmodel.compile(loss = joint_loss, optimizer=\u0026#39;Adam\u0026#39;) However, when we run model.fit(...) keras shows the progress something like this..\nEpoch 1/30 19488/144615 [===\u0026gt;..........................] - ETA: 1:52:37 - loss: 0.4103 Keras shows only the joint loss and does not give the individual MSE and MAE losses which makes it difficult to track how they evolve over epochs and to adjust $\\lambda_{mae}$ accordingly.\nIn order to track them, we will need to define individual losses as below.\nimport keras.backend as K lambda_mse = 10 # hyperparameter to be adjusted def joint_loss (y_true, y_pred): # mse mse_loss = K.mean(K.square(y_true - y_pred)) # mae mae_loss = K.mean(K.abs(y_true - y_pred)) return mae_loss + (lambda_mse * mse_loss) def mse_loss (y_true, y_pred): return K.mean(K.square(y_true - y_pred)) def mae_loss (y_true, y_pred): return K.mean(K.abs(y_true - y_pred)) Then we can use the metrics parameter in the model.compile to also track the MAE and MSE. This can be done by compiling the model using\nmodel.compile(loss = joint_loss, optimizer=\u0026#39;Adam\u0026#39;, metrics=[mse_loss, mae_loss]) Notice that the model is still compiled to optimize for the joint loss, but it also returns the MAE and MSE losses. Executing model.metrics_names will return three values, ['loss', 'mae_loss', 'mse_loss']. Now the model.fit(...) will show something like this\nEpoch 1/30 26336/144615 [====\u0026gt;.........................] - ETA: 1:46:54 - loss: 0.4078 - mae_loss: 0.1891 - mse_loss: 0.0219 Now we can see the joint loss and the individual losses that contributed to it. We can also verify that the joint loss indeed is mae_loss + 10 * mse_loss, where 10 was the value chosen for $\\lambda_{mse}$.\nSimiliarly, you can define your own loss terms and use the metrics parameter in model.compile to track them independently.\n","date":"2019-03-04T11:59:56+01:00","permalink":"https://deepakbaby.in/posts/keras-multiple-losses/","tags":["deep learning","keras"],"title":"Tracking Multiple Losses with Keras"},{"categories":[],"contents":"This post is intended for setting up tensorflow-gpu setup in a multi-user setting. This is written as a guide for GPU users at the WAVES research group, Ghent University, Belgium. But these are also applicable to any linux multi-user environment with GPU-based jobs.\nInstalling Tensorflow in conda conda installation conda tensorflow Testing Tensorflow Installation Admin only Installing the cuda compiler and nvidia drivers Installing Tensorflow in conda conda installation Anaconda is a popular python environment among the AI/ML community. The anaconda distribution can be downloaded from here. Follow the instructions here to properly install it to your user account.\nOnce you have installed anaconda into your user account, you can create a conda environment using\n$ conda create -n \u0026lt;name-of-your-environment\u0026gt; Then you can activate that environment using:\n$ conda activate \u0026lt;name-of-your-environment\u0026gt; Once you are in the environment, you can install whatever python packages you want. Anaconda already comes with numpy,scipy and many other useful python libraries. If you need a specific library, google for conda install and find how to install it. But always make sure that you are in the right conda environment before installing the new libraries. You can exit the environment by deactivating it\n$ conda deactivate conda tensorflow Anaconda also offers tensorflow and keras installations among many many other libraries. In order to install it to your environment, follow the steps below:\nActivate your conda environment Install keras $ conda install -c conda-forge keras Install tensorflow GPU version $ conda install tensorflow-gpu This should install other libraries that are required by keras and tensorflow. I found that it is better to install keras before installing tensorflow since keras also installs a tensorflow that may not be comaptible with the GPU (I am not 100% sure about this).\nNote: Instead of using conda install, we can also use pip install \u0026lt;the-library-you-need\u0026gt; in the same environment for installing libraries. But I recommend using conda.\nYou can use conda list to see all the installed libraries in your environment. conda env list will list all the conda environments in your system.\nWarning: A popular python editor called spyder also comes pre-installed with anaconda. But it does not work over remote desktop as the keyboard does not work properly. If somebody finds a workaround, kindly update this.\nTesting Tensorflow Installation You can test whether the tensorflow installation is using the GPU using the following options.\n$ python -c \u0026#34;import tensorflow as tf; sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\u0026#34; OR\n$ python -c \u0026#34;import tensorflow as tf; tf.test.is_gpu_available()\u0026#34; If it gives message like Adding visible gpu devices:, then it means that tensorflow indeed uses the GPU. If it only mentions CPU, then you will need to correct the installation. Often, it is better to install keras first and then install tensorflow, or use conda install tensorflow-gpu instead of conda install -c conda-forge tensorflow-gpu.\nAdmin only Warning: This section details the GPU installation guidelines. Only an admin should take care of this. Notice that this should be done only when there is a kernel update or when nvidia-smi command does not list any GPUs (meaning the system does not see the GPUs anymore).\nInstalling the cuda compiler and nvidia drivers These steps are adapted from here. Ignore the $ sign in the beginning of the commands.\nInstall the kernel headers for the current Ubuntu installation.\n$ sudo apt-get install linux-headers-$(uname -r) For other linux flavors, this step is different (Refer here for other linux distributions).\nDownload the runfile from the cuda downloads page.\nDisable the nouveau driver. The instructions are given in this page. For ubuntu, create a new file /etc/modprobe.d/blacklist-nouveau.conf with the following contents:\nblacklist nouveau \u0026lt;br\u0026gt; options nouveau modeset=0 Regenerate the kernel initramfs:\n$ sudo update-initramfs -u Disable the lightdm service to kill the X server from running.\n$ sudo service lightdm stop Also kill vncserver sessions (if they exist) (e.g., vncserver -kill :1 to kill the first vncserver and so on.). Also remove the .X0.lock or other .lock files present in the /tmp folder.\nGo to the Downloads folder where the downloaded runfile is stored. Make the file executable:\n$ chmod +x cuda\u0026lt;version\u0026gt;.linux.run Install the driver and compiler\n$ sudo ./cuda\u0026lt;version\u0026gt;.linux.run --no-opengl-libs The option --no-opengl-libs is important to avoid the login problems. You will then be asked the following and the requried responses are provided in bold font.\nAccept license agreement? yes You can press Ctrl+C to skip to the end of the license. Install NVIDIA driver? yes Should NVIDIA modify the x-config ? no Install CUDA? yes Path where cuda installations should be put: choose default or provide a path of your choice Install symbolic link? yes Install samples? yes Choose samples location: choose default or enter your choice This should install both the cuda compiler and nvidia drivers to the machine. Perform the post installation actions such as adding the cuda installation to your PATH and LD_LIBRARY_PATH. Follow the instructions here. You can also edit the ~/.bashrc file to add modify these variables.\nTry nvcc -V to check the nvidia compiler version and nvidia-smi to see the GPUs’ status in your machine.\nFinally, restart the lightdm service.\n$ sudo service lightdm restart The machine will have the GUI after the lightdm service is restarted. You will need to launch new vnc sessions in order to use remote desktop.\n","date":"2019-01-27T00:00:00Z","permalink":"https://deepakbaby.in/posts/cochlear-ugent/","tags":["linux","UGent"],"title":"Tensorflow-GPU in multi-user environment"},{"categories":[],"contents":"ഒരു കമ്പ്യൂട്ടർ നിങ്ങൾക്കുവേണ്ടി ഒരു ബാർബർ ഷോപ്പിലേക്കോ ഹോട്ടലിലേക്കോ ഫോൺ ചെയ്തു റിസർവേഷൻ എടുത്തുതരുന്ന കാലത്തെപ്പറ്റി നിങ്ങൾ ചിന്തിച്ചിട്ടുണ്ടോ? എങ്കിൽ അറിയുക, നാം അവിടെയെത്തിയെന്ന്! അതാണ് ഗൂഗിൾ ഡ്യൂപ്ളെക്സ് (Google Duplex) .\nആർട്ടിഫിഷ്യൽ ഇന്റലിജൻസ് സീരീസ് ഭാഗം- 5\nAI രംഗത്തെ ഏറ്റവും പ്രധാനപ്പെട്ട നാഴികക്കല്ലുകളിലൊന്നിനാണ് നമ്മൾ ഇന്നലെ സാക്ഷ്യം വഹിച്ചത്. നമുക്കുവേണ്ടി ഫോൺ കാളുകൾ നടത്താനും അവിടെയുള്ളവരോട് സംസാരിക്കാനും കഴിയുന്ന AI സംവിധാനമായ Google Duplex ഇന്നലെ ഗൂഗിൾ അവതരിപ്പിച്ചു (വീഡിയോ കാണുക). Google assistant കുറെ കാലമായി നമ്മൾ കണ്ടിരുന്നതാണെങ്കിലും അതിനു ധാരാളം പരിമിതികളുണ്ടായിരുന്നു. അതിൽനിന്നൊക്കെ വളരെയധികം മുന്നോട്ടുപോയ ഒരു മനുഷ്യൻതന്നെയെന്നു തോന്നിപ്പിക്കുമാറ് നമ്മുടെ സംസാരത്തിലെ ചെറിയ കാര്യങ്ങൾ വരെ (ഇടക്കുള്ള pause, hmmm, err ശബ്ദങ്ങൾ) ഉൾപ്പെടുത്തിയാണ് ഈ AI സംവിധാനം സംസാരിക്കുന്നത്!\nഇന്നലത്തെ പരിപാടിയിൽ രണ്ടു ഉദാഹരണങ്ങളാണ് google അവതരിപ്പിച്ചത്. അതിൽ ആദ്യത്തേത് ഒരു ബാർബർ ഷോപ്പിൽ മുടിവെട്ടാൻ റിസേർവ് ചെയ്യുന്നതാണ്. ഇത്തരം ഒരു കോളിൽ എന്തൊക്കെ സംഭവിക്കുമെന്ന് മുൻകൂട്ടി അറിയുക സാധ്യമല്ല. ബാർബർ ഷോപ്പിൽ ഫോൺ എടുത്തയാൾ പറയുന്നതിനനുസരിച്ചു ബുദ്ധിപരമായി സ്വയം പ്രതികരിക്കാനുള്ള കൃത്രിമബുദ്ധിക്കുപിന്നിലെ സങ്കീർണതകൾ പലതാണ്. അതെല്ലാം കൃത്യമായി ഉൾക്കൊള്ളിച്ച് ബാർബർ ഷോപ്പിലെ ആൾക്ക് തന്നോട് സംസാരിക്കുന്നത് ഒരു മെഷീനാണെന്നു ഒരിക്കൽപോലും സംശയം തോന്നാത്ത രീതിയിലാണ് google duplex സംസാരിക്കുന്നത്!\nരണ്ടാമത്തെ ഉദാഹരണം അതിലും ബുദ്ധിമുട്ടേറിയതായിരുന്നു. restaurant reservation ആണ് അതിൽ google duplex ചെയ്യുന്നത്. അതിൽ ഫോൺ എടുക്കുന്നതാകാട്ടെ ചൈനീസ് അക്‌സെന്റിൽ ഇംഗ്ലീഷ് സംസാരിക്കുന്ന ആളും. അതോടൊപ്പം നാലുപേരിൽ താഴേ ഉള്ള ഗ്രൂപ്പിന് റിസർവേഷൻ പറ്റില്ല എന്ന് പറയുമ്പോൾ അതും AI മനസ്സിലാക്കുന്നുണ്ട്! ഇത്തരമൊരു സാഹചര്യമൊന്നും മുൻകൂട്ടി പഠിച്ച മാതൃകകളിൽനിന്നു പഠിച്ചെടുക്കാനായെന്നുവരില്ല. അവിടെയാണ് ഗൂഗിൾ എന്ന ബിഗ് ഡാറ്റ ഭീമൻ വികസിപ്പിച്ചെടുത്ത ഈ സംവിധാനം വേറിട്ടു നിൽക്കുന്നത്.\nGoogle Duplex ഇൽ AI രംഗത്തെ ധാരാളം സാങ്കേതികവിദ്യകൾ ഉപയോഗിച്ചിട്ടുണ്ട്. ആദ്യം നമ്മൾ കൊടുക്കുന്ന ടാസ്ക് മനസിലാക്കണം. അതിനു automatic speech recognition (ASR) സാങ്കേതികവിദ്യയാണ് ഉപയോഗിക്കുന്നത്. നമ്മുടെ ശബ്ദത്തെ മൈക്രോഫോൺവച്ചു റെക്കോർഡ് ചെയ്ത്, അതിലെ വാക്കുകൾ കണ്ടുപിടിക്കുന്നതാണ് ASR. ഒരുതരത്തിൽ പറഞ്ഞാൽ speech-to-text. ഈ വാക്കുകളിൽനിന്നും അതിലെ അർഥം മനസിലാക്കണം. ഒരു ടാസ്ക് ആണെങ്കിൽ അതിലെ പ്രധാനവാക്കുകൾ (keywords) കണ്ടെത്തുകയാണ് ആദ്യപടി.\nഉദാഹരണത്തിന്, Book a dinner reservation for six people at ZamZam next Wednesday evening, എന്നാണു നമ്മൾ പറയുന്നതെങ്കിൽ ആ റെക്കോർഡിങ്ങിൽ നിന്നും ഈ വാക്കുകൾ ASR ഉപയോഗിച്ച് ആദ്യം കണ്ടെത്തും. എന്നിട്ടു keywords കണ്ടുപിടിക്കും.പ്രധാനമായും കണ്ടെത്തുന്നവ\nwhat? : Book dinner\nwhen? : Wednesday evening\nwhere?: ZamZam\nhow many seats ? : six\nഇതിൽ നിന്നും വെബ് സെർച്ച് നടത്തി സംസമിലെ ഫോൺ നമ്പർ കണ്ടെത്തി ഫോൺ ചെയ്യുകയാണ് അടുത്തപടി. അവിടെ ഫോൺ എടുക്കുന്നയാൾ എങ്ങനെയാണ് പ്രതികരിക്കുകയെന്നു google duplex നു അറിയില്ലെന്നോർക്കണം. പിന്നീട് സംഭാഷണം നടക്കുകയാണ്. ഓരോ വാക്യങ്ങളും ASR ഉപയോഗിച്ചാണ് AI മനസിലാക്കുന്നത്. അതോടൊപ്പം നാച്ചുറൽ ലാംഗ്വേജ് അണ്ടർസ്റ്റാൻഡിങ് (NLU) എന്ന സങ്കേതവുമുപയോഗിച്ചെങ്കിലേ AI ക്കു സംഭാഷണം സാധ്യമാകൂ. അപ്പോൾ ഫോൺ എടുത്തയാളുടെ വാക്യങ്ങളുടെ ASR ഉം NLU ഉം ഉപയോഗിച്ച്, അടുത്തതായിപറയാനുള്ള മറുപടി text രൂപത്തിൽ ഉണ്ടാക്കപ്പെടുന്നു.\nഈ text നെ ശബ്ദങ്ങൾ (speech) ആക്കുകയാണ് അടുത്തപടി. ഇതിനു text-to-speech (TTS) എന്നാണു പറയുന്നത്. TTS ആണ് നമ്മുടെ സംസാരത്തിലെ അക്‌സെന്റ്, ഇടക്കുള്ള pause കൾ hmm,err പോലുള്ള ശബ്ദങ്ങൾ ഇടുന്നത്. ഒരു മെഷീന് അങ്ങനെ ശബ്ദങ്ങൾ ഉണ്ടാക്കണ്ടകാര്യമില്ല. പക്ഷെ അപ്പുറത്തു മഷിനോട്‌ സംസാരിക്കുന്നത് ഒരു മനുഷ്യനാണ്. തന്നോട് സംസാരിക്കുന്നത് ഒരു മെഷീൻ അല്ല, ഒരു മനുഷ്യൻ തന്നെയാണ് എന്നു തോന്നിപ്പിക്കാനാണ് ഇത്തരം ശബ്ദങ്ങൾ മനഃപൂർവം ഇടുന്നത്.\nഅപ്പോൾ Google Duplex എന്നാൽ ASR, NLU, TTS, web search integration എന്നീ അതിസങ്കീർണമായ AI സാങ്കേതികവിദ്യകളുടെ ആകെത്തുകയാണ്. ഗൂഗിളിന്റെ എതിരാളികളായ ആപ്പിൾ, ആമസോൺ, മൈക്രോസോഫ്ട് തുടങ്ങിയ കമ്പനികൾക്ക് വലിയൊരു വെല്ലുവിളിതന്നെയാണ് duplex.\nഅതോടൊപ്പം ധാരാളം ചോദ്യങ്ങളും ഇതുയർത്തുന്നുണ്ട്. ഒരു AI ആണ് തന്നോട് സംസാരിക്കുന്നതെന്ന് ഫോൺ എടുത്ത മനുഷ്യർക്ക്‌ മനസിലായിട്ടില്ല (ഒരു AI ക്കു ഇതു സാധ്യമായാൽ അത് Turing test പാസായി എന്നാണു പറയുക). അതുപറയാനുള്ള ഉത്തരവാദിത്വം ഗൂഗിളിനില്ലേ എന്നതാണ് പ്രധാനചോദ്യം. മറ്റുപല സ്വകാര്യതാപ്രശ്നങ്ങളും പലരും ഉയർത്തുന്നുണ്ട്‌.\nഎന്തൊക്കെയായാലും, AI രംഗത്തെ പ്രധാനപ്പെട്ട ഒരു മുന്നേറ്റമാണ് Google Duplex. മനുഷ്യൻ മറ്റുള്ളവരുമായി ബന്ധപ്പെടാൻ ഏറ്റവുമധികം ഉപയോഗിക്കുന്ന ഉപാധിയാണ് speech. അതിൽ ഇത്രവലിയൊരു മുന്നേറ്റമെന്നത് AI രംഗത്തുള്ളവർക്ക് വലിയ ഉണർവുതന്നെയാണ്. അതോടൊപ്പം അതുയർത്തുന്ന വെല്ലുവിളികളെ നാം എങ്ങനെ നേരിടുമെന്നും കാത്തിരുന്നു കാണാം.\n","date":"2018-05-09T00:00:00Z","permalink":"https://deepakbaby.in/posts/ai-5/","tags":["ai","malayalam"],"title":"Google Duplex"},{"categories":[],"contents":"ആർട്ടിഫിഷ്യൽ ഇന്റലിജൻസ് ഗവേഷണത്തിൽ ഇന്ന് ഏറ്റവുമധികം ഉപയോഗത്തിലിരിക്കുന്ന ന്യൂറൽ നെറ്റ്‌വർക്കുകളെ കുറിച്ചാണ് കഴിഞ്ഞ ഭാഗത്തിൽ പറഞ്ഞത് (കഴിഞ്ഞ ഭാഗം ഇവിടെ വായിക്കാം). ഇനി ചരിത്രത്തിലെ രണ്ടാം ഘട്ടത്തിലേക്ക്. മുമ്പുപറഞ്ഞതുപോലെ ന്യൂറൽ നെറ്റ്‌വർക്കുമായി ബന്ധപ്പെട്ട ആദ്യകാല ശ്രമങ്ങളും ഗവേഷണങ്ങളുമാണ് ഈ ഭാഗത്തിൽ.\n1943 - ഇലെക്ട്രിക്കൽ സർക്യൂട്ടുകൾ ഉപയോഗിച്ച് ഒരു ന്യൂറൽ നെറ്റ്‌വർക്ക് നിർമ്മിക്കപ്പെട്ടു\nനമ്മുടെ ന്യൂറോണുകൾ എങ്ങനെയായിരിക്കും പ്രവർത്തിക്കുന്നത് എന്നതിനെപ്പറ്റി ന്യൂറോഫൈസിയോളജിസ്റ്റായ വാറൻ മക്കുല്ലോഷും ഗണിതജ്ഞനായ വാൾട്ടർ പിട്സും ചേർന്ന് ഒരു സിദ്ധാന്തം അവതരിപ്പിച്ചു. അതിന്റെ ഒരു ചെറിയ മോഡൽ ഇലെക്ട്രിക്കൽ സർക്യൂട്ടുകൾ ഉപയോഗിച്ച് അവർ നിർമിക്കുകയും ചെയ്തു. നമ്മുടെ ശരീരത്തിലെ ന്യൂറോണുകളുടെ പ്രവർത്തനത്തെപ്പറ്റി അനുലഭ്യമായിരുന്ന പരിമിതമായ അറിവുവച്ചാണ് അത്തരമൊരു മാതൃക അവർ നിർമ്മിച്ചത്.\n1952- checkers game കളിക്കുന്ന കമ്പ്യൂട്ടർ\nഅമേരിക്കയിൽ പ്രചാരത്തിലുള്ള ഒരു ബോർഡ് ഗെയിം ആണ് checkers. AI യുടെ തുടക്കക്കാരിലൊരാളായ ആർതർ സാമുവേൽ ഓരോ കളികഴിയുന്തോറും കളി മെച്ചപ്പെടുത്തുന്ന ഒരു കമ്പ്യൂട്ടർ പ്രോഗ്രാം വികസിപ്പിച്ചു. ആരംഭകാലത്തെ AI ഗവേഷണങ്ങൾ മിക്കതും വികസിപ്പിക്കപ്പെട്ടത് ബോർഡ് ഗെയിമുകളെ അടിസ്ഥാനപ്പെടുത്തിയായിരുന്നു. താരതമ്യേന ലളിതമായ നിയമങ്ങളും എന്നാൽ കളിക്കുമ്പോൾ വളരെയധികം സങ്കീര്ണമാവുകയും ചെയ്യുന്നവയാണ് മിക്ക ബോർഡ് ഗെയിമുകളും (ചെസ്സ് ആണ് ഒരു നല്ല ഉദാഹരണം). ആർതർ സാമുവേൽ ആണ് മെഷീൻ ലേർണിംഗ് എന്ന വാക്ക് ആദ്യമായി അവതരിപ്പിച്ചത്.\n1957- The perceptron\n1957-ൽ ഫ്രാങ്ക് റോസെൻബ്ലേറ്റ് എന്ന ഗവേഷകൻ പെർസെപ്ട്രോൺ എന്ന ഒരുതരം ന്യൂറൽ നെറ്റ്‌വർക്ക് അവതരിപ്പിച്ചു. നമ്മുടെ തലച്ചോറിലെയെന്നപോലെ പല ന്യൂറോണുകളിൽനിന്നും വരുന്ന വിവരങ്ങളെ എകോപിപ്പിച്ചു തീരുമാനങ്ങൾ എടുക്കാൻ സാധിക്കത്തക്കരീതിയിലാണ് ഇവയെ രൂപകല്പന ചെയ്തിരുന്നത്. ഭാവിയിൽ മനുഷ്യരെപ്പോലെ നടക്കാനും സംസാരിക്കാനും കാണാനുമൊക്കെ കഴിയുകയും സ്വന്തമായി ഒരു അസ്തിത്വമുണ്ടെന്നു സ്വയം മനസിലാക്കാനും കഴിയുന്ന കംപ്യൂട്ടറുകളിലേക്കുള്ള ആദ്യപടിയാണിതെന്നാണ് അദ്ദേഹം ഇതിനെപറ്റി പറഞ്ഞത്. ഇത് അക്കാലത്തു വിവാദമാവുകയും ചെയ്തിരുന്നു. ഒരു ഫോട്ടോയിലുള്ളത് ഒരു വസ്തുവാണോ അല്ലയോ എന്നുള്ള തീരുമാനം മാത്രം എടുക്കാൻ സാധിക്കുന്നവയായിരുന്നു പെർസെപ്ട്രോൺസ്. ഉദാഹരണത്തിന് ഒരു ചിത്രത്തിലുള്ളത് നായയാണോ അല്ലയോ എന്ന് മാത്രം പറയാൻ കഴിയുന്നവ. പൂച്ചയുടെ ചിത്രം കാണിച്ചാൽ അതു പൂച്ചയാണെന്നു അതിനു മനസിലാക്കാനാവില്ല. പകരം അതൊരു നായയല്ല എന്ന് മാത്രം മനസിലാകും (Dog അല്ലെങ്കിൽ Not Dog). ആരംഭകാലത്തു പെർസെപ്ട്രോൺസ് പ്രതീക്ഷക്കു വക നൽകിയിരുന്നെങ്കിലും പിന്നീട് അവക്ക് ഒന്നിലധികം വസ്തുക്കളെ തിരിച്ചറിയാനാകില്ല എന്ന പോരായ്മ മൂലം അധികം ഗവേഷകർ അത് എറ്റെടുത്തില്ല. ന്യൂറൽ നെറ്റ്‌വർക്ക് അടിസ്ഥാനമാക്കിയുള്ള ഗവേഷണം ഇതുകൊണ്ടൊക്കെ കുറെ നാൾ ആരും തുടർന്നില്ല.\n1959- Stanford\u0026rsquo;s MADALINE\nഒരു ന്യൂറൽ നെറ്റ്‌വർക്ക് ആദ്യമായി ഒരു റിയൽ വേൾഡ് അപ്പ്ലിക്കേഷനിൽ ഉപയോഗിക്കപ്പെട്ടു. സ്റ്റാൻഫോർഡിൽ വികസിപ്പിക്കപ്പെട്ട MADALINE എന്ന പ്രോഗ്രാം ഫോൺ കോളുകളിലെ മുഴക്കം (echo) കുറയ്ക്കാനാണ് ഉപയോഗിച്ചത്. ഈ പ്രോഗ്രാം ഇന്നും ഉപയോഗിക്കപ്പെടുന്നുണ്ട്. ഇത് പൂർണമായ അർത്ഥത്തിൽ ഒരു ന്യൂറൽ നെറ്റ്‌വർക്ക് എന്ന് പറയാനാവില്ലെങ്കിലും ഗവേഷണങ്ങളിൽ മാത്രം ഒതുങ്ങാതെ നമ്മുടെ ദൈനംദിന ആവശ്യങ്ങൾക്കും ഇത്തരം അൽഗോരിതങ്ങൾ ഉപയോഗപ്പെടുത്താമെന്നതിനുള്ള ഒരുപക്ഷേ ആദ്യത്തെ ഉദാഹരണമാവും MADALINE.\n1985- NETtalk\nഇംഗ്ലീഷ് വാക്കുകൾ ഉച്ചരിക്കാൻവേണ്ടി ടെറി സെയ്‌നോവ്സ്കി, ചാൾസ് റോസെൻബെർഗ് എന്നീ ഗവേഷകർ രൂപകല്പന ചെയ്ത ആർട്ടിഫിഷ്യൽ ന്യൂറൽ നെറ്റ്‌വർക്ക് ആണ് NETtalk. 20000 ഇംഗ്ലീഷ് വാക്കുകൾ ഉച്ചരിക്കുവാൻ ഈ ന്യൂറൽ നെറ്റ്‌വർക്കിന് കഴിയുമായിരുന്നു. അതായത്, അതിലേക്കു ഇംഗ്ലീഷ് വാക്ക് ഇൻപുട്ടായി കൊടുത്താൽ ആ വാക്കിന്റെ ഉച്ചാരണം അത് ഔട്പുട്ടിൽ നൽകും. ഇവ എങ്ങനെയാണ് പ്രവർത്തിച്ചിരുന്നതെന്നു മനസ്സിലാക്കണമെങ്കിൽ നമുക്ക് കൂടുതൽ കാര്യങ്ങൾ പഠിക്കേണ്ടതുണ്ട്. അതിലേക്കു അടുത്ത ഭാഗത്തിൽ കടക്കാം.\nകാര്യങ്ങൾ പ്രതീക്ഷക്കു വകനല്കുന്നവ ആയിരുന്നെങ്കിലും ന്യൂറൽ നെറ്റ്‌വർക്ക് അടിസ്ഥാനപ്പെടുത്തിയുള്ള AI ഗവേഷണങ്ങൾ അധികമാരും തുടർന്നില്ല. അതിനുള്ള പ്രധാനകാരണങ്ങൾ ഇവയാണ്.\nന്യൂറൽ നെറ്റ്‌വർക്കുകൾ എങ്ങനെയാണ് പ്രവർത്തിക്കുന്നതെന്നോ അവയെ എങ്ങനെ ഓരോ കൃത്യങ്ങൾക്കു ഉപയോഗിക്കാമെന്നുള്ള വ്യകതമായ ധാരണയോ പണ്ടുണ്ടായിരുന്നില്ല. അതിൽ ഉപയോഗിയ്ക്കപ്പെടുന്ന സങ്കീർണമായഗണിതവും അത് കൃത്യമായി നിർധാരണം ചെയ്തെടുക്കാനുള്ള രീതികൾ നിലവിലില്ലാതിരുന്നതുമായിരുന്നു കാരണം. ന്യൂറൽ നെറ്റ്‌വർക്കുകൾക്കു പാറ്റേണുകൾ പഠിക്കണമെങ്കിൽ ധാരാളം ട്രെയിനിങ് ഡാറ്റ ആവശ്യമായിരുന്നു. അന്നത്തെക്കാലത്ത് ഇന്നുള്ളതുപോലെ ബിഗ് ഡാറ്റയൊന്നും ലഭ്യമായിരുന്നില്ല. ന്യൂറൽ നെറ്റ്‌വർക്കുകൾ ട്രെയിൻ ചെയ്യാൻ, ഓരോവസ്തുക്കളിലെയും പാറ്റേണുകൾ തമ്മിൽ വേർതിരിച്ചറിയുന്നതെങ്ങനെ എന്നതൊക്കെ പഠിപ്പിക്കാൻ ധാരാളം കമ്പ്യൂട്ടർ പവർ വേണമായിരുന്നു. അന്ന് അതും ഉണ്ടായിരുന്നില്ല. ന്യൂറൽ നെറ്റ്‌വർക്കുകളെ ഓരോ ടാസ്കുകൾ ചെയ്യാൻ പഠിപ്പിക്കുന്ന ട്രെയിനിങ് രീതികൾ അന്നധികം വികസിപ്പിക്കപ്പെട്ടിരുന്നില്ല. ഉള്ളവയാകട്ടെ ധാരാളം സമയമെടുക്കുന്നവയും! ഇതോടൊപ്പം AI രംഗത്ത് താരതമ്യേന കുറച്ചു കമ്പ്യൂട്ടർ റിസോഴ്‌സ് ആവശ്യമുള്ള സപ്പോർട് വെക്ടർ മഷീൻസ്, nearest neighbor തുടങ്ങിയ സങ്കേതങ്ങൾ വ്യാപകമായി ഉപയോഗത്തിൽ വന്നു. അതോടെ ന്യൂറൽ നെറ്റ്‌വർക്ക് ഗവേഷണം എകദേശം എല്ലാവരും കൈവിട്ടു. പിന്നീടൊരു ഉയർത്തെഴുന്നേല്പുണ്ടാകുന്നത് 2006 ഇലാണ്. അതേപറ്റി അടുത്ത ഭാഗം ചരിത്രത്തിൽ.\n","date":"2018-04-24T00:00:00Z","permalink":"https://deepakbaby.in/posts/ai-4/","tags":["ai","malayalam"],"title":"ആർട്ടിഫിഷ്യൽ ഇന്റലിജൻസ് സീരീസ്: ഭാഗം - 4 "},{"categories":[],"contents":"ന്യൂറൽ നെറ്റ്‌വർക്കുകൾ 1950 കൾക്ക് മുൻപുള്ള, ഇന്നത്തെ കമ്പ്യൂട്ടർ സയൻസിന്റെയും ആർട്ടിഫിഷ്യൽ ഇന്റലിജൻസിന്റെയും വളർച്ചക്ക് വിത്തുപാകിയ ചില സിദ്ധാന്തങ്ങളാണ് കഴിഞ്ഞ ഭാഗത്തിൽ പറഞ്ഞത്. അത്തരം സിദ്ധാന്തങ്ങളിൽ നിന്നും പ്രചോദനമുൾക്കൊണ്ട് പ്രവർത്തിക്കുന്ന യന്ത്രങ്ങൾ 1950 കൾക്ക് ശേഷമാണ് യാഥാർഥ്യമായത്. അന്നുമുതൽ 2006 വരെയുള്ള കാലമാണ് AI ചരിത്രത്തിലെ രണ്ടാം ഘട്ടം. അതിലേക്കു കടക്കുന്നതിനുമുന്പ് നമ്മൾ ന്യൂറൽ നെറ്റ്‌വർക്ക് എന്താണെന്ന് മനസിലാക്കേണ്ടതുണ്ട്.\nകൃത്രിമബുദ്ധി അഥവാ AI എന്നത് മഷിനുകൾക്കു മനുഷ്യനെപ്പോലെ ചിന്തിക്കാനുള്ള ബുദ്ധി കൃത്രിമമായി നിർമിക്കുന്ന പ്രക്രിയയാണ്. നമ്മുടെ തലച്ചോറിനെയും, അതിലേക്കു ബന്ധപ്പെട്ടിരിക്കുന്ന, നമ്മുടെ ഇന്ദ്രിയങ്ങളിൽനിന്നും സിഗ്നലുകൾ അവിടേക്കെത്തിക്കുന്ന ന്യൂറോണുകളെയും അടിസ്ഥാനപ്പെട്ടാണ് നമ്മുടെ ബുദ്ധി ഇരിക്കുന്നത്. നമ്മുടെ തലച്ചോറിൽ 100 ബില്യണിലധികം ന്യൂറോണുകൾ ഉണ്ടെന്നാണ് കണക്ക്. നമ്മുടെ ഇന്ദ്രിയങ്ങളിൽ നിന്നും വരുന്ന ഇത്തരം കോടിക്കണക്കിനു സിഗ്നലുകളെ ക്രോഡീകരിച്ചാണ് നമ്മുടെ തലച്ചോറ് സംവേദനം (പെർസെപ്ഷൻ) എന്നത് സാധ്യമാക്കുന്നത്. ഉദാഹരണം പറഞ്ഞാൽ, നാം ഒരാളെ കാണുമ്പോൾ അയാളിൽനിന്നും വരുന്ന പ്രകാശകിരണങ്ങൾ നമ്മുടെ കണ്ണിൽ പതിക്കുകയും ആ കിരണങ്ങൾക്കനുസൃതമായി കണ്ണിൽ നിന്നും സിഗ്നലുകൾ തലച്ചോറിലേക്ക് ന്യൂറോണുകൾ എത്തിക്കുകയും ചെയ്യും. ഈ സിഗ്നലുകളിൽ നിന്നാണ് നമ്മുടെ തലച്ചോറ് നാം ആ ആളെ കണ്ടു എന്ന തോന്നൽ അല്ലെങ്കിൽ perception ഉണ്ടാക്കുന്നത്. കാഴ്ചക്ക് തലച്ചോറിലെ visual cortex എന്ന ഭാഗമാണ് പ്രധാനമായും ഉപയോഗിക്കുന്നത്. നമ്മുടെ തലച്ചോറ് ന്യൂറോണുകളിൽനിന്നും വരുന്ന സിഗ്നലുകളെ എങ്ങനെ ക്രോഡീകരിക്കുന്നു എന്നത് ഇന്നും നമുക്കധികം മനസിലാകാത്ത വിഷയമാണ്. അത് അറിയില്ലാത്തതുകൊണ്ടാണ് AI തീരുമാനങ്ങൾ (decision making ) എടുക്കാൻ മറ്റു സങ്കേതങ്ങൾ ഉപയോഗിക്കുന്നതും. (മറ്റു സങ്കേതങ്ങളെ പറ്റി വരും ഭാഗങ്ങളിൽ പറയാം)\nനമ്മുടെ ശരീരത്തിലെ സിഗ്നലുകൾ (ഉദാഹരണത്തിന് കണ്ണിൽ പതിഞ്ഞ പ്രകാശമോ, ഒരാൾ നമ്മളെ സ്പർശിക്കുന്നതോ) ന്യൂറോണുകൾ വഴിയാണ് തലച്ചോറിൽ എത്തുന്നതെന്ന് പറഞ്ഞല്ലോ. നമ്മുടെ ശരീരത്തിലെ ന്യൂറോണുകൾ എല്ലാം നേരെ തലച്ചോറുമായല്ല ബന്ധിപ്പിക്കപ്പെട്ടിരിക്കുന്നത്.. ഒരു ന്യൂറോണിൽ വരുന്ന സിഗ്നൽ മറ്റു പല ന്യൂറോണുകളിലൂടെ സഞ്ചരിച്ചാണ് ഒടുവിൽ തലച്ചോറിലെത്തുന്നത്. അതായത്, ഒരാളിൽ നിന്നും പ്രതിഫലിക്കുന്ന പ്രകാശം നമ്മുടെ കണ്ണിലെത്തുകയും, കണ്ണിനോട് നേരിട്ടു ബന്ധിപ്പിക്കപ്പെട്ടിരിക്കുന്ന നെർവ്കൾ ആ പ്രകാശത്തിനു ആനുപാതികമായരീതിയിൽ ഒരു ഇലക്ട്രിക്ക് സിഗ്നൽ ഉണ്ടാക്കുകയും ചെയ്യുന്നു. ഇത് അടുത്ത ന്യൂറോണിലേക്കു അതിലെ സിനാപ്സുകൾ വഴി കൈമാറ്റം ചെയ്യപ്പെടുന്നു (ചിത്രം കാണുക).\nസിനാപ്സുകളിൽ ആ കൈമാറ്റം നടക്കുമ്പോൾ ഈ സിഗ്നലിൽ വീണ്ടും മാറ്റങ്ങൾ ഉണ്ടാവും. തലച്ചോറിനുള്ളിലും ന്യൂറോൺ വഴിയുള്ള കൈമാറ്റങ്ങൾ നടക്കുന്നുണ്ട്. കണ്ണിൽ ഓരോ പ്രകാശംവന്നു പതിക്കുമ്പഴും ഒന്നിലധികം ന്യൂറോണുകൾ ഇലെക്ട്രിക്കൽ സിഗ്നൽ ഉത്പാദിപ്പിക്കുന്നുണ്ട്. അതിനെ നെർവ് ഫയറിങ് (nerve firing) എന്നാണു വിളിക്കുന്നത്. ഇത്തരം സിഗ്നലുകളുടെ പവർ കണ്ണിൽ പതിക്കുന്ന പ്രകാശത്തിനു ആനുപാതികമായിട്ടായിരിക്കും. ഓരോ കാഴ്ചക്കും ഓരോ രീതിയിലാണ് ഫയറിംഗ് ഉണ്ടാവുക. അതായത് രണ്ടുവസ്തുക്കളിൽ നിന്നും പ്രതിഫലിക്കുന്ന പ്രകാശം വ്യത്യസ്തമായതിനാൽ, അവ കണ്ണിൽ പതിക്കുമ്പോൾ ഉള്ള നെർവ് ഫയറിങ്ങുകളും വ്യത്യസ്തങ്ങളായിരിക്കും. ഈ ഫയറിങ്ങിലെ വ്യത്യാസമാണ് നാം രണ്ടു വസ്തുക്കളെ രണ്ടായി സംവേദിക്കാൻ (perceive) ചെയ്യാനുള്ള കാരണം.\nഇത്തരം ന്യൂറൽ ഫയറിങ്ങുകൾ ഉത്പാദിപ്പിക്കുന്ന ഇലെക്ട്രിക്കൽ സിഗ്നലുകൾ തലച്ചോറിൽ എത്തുന്നത് ഒന്നിലധികം ന്യൂറോണുകൾ വഴിയാണെന്ന് പറഞ്ഞല്ലോ. ഓരോ സിനാപ്സിലും സിഗ്നലുകൾക്കു രൂപമാറ്റം സംഭവിക്കപ്പെടുമെന്നും. നമ്മൾ ഇതിനെ ലേയേറുകൾ (layers) എന്നാണു വിളിക്കുന്നത്. ഉദാഹരത്തിനു കണ്ണിലെ 1000 ന്യൂറോണുകൾ ആണ് ആദ്യ ലേയർ (ഈ സംഖ്യകൾ കൃത്യമല്ല). അത് 4000 ന്യൂറോണുകളോട് ബന്ധപ്പെട്ടിരിക്കുന്നു എന്ന് സങ്കല്പിക്കുക. കണ്ണിലെ ഈ 1000 ന്യൂറോണുകളിൽ നിന്നും അടുത്ത ലെയറിലെ 4000 ന്യൂറോണുകളിലേക്കു സിഗ്നൽ കൈമാറ്റം ചെയ്യപ്പെടുന്നു. അവിടന്ന് അടുത്ത ലെയറിലേക്ക്. അവിടെ ഒരുപക്ഷേ 10000 ന്യൂറോണുകൾ ആകാം ഉണ്ടാവുക. അങ്ങനെ പല ലേയറുകളിലൂടെ സഞ്ചരിച്ചാണ് സിഗ്നൽ തലച്ചോറിലെത്തുന്നത്. ഇതിനെയാണ് നമ്മൾ multi-layer neural networks എന്ന് വിളിക്കുന്നത്. ഓരോ ലെയറിലും സിനാപ്‌സിൽ കൈമാറ്റം നടക്കുമ്പോൾ സിഗ്നലിന്റെ രൂപം മാറുകയും ചെയ്യുന്നുണ്ട്.\nചുരുക്കത്തിൽ, ന്യൂറോണുകളുടെ ഒരു വലിയ നെറ്റ്‌വർക്ക് ആണ് നമ്മുടെ തലച്ചോറ് ഉപയോഗപ്പെടുത്തുന്നത്. വർഷങ്ങൾ നീണ്ട പരിണാമത്തിനു വിധേയമാക്കപ്പെട്ടു, ഏറ്റവും മികച്ച വികാസം കൈവരിച്ച, perfect machine ആണ് നമ്മുടെ തലച്ചോറ്. അതുകൊണ്ടുതന്നെ കൃത്രിമബുദ്ധി ഉണ്ടാക്കാനുള്ള പല ശ്രമങ്ങളും നമ്മുടെ തലച്ചോറിന്റെ പ്രവർത്തനത്തിൽനിന്നും പ്രചോദനം ഉൾക്കൊണ്ടാണ്. ഇത്തരം ഗവേഷണശാഖകളെ ആർട്ടിഫിഷ്യൽ ന്യൂറൽ നെറ്റ്‌വർക്കുകൾ എന്നാണ് വിളിക്കുന്നത്. ന്യൂറൽ നെറ്റ്‌വർക്കുകൾ എന്ന പദം ഈ സീരിസിൽ ഇനി ഒത്തിരിതവണ ആവർത്തിക്കപ്പെടുന്നതായതുകൊണ്ടാണ് ഇത്രവലിയൊരു ആമുഖം.\nനമ്മുടെ ശരീരത്തിലെയെന്നപോലെ ഒന്നിലധികം ലേയേറുകളുള്ള, സിനാപ്സുകളിലെ കൈമാറ്റത്തിൽ സംഭവിക്കുന്ന മാറ്റങ്ങളെ ഗണിതശാസ്ത്രപരമായി approximate ചെയ്യുന്ന മോഡലുകൾ ഇന്ന് വികസിപ്പിക്കപ്പെട്ടിട്ടുണ്ട്. അതിനെ deep neural networks എന്നാണു വിളിക്കുന്നത്. ഒന്നിലധികം എന്നാൽ 6 മുതൽ 150 വരെ (അതിലും deep ആയിട്ടുള്ളവ ഉണ്ടാകാം) ലേയറുകൾ ഉള്ളവയായതിനാലാണ് deep എന്ന് വിളിക്കുന്നത്. ഇന്ന് ഏറ്റവും പ്രചാരത്തിലിരിക്കുന്ന, ഓരോ ദിവസവും അതിവേഗം വളരുന്ന ഗവേഷണമേഖലയാണ് deep neural networks (DNN) അഥവാ deep learning. കഴിഞ്ഞ 3-4 വർഷങ്ങൾക്കിടയിൽ നമ്മൾ ടെക്നോളജി കൂടുതലായി നേരിട്ടു ഉപയോഗിക്കുകയും, അതിന്റെ user experience ഇൽ അഭൂതപൂർവമായ പുരോഗതി ഉണ്ടാവുകയും ചെയ്തിട്ടുണ്ട്. ഉദാഹരണത്തിന്, translate, google search autocomplete, photo-tagging, relevant search, അങ്ങനെ പലതും നാം ദൈനംദിനമെന്നോണം ഉപയോഗിക്കുന്നുണ്ട്. ആ വളർച്ചക്കെല്ലാം പിന്നിൽ ഒരൊറ്റ പേരാണ്. DNN !!\n***********************\nDeep nueral networks, deep learning, multi-layered nueral networks തുടങ്ങിയ പദങ്ങളെ അവതരിപ്പിക്കാനാണ് ഈ പോസ്റ്റിൽ ശ്രമിച്ചിരിക്കുന്നത്. മുന്നോട്ടുള്ള ഈ സീരിസിന്റെ വായനയിൽ ഈ പോസ്റ്റിൽ പറഞ്ഞിരിക്കുന്ന പദങ്ങൾ പലതും ധാരാളം തവണ ആവർത്തിക്കപ്പെടും. അതുകൊണ്ടു ഈ പോസ്റ്റ് മനസിലാക്കാതെ അടുത്ത ഭാഗങ്ങളിലേക്ക് കടക്കാനാവില്ല. സംശയങ്ങൾ തീർച്ചയായും കമന്റിൽ ചോദിക്കുമല്ലോ.\nAI ചരിത്രം അടുത്ത ഭാഗത്തിൽ തുടരും. മുൻപ് പറഞ്ഞതുപോലെ, ചരിത്രത്തിൽ ഇനി 2 ഭാഗങ്ങൾ ഉണ്ടാകും. അതിൽ ആദ്യഭാഗം deep neural networks (DNN) ന്റെ ആദ്യകാലവികാസങ്ങളും അതെങ്ങനെ, എന്തുകൊണ്ട് 1980കളോടെ അവസാനിക്കപ്പെട്ടു എന്നുള്ളതുമാണ്. രണ്ടാം ഭാഗത്തിൽ deep neural network ന്റെ 2006 ഇലെ രണ്ടാം വരവിനെപ്പറ്റിയുമാണ്. ആ രണ്ടാം വരവിലാണ് ഇന്നുകാണുന്ന DNN ന്റെ വളർച്ചയും അതുവഴി AI യുടെയും നമ്മുടെ ടെക്നോളജിയുടെയും നമ്മുടെപോലും തലവര മാറ്റി എഴുതപ്പെടുന്നതും.\n","date":"2018-04-16T00:00:00Z","permalink":"https://deepakbaby.in/posts/ai-3/","tags":["ai","malayalam"],"title":"ആർട്ടിഫിഷ്യൽ ഇന്റലിജൻസ് സീരീസ്: ഭാഗം - 3"},{"categories":[],"contents":"യന്ത്രങ്ങൾ കണ്ടുപിടിച്ച കാലം മുതൽക്കേ മനുഷ്യനെ ത്രസിപ്പിച്ചിരുന്ന ആശയമായിരുന്നു സ്വയബുദ്ധിയോടെ പ്രവർത്തിക്കുന്ന യന്ത്രങ്ങൾ. കുറെ യന്ത്രഭാഗങ്ങളുടെ ചലനത്തെമാത്രം അടിസ്ഥാനമാക്കി പ്രവർത്തിച്ചിരുന്ന യന്ത്രങ്ങളിൽ നിന്ന് ഇന്നു നമ്മുടെ സാങ്കേതികവിദ്യ വളരെയേറെ മുന്നോട്ടുപോയിരിക്കുന്നു. ആ വഴിയിലെ ചില പ്രധാനപ്പെട്ട കണ്ടുപിടുത്തങ്ങളാണ് ഈ ഭാഗത്തിൽ.\nഈ ചരിത്രത്തെ മൂന്ന് ഘട്ടങ്ങളായി തിരിക്കാം. കംപ്യൂട്ടർ എന്ന മെഷിൻ നിര്മിക്കപ്പെടുന്നതിനുമുമ്പ് ഇങ്ങനെയൊരു മഷിന്റെ സാധ്യതകളെക്കുറിച്ചു ചില സിദ്ധാന്തങ്ങൾ അവതരിപ്പിക്കപ്പെട്ടു. ഇന്നത്തെ കംപ്യൂട്ടറുകൾ പലതും അത്തരം സിദ്ധാന്തങ്ങൾ അടിസ്ഥാനമാക്കി നിർമിക്കപ്പെട്ടവയാണ്. 1950 നു മുൻപുള്ള ആ കാലഘട്ടത്തിലെ പ്രധാന നാഴികക്കല്ലുകളാണ് ഈ ഭാഗത്തിൽ. സിദ്ധാന്തങ്ങളിൽ നിന്നും കമ്പ്യൂട്ടർ എന്ന മെഷിൻ എങ്ങനെയാണ് യാഥാർഥ്യമായതു എന്നതും അതിനോടൊപ്പം മെഷീൻ ലേർണിംഗ്/ കൃത്രിമബുദ്ധി ഉണ്ടാക്കാനുള്ള ശ്രമങ്ങളുമാണ് രണ്ടാം ഭാഗം. അത് 1980 കളോടെ അവസാനിക്കും. പിന്നീട് AI ഗവേഷണം ഉയർത്തെഴുന്നേൽക്കുന്നത് 2006 ലാണ്. അന്നുതൊട്ടുള്ള ചെറുചരിത്രവും അതിനു ചുക്കാൻപിടിച്ച ഇന്നത്തെ പ്രമുഖരായ ഗവേഷകരെപ്പറ്റിയുമായിരിക്കും മൂന്നാം ഭാഗം. 1642 : Pascal’s Calculator\n1642, ഈ വർഷമാണ് ബ്ലേയ്‌സ് പാസ്കൽ എന്ന ഇരുപതുകാരൻ മെക്കാനിക്കൽ കാൽക്കുലേറ്റർ കണ്ടുപിടിക്കുന്നത്. കുറെ ഗിയറുകളും വീലുകളും ഉപയോഗിച്ച്‌ രണ്ടു സംഖ്യകൾ കൂട്ടാനും കുറക്കാനും കഴിയുന്ന യന്ത്രമായിരുന്നു ആദ്യത്തെ കാൽക്കുലേറ്റർ. ടാക്സ് കമ്മീഷനായിരുന്ന തന്റെ പിതാവിനു ജോലിഭാരം കുറയ്ക്കാനാണ് പാസ്കൽ ഇത്തരമൊരു യന്ത്രം നിർമിച്ചത്. ഒരു മെക്കാനിക്കൽ കാല്കുലേറ്ററിനു മെഷീൻ ലേർണിംഗുമായി എന്തുബന്ധം എന്ന് നിങ്ങള്ക്ക് സംശയം തോന്നിയേക്കാം. ഒരു പക്ഷെ, ഡാറ്റ ഓട്ടോമാറ്റിക് ആയി പ്രോസസ്സ് ചെയ്യാൻ മനുഷ്യൻ കണ്ടുപിടിച്ച ആദ്യത്തെ യന്ത്രമെന്ന നിലക്കാണ് ഈ കാല്കുലേറ്ററിനു പ്രാധാന്യം. (കൂടുതൽ വായനക്ക് : Pascal\u0026rsquo;s Calculator)\n1801 : Punched Cards : Data storage ഇന്നത്തെ മെഷീൻ ലേർണിംഗിന് ധാരാളം ട്രെയിനിങ് ഡാറ്റ വേണമെന്ന് കഴിഞ്ഞ ഭാഗത്തിൽ പറഞ്ഞിരുന്നുവല്ലോ. കംപ്യുട്ടറിന്റെ വളർച്ചയിലെ ഏറ്റവും പ്രധാനപ്പെട്ട നാഴികക്കല്ലായിരുന്നു punched cards ഉപയോഗപ്പെടുത്തിയുള്ള ആദ്യത്തെ ഡാറ്റ സ്റ്റോറേജ് ഡിവൈസ്. അത് കണ്ടുപിടിച്ചതാകട്ടെ ഒരു കൈത്തറിയിൽ ഒരേ പാറ്റേണിലുള്ള വസ്ത്രങ്ങൾ നെയ്യാനും! ഫ്രഞ്ചുകാരനായിരുന്ന ജോസഫ് മേരി ജാക്ക്വാർഡ് എന്ന കൈത്തറിമുതലാളിയാണ് ഈ കണ്ടുപിടിത്തത്തിനു പിന്നിൽ. ഇവ ജാക്ക്വാർഡ് കൈത്തറികൾ (Jacquard Loom) എന്നാണു അറിയപ്പെടുന്നത്. ഒരു മെറ്റൽ ഷീറ്റിൽ നമുക്കു വേണ്ട നെയ്ത്തുപാറ്റേണിന് അനുസരിച്ചു കുറെ തുളകൾ ഇട്ടാണ് അദ്ദേഹം punched cards ഉണ്ടാക്കിയത് (ചിത്രം കാണുക). ആ തുളകളിലൂടെ ഓരോ നിറത്തിലുള്ള നൂലുകൾ കടത്തിവിടുമ്പോൾ നമുക്ക് വേണ്ട പാറ്റേൺ കിറുകൃത്യമായി വീണ്ടും വീണ്ടും ഉണ്ടാക്കാൻ സാധിക്കും. ഓരോ ഡിസൈനിനും അനുസരിച്ച്‌ പുതിയ punched cards ഉണ്ടാക്കും. ഇതെങ്ങനെയാണ് ഡാറ്റ സ്റ്റോറേജ് ആകുന്നതെന്നാവും നിങ്ങളുടെ സംശയം. ഒരേ പാറ്റേണുകളെ പ്രത്യേകരീതിയിൽ സൂക്ഷിക്കുകയായിരുന്നു ആ തുളകൾ! ആദ്യകാലത്തെ കമ്പ്യൂട്ടറുകളിൽ ഇതിന്റെ പരിഷ്കരിച്ച വകഭേദമാണ് ഡാറ്റ സൂക്ഷിക്കാൻ ഉപയോഗപ്പെടുത്തിയിരുന്നത്. (കൂടുതൽ വായനക്ക്: Jacquard\u0026rsquo;s Loom)\n1837: Charles Babbage: The father of computers പാസ്കലിന്റെ മെക്കാനിക്കൽ കാല്കുലേറ്ററും ജാക്ക്വാഡിന്റെ punched cards ഉം ഉപയോഗിച്ച്, കമ്പ്യൂട്ടർ സയൻസിന്റെ പിതാവെന്നറിയപ്പെടുന്ന ചാൾസ് ബാബേജ് കംപ്യൂട്ടറിന്റെ ആദിമരൂപം നിർമിക്കാൻ ശ്രമം തുടങ്ങി. Analytical Engine എന്നറിയപ്പെട്ടിരുന്ന ഈ മെക്കാനിക്കൽ കമ്പ്യൂട്ടറിനു ഇന്നത്തെ കമ്പ്യൂട്ടറിൽ കാണുന്ന arithmetic logic unit (ALU), control flow തുടങ്ങിയവയും ഉണ്ടായിരുന്നു. പ്രധാനമായും സങ്കലനം, വ്യവകലനം, ഗുണനം, ഹരണം അടിസ്ഥാന ഗണിതക്രിയകൾ മാത്രം ചെയ്യാൻ പര്യാപ്തമായ ഒരു മെക്കാനിക്കൽ കംപ്യൂറ്ററായിരുന്നു analytical engine. നമുക്ക്‌ ചെയ്യണ്ട ഗണിതക്രിയയും (+, - , *, /) സംഖ്യകളും punched cards ഉപയോഗിച്ച് ഇൻപുട്ടായി സ്വീകരിക്കുകയും അവയുടെ ഉത്തരം കാണിക്കാൻ ഔട്ട്പുട്ടിന് ഒരു പ്രിന്ററും ബെല്ലുമാണ് ഉപയോഗിച്ചിരുന്നത്. ഇന്നത്തെ കാലത്തെ മോണിറ്ററുകൾ അന്നില്ലല്ലോ. ആശയം ഗംഭീരമായിരുന്നെങ്കിലും അനലിറ്റിക്കൽ എൻജിൻ നിർമിക്കാൻ ബാബേജിനായില്ല. മേലധികാരിയുമായുള്ള പ്രശ്നങ്ങളും ഫണ്ടിങ്ങിന്റെ അഭാവവുമായിരുന്നു കാരണം. എന്നിരുന്നാലും ഇന്നത്തെ ഇലക്ട്രോണിക് യുഗത്തിലെ കംപ്യൂട്ടറുകളെല്ലാം ബാബേജ് മുന്നോട്ടുവച്ച ആശയങ്ങളെ അടിസ്ഥാനപ്പെടുത്തിയാണ് നിർമിച്ചിരിക്കുന്നത്‌. (കൂടുതൽ വായനക്ക്: Anlaytical Engine)\n**1842: Ada Lovelace : First Computer programmer **\nഒരു സാധാരണ കാൽക്കുലേറ്റർ എന്നതിലുപരി ഒരു കമ്പ്യൂട്ടറിനു അതിലും സങ്കീർണമായ പലതും ചെയ്യാനാവുമെന്നു ഇംഗ്ലീഷ് ഗണിതശാസ്ത്രജ്ഞയായിരുന്ന അഡ ലവ്ലെയ്സ് തിരിച്ചറിഞ്ഞു. ഇതിനായി ഓരോ ടാസ്കിനും കമ്പ്യൂട്ടർ എതൊക്കെ സ്റ്റെപ്പുകൾ എടുക്കണമെന്ന് കാണിക്കുന്ന അൽഗോരിതമെന്ന ആശയം അവർ മുന്നോട്ടുവച്ചു. ഉദാഹരണത്തിന് മ്യൂസിക് നോട്ടുകളെ അടയാളപ്പെടുത്താൻ സംഖ്യകളെ ഉപയോഗിച്ചാൽ, ഒരു കമ്പ്യൂട്ടറിന് ആ മ്യൂസിക്കിനെ പ്രോസസ്സ് ചെയ്യാൻ സാധിക്കുമെന്നു അവർ കണ്ടെത്തി. ഒരുപക്ഷേ അനലിറ്റിക്കൽ എൻജിന്റെ സൃഷ്ടാവായ ചാൾസ് ബാബേജ്പോലും തിരിച്ചറിയാത്ത ആശയം. ഇന്നത്തെ ഓരോ കംപ്യൂട്ടർ പ്രോഗ്രാമും ഓരോ അൽഗോരിതങ്ങൾ ഉപയോഗിച്ചാണ് പ്രവർത്തിക്കുന്നത്. ഡാറ്റയെ, അത് ഫോട്ടോ ആകട്ടെ, മ്യൂസിക് ആകട്ടെ, letters ആകട്ടെ, കമ്പ്യൂട്ടറുകൾ കാണുന്നത് കുറെ സംഖ്യകൾ ആയിട്ടാണ്. ഉദാഹരണം പറഞ്ഞാൽ, ഫോട്ടോ എഡിറ്റിംഗ് ഇൽ brightness കൂട്ടുമ്പോൾ, നിങ്ങളുടെ ഭംഗി കൂട്ടുമ്പോൾ, enlarge ചെയ്യുമ്പോളെല്ലാം സംഖ്യകളായി സൂക്ഷിക്കപ്പെട്ടിരിക്കുന്ന നിങ്ങളുടെ ഫോട്ടോയിലെ സംഖ്യകൾ ചില അൽഗോരിതങ്ങൾക്കനുസരിച്ചു മാറ്റപ്പെടുകയാണ് ചെയ്യുന്നത്. ഇതാണ് 160 വർഷങ്ങൾക്കുമുൻപ് അഡ ലവ്ലെയ്സ് എന്ന ആദ്യത്തെ കമ്പ്യൂട്ടർ പ്രോഗ്രാമർ മുന്നോട്ടുവച്ച ആശയവും. (കൂടുതൽ വായനക്ക്: Ada Lovelace)\n**1890 Census statistics calculator **\nUS സെൻസസ് ഡാറ്റയിൽ നിന്ന് പലതരം സ്റ്റാറ്റിസ്റ്റിക്‌സ് കണക്കാക്കാൻ ഉപയോഗിച്ചിരുന്ന data tabulating machine. ഹെർമൻ ഹോളറിത് കണ്ടുപിടിച്ച ഈ ഉപകരണം 1890-ഇലെ US സെൻസസ് ഡാറ്റ അനലൈസ് ചെയാനാണ് പ്രധാനമായും ഉപയോഗിച്ചത്. ഇന്നത്തെ IBM-ഇന്റെ മാതൃകമ്പനിയുടെ സ്ഥാപകനാണ് ഇദ്ദേഹം.\n1927: First Robot on silver screen\nആദ്യമായി സ്വയം ചിന്തിക്കുന്ന ഒരു റോബോട്ട് സിനിമയിൽ പ്രത്യക്ഷപ്പെട്ടു. സിനിമയിലെ ഭാവനകളാണ് പല കണ്ടുപിടുത്തങ്ങൾക്കും കാരണമായിട്ടുള്ളത്. അല്ലെങ്കിലും നമുക്കു സിനിമ വിട്ടൊരു കളിയുമില്ല. Sci-Fi ഫിക്ഷൻ എന്ന ജോണർ തന്നെയുണ്ടല്ലോ. Metropolis എന്ന ചിത്രത്തിൽ False Maria എന്ന റോബോട്ട് ക്യാരക്ടർ പക്ഷെ ആളുകളെ തമ്മിലടിപ്പിക്കുന്ന ഒരു വില്ലൻ കഥാപാത്രമായിരുന്നു.\n**1936: Alan Turing’s Universal Machine **\nഓരോ ടാസ്ക് ചെയ്യുന്നതിനും നമ്മൾ മനുഷ്യർ ഓരോ രീതി പിന്തുടരുന്നുണ്ട്. step-by-step ആയിട്ടാണ് നമ്മൾ സങ്കീർണമായ പലജോലികളും പൂർത്തിയാക്കുന്നത്. അതുപോലെ മെഷിനുകൾക്കും പല സങ്കീർണമായ ജോലികളും ചെയ്യാൻ സാധിക്കുമെന്ന് ഇംഗ്ലീഷ് ഗണിതജ്ഞനും cryptologist -മായ അലൻ ടൂറിംഗ് 1936 ഇൽ ഒരു സിദ്ധാന്തം അവതരിപ്പിച്ചു. ഈ സിദ്ധാന്തമാണ് ആധൂനിക കമ്പ്യൂട്ടർ സയൻസിന്റെ അടിസ്ഥാനമായി കണക്കാക്കുന്നത്. (കൂടുതൽ വായനക്ക്:Turing Machine)\n_______________________________________________________________________\nകമ്പ്യൂട്ടർ സയൻസിന്റെ വളർച്ചയുടെ ആദ്യഘട്ടങ്ങളും അടിസ്ഥാനതത്വങ്ങളും സിദ്ധാന്തങ്ങളുമാണ് ഈ പോസ്റ്റിൽ പറയാനുദ്ദേശിച്ചത്. ഇത്തരം സിദ്ധാന്തങ്ങളിൽ നിന്നും കമ്പ്യൂട്ടർ എന്ന, ഇന്നു ദൈനംദിനം ഉപയോഗത്തിലിരിക്കുന്ന മെഷിൻ എന്ന യാഥാർഥ്യത്തിലേക്കുള്ള യാത്രയാണ് അടുത്ത ഭാഗത്തിൽ.\nഈ ലിസ്റ്റ് തീർച്ചയായും അപൂർണ്ണമാണ്‌. 1950 വരെയുള്ള കണ്ടുപിടുത്തങ്ങളാണ് ഈ ഭാഗത്തിൽ ഉദ്ദേശിക്കുന്നത്. വിട്ടുപോയവ വായനക്കാർ പറയുന്നതിന് അനുസരിച്ചു ഇവിടെ എഡിറ്റ് ചെയ്യാം.\n-ജോർജ് ബൂളിന്റെ Boolean algebra. ബൈനറി സംഖ്യകൾ വച്ചുള്ള ഗേറ്റ് ഓപ്പറേഷനുകൾ (NOT, AND, OR Gates തുടങ്ങിയവ ).\n__________________________________________________________________________\nവാൽ: മഹാഭാരതകാലത്തെ ഇന്റർനെറ്റും, മെഷിൻ ലേണിങ്ങിനു ഏറ്റവും അനുയോജ്യമായ പ്രോഗ്രാമിങ് ലാംഗ്വേജ് ആയ സംസ്കൃതവും തുടങ്ങി പുരാതനഭാരതത്തിലെ സാങ്കേതികവിദ്യകളൊന്നുംതന്നെ ഈ സീരിസിലെ ചരിത്രത്തിൽ ഉൾപ്പെടുത്തിയിട്ടില്ല. അതിനെപ്പറ്റി പറയാൻ നമുക്ക് മന്ത്രിമാരും \u0026lsquo;പരിവാര\u0026rsquo;ങ്ങളുമുള്ളതുകൊണ്ടാണ്. ആർഷഭാരതസ്നേഹികൾ ക്ഷമിക്കുമല്ലോ.\n_________________\nImage sources : Wikipaedia\n","date":"2018-04-08T00:00:00Z","permalink":"https://deepakbaby.in/posts/ai-2/","tags":["ai","malayalam"],"title":"ആർട്ടിഫിഷ്യൽ ഇന്റലിജൻസ് സീരീസ്: ഭാഗം - 2"},{"categories":[],"contents":"\nകേംബ്രിഡ്ജ് അനാലിറ്റിക്കയും അതുവഴി ഫേസ്ബുക് പിടിച്ച പുലിവാലുമൊക്കെ എല്ലാവരും അറിഞ്ഞിരിക്കുമല്ലോ. ഉപയോക്താക്കളുടെ വിവരങ്ങൾ ചോർത്തി, ആ വിവരങ്ങൾ ഉപയോഗിച്ച് നമ്മുടെ ചിന്തകളെ സ്വാധീനിക്കുന്ന തരം പോസ്റ്റുകൾ നമ്മുടെ ന്യൂസ് ഫീഡിലേക്ക് കടത്തിവിടുകയാണ് കേംബ്രിഡ്ജ് അനാലിറ്റിക്ക ചെയ്തതെന്നും പലരും വായിച്ചിരിക്കും. എന്നാൽ എങ്ങനെയാണ് ഒരാളുടെ വിവരങ്ങളിൽ നിന്നും ഇതെല്ലാം മനസിലാക്കി, എന്തുതരം പോസ്റ്റുകൾ ഇടണം എന്ന തീരുമാനം എടുക്കുന്നതെന്നു പലർക്കും മനസിലായിട്ടുണ്ടാവില്ല. ഇത്രയധികം ഉപയോക്താക്കളുടെ ഡാറ്റ പരിശോധിച്ച് അവരുടെ അഭിരുചികൾ മനസിലാക്കി കൃത്യമായ പോസ്റ്റുകൾ കടത്തിവിടാൻ ഒരു മനുഷ്യനെക്കൊണ്ടു സാധിക്കില്ലെന്നുറപ്പ്. അപ്പോൾ പിന്നെ അത് കമ്പ്യൂട്ടർ തന്നെ.\nഎന്നാലും കമ്പ്യൂട്ടർ ഒരു മെഷിനല്ലേ. അതിനു ഇത്തരത്തിലൊരു കഴിവുണ്ടോ ? കംപ്യൂട്ടറുകൾ സത്യത്തിൽ വെറും മണ്ടന്മാരാണ്. അതിനു ആകെക്കൂടെ കുറെ സംഖ്യകളെ കൂട്ടാനും കുറക്കാനും ഗുണിക്കാനും ഹരിക്കാനും അറിയാം.. നമ്മൾ മനുഷ്യരെപോലെ പഞ്ചേന്ദ്രിയങ്ങളോ അവയിൽനിന്നു വരുന്ന വിവരങ്ങളെ ഏകോപിപ്പിക്കുന്ന ഒരു തലച്ചോറോ ഇല്ല. നമ്മുടെ വിവരങ്ങളെല്ലാം കമ്പ്യൂട്ടറുകൾ കാണുന്നത് സംഖ്യകൾ ആയിട്ടാണ്. എല്ലാവര്ക്കും ബൈനറി നമ്പർ സിസ്റ്റം അറിയാമെന്നു കരുതുന്നു. കമ്പ്യൂട്ടറിൽ എല്ലാം 1 അല്ലെങ്കിൽ 0 ആയിട്ടാണ് എല്ലാം ശേഖരിച്ചുവച്ചിരിക്കുന്നത്. ഇങ്ങനെയുള്ള കുറെ ഒന്നുകളിൽ നിന്നും പൂജ്യങ്ങളിൽ നിന്നും കമ്പ്യൂട്ടറിനെ ഒരു തീരുമാനം എടുക്കാൻ പഠിപ്പിക്കുന്ന ശാസ്ത്രശാഖയാണ് ആർട്ടിഫിഷ്യൽ ഇന്റലിജൻസ് അഥവാ മെഷീൻ ലേർണിംഗ്.\nഉദാഹരണത്തിന്, നിങ്ങൾ ഫേസ്ബുക്കിൽ ഒരു ഫോട്ടോ ഇടുമ്പോൾ ഉടനെ ടാഗ് ചെയ്യാനുള്ള ഓപ്‌ഷൻസ് വരും. നിങ്ങളുടെയും നിങ്ങളുടെ സുഹൃത്തുക്കളുടെയും ഒക്കെ സജഷൻസ് വരും. ആദ്യമൊക്കെ ഇത്തരം സജഷൻസ് തെറ്റായിരിക്കും. നിങ്ങൾ അത് ശരിയാക്കുമ്പോൾ ഫേസ്ബുക്കിന് നിങ്ങളുടെ മുഖമുള്ള ഒരു ഫോട്ടോ കിട്ടും. ഓരോ തവണ പുതിയ ഫോട്ടോ ഇടുംതോറും സജഷൻസ് മെച്ചപ്പെടും. കാരണം നിങ്ങളുടെ പല പോസിൽ പല ആംഗിളുകളിലുള്ള ഫോട്ടോകൾ ഫേസ്ബുക്കിന് അല്ലെങ്കിൽ ഫേസ്ബുക് മെഷിനു അറിയാം. ഈ ഫോട്ടോകൾ വച്ച് ഇത് നിങ്ങളുടെ ഫോട്ടോയാണെന്നു പറഞ്ഞു മെഷിനെ പഠിപ്പിക്കുകയാണ് ചെയ്യുന്നത്. ഒരു കുഞ്ഞിനെ ഒരു പൂവുകാണിച്ചു അത് പൂവാണെന്നു പറഞ്ഞു പഠിപ്പിക്കുന്നതുപോലെ. ഇതിനു ട്രെയിനിങ് എന്ന് പറയും. നമ്മൾ അപ്ലോഡ് ചെയ്യുന്ന ഓരോ ഫോട്ടോയും ട്രെയിനിങ് എക്‌സാംപിളുകളാണ്. നമ്മൾ കൊടുക്കുന്ന ടാഗിംഗ് ഇൻഫർമേഷൻ ആണ് ഫേസ്ബുക് അത് ആരുടെയാണെന്നു തിരിച്ചറിയാൻ ഉപയോഗിക്കുന്നത്.. അതിനെ നമ്മൾ ലേബലിംഗ് എന്ന് വിളിക്കും. ഇത്തരത്തിൽ ലേബൽ ചെയ്യപ്പെട്ടിട്ടുള്ള ട്രെയിനിങ് ഡാറ്റായാണ് മെഷിൻ ലേർണിംഗിനു ആവശ്യം വേണ്ടത്.\nമുകളിൽ പറഞ്ഞത് മെഷീൻ ലേണിങിന്റെ ഏറ്റവും സിമ്പിൾ ആയ ഒരുദാഹരണമാണ്. പണ്ടൊക്കെ സയൻസ് ഫിക്ഷൻ സിനിമകളിൽ മാത്രം കണ്ടുശീലിച്ചിരുന്ന പലതും ചെയ്യാൻ ഇന്ന് മെഷിനുകൾ പ്രാപ്തരാണ്. നമുക്കിന്നു മഷിനുകളോട് സംസാരിക്കാം (automatic speech recognition), നമ്മുടെ ചിത്രങ്ങൾ വേർതിരിക്കാം (image classification), വിവിധ ഭാഷകൾ translate ചെയ്യാം (machine translation), എന്തിന്, നമ്മളോട് ചാറ്റ് ചെയ്യാൻ പോലും ഉപയോഗിക്കാം (chat bots). ആർട്ടിഫിഷ്യൽ ഇന്റലിജൻസ് ഉപയോഗിച്ചു കമ്പ്യൂട്ടറിനു സ്വന്തമായി ചിത്രം വരക്കാനും സംഗീതം ഉണ്ടാക്കാനുമെല്ലാം കഴിയും. സോഷ്യൽ മീഡിയകളും സ്മാർട്ഫോണുകളും മെച്ചപ്പെട്ട സാങ്കേതികവിദ്യകളുമെല്ലാം ഇന്നത്തെ ഈ വളർച്ചക്കുപിന്നിലുണ്ട്.\nഅപ്പോൾ നല്ല കൃത്രിമബുദ്ധിയുണ്ടാക്കാൻ വേണ്ടത് ഡാറ്റയാണ്. കൃത്യമായ ലേബലിംഗ് ഉള്ള നല്ല ട്രെയിനിങ് ഡാറ്റ. ഡാറ്റയുള്ളവനാണ് രാജാവ്. മനുഷ്യർ മണിക്കൂറുകളോളം എടുത്തുചെയ്തിരുന്ന പലജോലികളും ഇന്ന് കമ്പ്യൂട്ടറുകൾ അതിവേഗം ഏറ്റെടുത്തുകൊണ്ടിരിക്കുകയാണ്. മെഷിൻ ലേർണിംഗ് അഥവാ ബിഗ് ഡാറ്റ അഥവാ ഡാറ്റ സയൻസ് ഇന്ന് ഏറ്റവും ഡിമാന്റുള്ള ഫീൽഡുകളിലൊന്നാണ്. ഇന്ത്യയിൽ ഈ ഏരിയായിൽ അധികം ഗവേഷണമൊന്നും നടക്കുന്നില്ല. പക്ഷേ ഓൺലൈനിൽ ധാരാളം കോഴ്‌സുകളും മെഷീൻ ലേർണിംഗ് ഉദാഹരണങ്ങളും ലഭ്യമാണ്. എഞ്ചിനീയറിംഗ് രംഗത്ത്, പ്രത്യേകിച്ച് ഇലെക്ട്രിക്കൽ, കമ്പ്യൂട്ടർ സയൻസ്, ഇൻഫർമേഷൻ ടെക്നോളജി, മാത്തമാറ്റിക്സ്, ഫിസിക്സ് മേഖലയിലുള്ളവർ അത്യാവശ്യമായി അറിഞ്ഞിരിക്കേണ്ട ഒന്നാണ് ഡാറ്റാ സയൻസ്. വലിയ കമ്പനികളെല്ലാം ഒന്നിനുപുറകെ ഒന്നായി കോടികളാണ് ഈ മേഖലയിൽ ഇൻവെസ്റ്റ് ചെയ്യുന്നത്. അതിനാൽത്തന്നെ ഡാറ്റ സയന്റിസ്റ്റുകൾക്കു നല്ല ഡിമാൻഡാണ്.\nനമ്മുടെ ഡാറ്റകളെല്ലാം സംഖ്യകളായി മാത്രം കാണുന്ന, സ്വന്തമായി യാതൊരു ബുദ്ധിയുമില്ലാത്ത കംപ്യുട്ടറുകൾക്ക് ഇത്തരം കൃത്രിമബുദ്ധി ഉണ്ടാക്കുന്നതിനുപിന്നിൽ ഗണിതശാസ്ത്രത്തിലെ തീർത്തും സങ്കീർണമായ പല തത്വങ്ങളും ഉപയോഗിച്ചിട്ടുണ്ട്. ഇത്തരം സാങ്കേതികവിദ്യകളെ കഴിയുന്നതും ലളിതമായി അവതരിപ്പിക്കാൻ ശ്രമിക്കുകയാണ് ഈ സീരിസിൽ.\nധാരാളം ഗണിതവും ടെക്നിക്കൽ വാക്കുകളുമെല്ലാമുള്ള ഒരു മേഖലയെ മലയാളത്തിൽ ലളിതമായി അവതരിപ്പിക്കുക എന്നത് എളുപ്പമല്ല. ജോലിക്കിടയിലാണ് എഴുതാൻ സമയം കണ്ടെത്തുന്നത്. ആഴ്ചയിൽ ഒരെണ്ണമെങ്കിലും വച്ച് എഴുതാനാണു ശ്രമിക്കുന്നത്. എഴുത്തിലും തിരുത്തിലും ചിലരുടെ സഹായവും പ്രതീക്ഷിക്കുന്നുണ്ട്. ഈ സീരീസിലേക്കു എഴുതാൻ താല്പര്യമുള്ള ഗ്രൂപ്പ് മെമ്പേഴ്സിന്റെയും സഹകരണം പ്രതീക്ഷിക്കുന്നു. !\nഇപ്പോൾ നിലവിലുള്ള ടെക്‌നിക്കുകളെ പരിചയപ്പെടുത്തുകയും കൂടുതൽ അറിയാൻ താല്പര്യമുള്ളവർക്ക് അതിനുപിന്നിലുള്ള ഗണിതവും (ലളിതമാക്കാൻ ശ്രമിക്കാം) ചെറുതായി വിവരിക്കാനാണ് ഉദ്ദേശിക്കുന്നത്.\nഅപ്പോൾ അടുത്ത പോസ്റ്റിൽ അല്പം ചരിത്രമാവാം. താല്പര്യമുള്ളവർക്ക് ചെയ്യാൻ പറ്റിയ ഓൺലൈൻ കോഴ്‌സുകളും പ്രോഗ്രാമിങിന് സഹായിക്കുന്ന ടൂൾകിറ്റുകളുമെല്ലാം വഴിയേ പരിചയപ്പെടുത്താം.\n","date":"2018-04-01T00:00:00Z","permalink":"https://deepakbaby.in/posts/ai-1/","tags":["ai","malayalam"],"title":"മെഷീൻ ലേണിങ്/ ആർട്ടിഫിഷ്യൽ ഇന്റലിജൻസ് സീരീസ്"},{"categories":[],"contents":" What does this guide offer? Software packages and OS Install a linux OS Openvpn Matlab Miscellaneous Configuring printer Before using apollo webpage Mounting Intec file share What does this guide offer? This report aims as a quick guide for setting up linux machines in Intec, UGent. It may not contain help on everything, but the goal is to keep adding things as we encounter and solve them. The instructions are for fedora 25, unless otherwise mentioned. For other linux distributions, you might need to appropriately change the command-line instructions.\nYou might encounter missing libraries while installing various packages. This report may not give solutions to such errors. It is suggested to find the right library for the OS you use and solve the dependencies yourself. Some sections might contain a subsubsection on possible issues and fixes.\nSoftware packages and OS Install a linux OS The Intec machines typically come with pre-installed windows 10 (or latest release). Installing linux with dual boot option is preferred on these machines. Choose your linux OS and follow the install instructions provided by the respective OS manuals to install with dual boot. This report does not include the installation instructions for dual boot linux with windows.\nAlso do not forget to enable GRUB bootloader or edit the windows bootloader easybcd is a good software to do this, but should be done from windows) so that we can see the dual boot menu on bootup.\nOpenvpn We also need to set-up an VPN connection to be a part of the Intec network. For this, you will need an intec account and password (it is not the ugent account). You will receive an email about this or you might need to request the Tech people at Intec. Once you have the account, you can set up the VPN. You will receive an email with instructions for setting up VPN for windows. The mail will also have a .rar file which contains a .ovpn and a .crt file.\nFor linux, download the .tar.gz file from this page. Open a terminal and go to the folder where openvpn is downloaded. You might need to use these commands with sudo.\n$ tar xzvf openvpn\u0026lt;version\u0026gt;.tar.gz $ cd openvpn\u0026lt;version\u0026gt; $ ./configure $ make $ make install You might need to install make or cmake for your linux OS. Also there could be some missing libraries. Install them and do ./configure until there are no errors, then do make followed by make install. This post may be updated with instructions for installing the missing libraries.\nOnce the openvpn is installed, you can set up the VPN using the .ovpn and a .crt files. From the terminal, go to the folder where these files are located. Then execute :\n$ sudo openvpn --config \u0026lt;.ovpn file\u0026gt; --ca \u0026lt;.crt file\u0026gt; This will ask your Intec username and password for authentication (Don\u0026rsquo;t give ugent credentials !). This should give an output similar to:\n[sudo] password for \u0026lt;user\u0026gt;: OpenVPN 2.4.0 x86_64-redhat-linux-gnu [SSL (OpenSSL)] [LZO] [LZ4] [EPOLL] [MH/PKTINFO] [AEAD] built on \u0026lt;date\u0026gt; library versions: OpenSSL 1.0.2k-fips 26 Jan 2017, LZO 2.08 Enter Auth Username:\u0026lt;Enter Intec username\u0026gt; Enter Auth Password:\u0026lt;Enter Intec password\u0026gt; TCP/UDP: Preserving recently used remote address: [AF_INET]157.193.214.20:443 Attempting to establish TCP connection with [AF_INET]157.193.214.20:443 [nonblock] TCP connection established with [AF_INET]157.193.214.20:443 TCP_CLIENT link local: (not bound) TCP_CLIENT link remote: [AF_INET]157.193.214.20:443 TUN/TAP device tun0 opened do_ifconfig, tt-\u0026gt;did_ifconfig_ipv6_setup=0 /usr/sbin/ifconfig tun0 192.168.126.4 netmask 255.255.255.0 mtu 1500 broadcast 192.168.126.255 Initialization Sequence Completed You will need to leave that terminal open to maintain the VPN connection. Also, check the terminal if the VPN is active if you encounter internet or printing problems.\nYou can ignore warnings related to the cipher length.\nMatlab The intec file share (ref. this Section) provides some useful software packages. However, the matlab folder does not contain the complete installation files for linux. You can obtain a copy of linux installation files from me or Sarah (we have matlab 2016a). Copy the contents to /home/\u0026lt;username\u0026gt;/matlab2016b/linux. Copying it to /usr/local is not recommended as it may not work. And even if you run as su, it may not work. I don\u0026rsquo;t know why! So copy to /home/\u0026lt;username\u0026gt;/matlab2016b/linux and don\u0026rsquo;t run as sudo or su.\nThe fileshare contains the key and the license files for matlab. Copy those two from the fileshare to some folder. Find the key for matlab 2016a from the file INTEC read key. From the terminal (Do not run as sudo, it did not work for me!):\n$ cd /home/\u0026lt;username\u0026gt;/matlab2016b $ sudo chmod -R 777 linux $ cd linux $ ./install This should launch a GUI and follow the instructions. Choose install without internet option and paste the key for matlab2016a. Then for the installation folder choose a folder in your home directory, say /home/\u0026lt;username\u0026gt;/matlab2016b\\_installation. It also asks for the license file, browse for the license.lic obtained from the Intec fileshare. Choose which packages you need and then install. Then wait for the installation to finish.\nAdd the matlab installation bin folder to the path in your bashrc. You can open the bashrc file using\n$ gedit ~/.bashrc \u0026amp; This will launch the gedit GUI and you can add the following line to the bashrc file and save it.\nexport PATH=/home/\u0026lt;username\u0026gt;/matlab2016b_installation/bin/:$PATH Open a new terminal and Matlab can be launched as:\n$ matlab \u0026amp; Miscellaneous Configuring the printer Follow the instructions in this page.\nIf you encounter errors with printing (eg., printer not responding, connecting to printer, printer may not be connected, etc.), check the openvpn terminal and see if the VPN is still active. If not, relaunch openvpn (refer openvpn Section).\nBefore using apollo webpage The athena webpage gives access to several online services including apollo, MS office and matlab. But these services are hosted on a Citrix server and you need to install the same to be able to use these services. The instructions can be found in the citrix information page.\nMounting Intec file share or Robspear To download the software packages provided by Intec, the installation files are available from a Samba server. In order to view and download the files, this server should be mounted on the linux machine first. There is also a robspear server for saving huge data files. The following example is to mount the robspear server to your linux folder.\ncreate a mount directory. This is where the server will be mounted to. Here the server will be mounted to /media/robspear folder. You can choose any other location for mounting the folder.\n$ sudo mkdir -p /media/robspear Mount the server to the created location\n$ sudo mount -t cifs //acoustserv.intec.ugent.be/robspear /media/robspear -o username=\u0026lt;your-intec-username\u0026gt; This will ask for your intec password and the mounting is done!\n","date":"2017-06-06T00:00:00Z","permalink":"https://deepakbaby.in/posts/ugent-linux/","tags":["linux","UGent"],"title":"A quick start guide for Linux users at Intec"},{"categories":["Basic"],"contents":"Greeting! This is an introduction post. This post tests the followings:\nHero image is in the same directory as the post. This post should be at top of the sidebar. Post author should be the same as specified in author.yaml file. ","date":"2000-06-08T08:06:25+06:00","permalink":"https://deepakbaby.in/posts/introduction/","tags":["Basic","Multi-lingual"],"title":"Introduction"},{"categories":null,"contents":"This is a sample post intended to test the followings:\nA different post author. Table of contents. Markdown content rendering. Math rendering. Emoji rendering. Markdown Syntax Rendering Headings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Inline Markdown In Table italics bold strikethrough code Code Blocks Code block with backticks html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nMath Rendering Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\nEmoji Rendering 🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2000-06-08T08:06:25+06:00","permalink":"https://deepakbaby.in/posts/markdown-sample/","tags":null,"title":"Markdown Samples"},{"categories":["Basic"],"contents":"This sample post tests the followings:\nCategory, sub-category nesting in the sidebar. Hero image and other images are in images folder inside this post directory. Different media rendering like image, tweet, YouTube video, Vimeo video etc. Image Sample Tweet Sample Owl bet you\u0026#39;ll lose this staring contest 🦉 pic.twitter.com/eJh4f2zncC\n\u0026mdash; San Diego Zoo Wildlife Alliance (@sandiegozoo) October 26, 2021 YouTube Video Sample Vimeo Video Sample ","date":"2000-06-08T08:06:25+06:00","permalink":"https://deepakbaby.in/posts/category/sub-category/rich-content/","tags":["Markdown","Content Organization","Multi-lingual"],"title":"Rich Content"},{"categories":null,"contents":"This is a sample post intended to test the followings:\nDefault hero image. Different shortcodes. Alert The following alerts are available in this theme.\nThis is sample alert with type=\u0026quot;success\u0026quot;. This is sample alert with type=\u0026quot;danger\u0026quot;. This is sample alert with type=\u0026quot;warning\u0026quot;. This is sample alert with type=\u0026quot;info\u0026quot;. This is sample alert with type=\u0026quot;dark\u0026quot;. This is sample alert with type=\u0026quot;primary\u0026quot;. This is sample alert with type=\u0026quot;secondary\u0026quot;. Image A sample image without any attribute. A sample image with height and width attributes. A center aligned image with height and width attributes. A image with float attribute. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies. Praesent tellus risus, eleifend vel efficitur ac, venenatis sit amet sem. Ut ut egestas erat. Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat. Suspendisse nec ipsum eu erat finibus dictum. Morbi volutpat nulla purus, vel maximus ex molestie id. Nullam posuere est urna, at fringilla eros venenatis quis.\nFusce vulputate dolor augue, ut porta sapien fringilla nec. Vivamus commodo erat felis, a sodales lectus finibus nec. In a pulvinar orci. Maecenas suscipit eget lorem non pretium. Nulla aliquam a augue nec blandit. Curabitur ac urna iaculis, ornare ligula nec, placerat nulla. Maecenas aliquam nisi vitae tempus vulputate.\nSplit This theme support splitting the page into as many columns as you wish.\nTwo column split Left Column Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies.\nRight Column Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat.\nThree column split Left Column Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies.\nMiddle Column Aenean dignissim dictum ex. Donec a nunc vel nibh placerat interdum.\nRight Column Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat.\nVertical Space Give vertical space between two lines.\nThis is line one. This is line two. It should have 4rem vertical space with previous line.\nVideo Video by Rahul Sharma from Pexels.\nMermaid Here, are few example of mermaid shortcode.\nGraph:\ngraph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] Sequence Diagram:\nsequenceDiagram participant Alice participant Bob Alice-\u003e\u003eJohn: Hello John, how are you? loop Healthcheck John-\u003e\u003eJohn: Fight against hypochondria end Note right of John: Rational thoughts prevail! John--\u003e\u003eAlice: Great! John-\u003e\u003eBob: How about you? Bob--\u003e\u003eJohn: Jolly good! Gantt diagram:\ngantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d Class Diagram:\nclassDiagram Class01 \u003c|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --\u003e C2 : Where am i? Class09 --* C3 Class09 --|\u003e Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 \u003c--\u003e C2: Cool label Git Graph:\ngitGraph commit id: \"ZERO\" branch develop commit id:\"A\" checkout main commit id:\"ONE\" checkout develop commit id:\"B\" checkout main commit id:\"TWO\" cherry-pick id:\"A\" commit id:\"THREE\" checkout develop commit id:\"C\" ER Diagram:\nerDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses Gist Embedded PDF Page: / Previous Next ","date":"2000-06-08T08:06:25+06:00","permalink":"https://deepakbaby.in/posts/shortcodes/","tags":null,"title":"Shortcodes Samples"}]