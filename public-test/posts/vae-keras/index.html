<!doctype html><html lang=en><head><title>Understanding Variational Autoencoders and Implementation in Keras</title><style>:root{--bg-primary:#0f172a;--bg-secondary:#1e293b;--bg-card:rgba(255, 255, 255, 0.03);--text-primary:#fff;--text-secondary:#9ca3af;--text-muted:#6b7280;--border-color:rgba(255, 255, 255, 0.08);--orb-opacity:0.5;--toggle-bg:rgba(255, 255, 255, 0.1);--toggle-border:rgba(255, 255, 255, 0.2);--social-bg:rgba(255, 255, 255, 0.05);--social-border:rgba(255, 255, 255, 0.1);--mouse-border:rgba(255, 255, 255, 0.2);--wheel-bg:rgba(255, 255, 255, 0.4);--card-hover-border:rgba(59, 130, 246, 0.3);--card-hover-shadow:rgba(59, 130, 246, 0.15)}[data-theme=light]{--bg-primary:#fafbfc;--bg-secondary:#ffffff;--bg-card:transparent;--text-primary:#1e293b;--text-secondary:#475569;--text-muted:#94a3b8;--border-color:rgba(0, 0, 0, 0.06);--orb-opacity:0.1;--toggle-bg:rgba(0, 0, 0, 0.04);--toggle-border:rgba(0, 0, 0, 0.08);--social-bg:transparent;--social-border:rgba(0, 0, 0, 0.1);--mouse-border:rgba(0, 0, 0, 0.12);--wheel-bg:rgba(0, 0, 0, 0.2);--card-hover-border:rgba(59, 130, 246, 0.3);--card-hover-shadow:rgba(59, 130, 246, 0.08);--card-bg:transparent;--card-border:rgba(0, 0, 0, 0.04);--timeline-color:#3b82f6;--section-bg:#ffffff;--glow-color:rgba(59, 130, 246, 0.08)}body{background-color:var(--bg-primary);color:var(--text-primary)}</style><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.c783c6e6b679af484325eb2b13483dfca8aa9b848c0b6e81bd4022ece7a9a0ef.css integrity="sha256-x4PG5rZ5r0hDJesrE0g9/Kiqm4SMC26BvUAi7OepoO8="><link rel=icon type=image/png href=/images/site/favicon_hu_be34366aac4a405.png><meta property="og:title" content="Deepak Baby"><meta property="og:type" content="website"><meta property="og:description" content="Personal website of Deepak Baby."><meta property="og:image" content="/images/author/deepak.jpeg"><meta property="og:url" content="https://deepakbaby.in"><meta name=description content="Understanding Variational Autoencoders and Implementation in Keras"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=stylesheet href=/css/collapsible-sidebars.css><link rel=stylesheet href=/css/comments.css><script integrity="sha256-DO4ugzEwhTW1Id1UIWn0gUJWaebCYOypeTit6LW4QB4=">let theme=localStorage.getItem("theme-scheme")||localStorage.getItem("darkmode:color-scheme")||"light";theme==="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid wrapper"><button class=theme-toggle-fixed id=themeToggle aria-label="Toggle theme"><svg class="sun-icon" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><svg class="moon-icon" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg></button><nav class=scroll-navbar id=scrollNavbar><div class=navbar-container><a href=https://deepakbaby.in/ class=navbar-brand><img src=/images/site/favicon.png alt class=navbar-favicon>
Deepak Baby</a><div class=navbar-links><a href=https://deepakbaby.in/#about class=navbar-link>About</a>
<a href=https://deepakbaby.in/#experiences class=navbar-link>Experience</a>
<a href=/posts class=navbar-link>Blog</a>
<a href=https://deepakbaby.in/publications class=navbar-link>Publications</a></div><button class=theme-toggle-navbar id=themeToggleNav aria-label="Toggle theme"><svg class="sun-icon" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><svg class="moon-icon" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg></button></div></nav><style>.theme-toggle-fixed{position:fixed;top:1.5rem;right:1.5rem;width:44px;height:44px;border-radius:50%;background:var(--toggle-bg,rgba(255,255,255,.1));border:1px solid var(--toggle-border,rgba(255,255,255,.2));display:flex;align-items:center;justify-content:center;cursor:pointer;transition:all .3s;color:var(--text-primary,#fff);z-index:1000}.theme-toggle-fixed:hover{background:var(--toggle-hover-bg,rgba(255,255,255,.2));transform:scale(1.05)}.theme-toggle-fixed svg{width:22px;height:22px}.scroll-navbar.visible~.theme-toggle-fixed,.theme-toggle-fixed.hidden{opacity:0;pointer-events:none}.scroll-navbar.visible~.section-wrapper .theme-toggle-fixed,body:has(.scroll-navbar.visible) .theme-toggle-fixed{opacity:0;pointer-events:none}.scroll-navbar{position:fixed;top:0;left:0;right:0;background:var(--navbar-bg,rgba(15,23,42,.95));backdrop-filter:blur(20px);border-bottom:1px solid var(--border-color,rgba(255,255,255,8%));z-index:999;transform:translateY(-100%);transition:transform .3s ease}.scroll-navbar.visible{transform:translateY(0)}.navbar-container{max-width:1200px;margin:0 auto;padding:1rem 2rem;display:flex;align-items:center;justify-content:space-between}.navbar-brand{display:flex;align-items:center;gap:.75rem;font-family:inter,sans-serif;font-size:1.125rem;font-weight:500;color:var(--text-primary,#fff);text-decoration:none!important;transition:transform .3s ease,color .3s}.navbar-brand:hover,.navbar-brand:focus,.navbar-brand:active{transform:translateY(-2px);color:#3b82f6;text-decoration:none!important}.navbar-favicon{width:28px;height:28px;border-radius:6px}.navbar-links{display:flex;gap:2rem}.navbar-link{font-family:inter,sans-serif;font-size:.875rem;font-weight:400;color:var(--text-secondary,rgba(255,255,255,.7));text-decoration:none!important;transition:transform .3s ease,color .3s;display:inline-block}.navbar-link:hover,.navbar-link:focus,.navbar-link:active{transform:translateY(-2px);color:var(--text-primary,#fff);text-decoration:none!important}.theme-toggle-navbar{width:36px;height:36px;border-radius:50%;background:var(--toggle-bg,rgba(255,255,255,.1));border:1px solid var(--toggle-border,rgba(255,255,255,.2));display:flex;align-items:center;justify-content:center;cursor:pointer;transition:all .3s;color:var(--text-primary,#fff)}.theme-toggle-navbar:hover{background:var(--toggle-hover-bg,rgba(255,255,255,.2))}.theme-toggle-navbar svg{width:18px;height:18px}.sun-icon{display:none}.moon-icon{display:block}[data-theme=light] .sun-icon{display:block}[data-theme=light] .moon-icon{display:none}[data-theme=light] .scroll-navbar{--navbar-bg:rgba(255, 255, 255, 0.98);box-shadow:0 1px 8px rgba(0,0,0,5%);border-bottom-color:transparent}@media(max-width:768px){.theme-toggle-fixed{top:1rem;right:1rem;width:40px;height:40px}.navbar-links{display:none}}</style><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("themeToggle"),a=document.getElementById("themeToggleNav"),t=document.getElementById("scrollNavbar"),n=document.documentElement,r=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: light)").matches?"light":"dark");n.setAttribute("data-theme",r);function s(){const t=n.getAttribute("data-theme"),e=t==="dark"?"light":"dark";n.setAttribute("data-theme",e),localStorage.setItem("theme",e)}e?.addEventListener("click",s),a?.addEventListener("click",s);const o=document.querySelector(".hero-section"),i=o?o.offsetHeight*.6:100;window.addEventListener("scroll",()=>{const n=window.pageYOffset;n>i?(t?.classList.add("visible"),e?.classList.add("hidden")):(t?.classList.remove("visible"),e?.classList.remove("hidden"))}),window.pageYOffset>i&&(t?.classList.add("visible"),e?.classList.add("hidden"))})</script><style>.type-posts .container-fluid.wrapper{max-width:none!important;width:100%!important;padding:0!important;margin:0!important;overflow-x:hidden!important}body.type-posts{overflow-x:hidden!important;width:100vw!important;max-width:100vw!important}html{overflow-x:hidden!important}</style><div class=modern-post-wrapper><section class=post-hero-section><div class=post-hero-overlay></div><div class=post-hero-image style=background-image:url(/images/default-hero.jpg)></div><div class=post-hero-content><div class=post-meta-badges><span class=post-category-badge>deep learning</span></div><h1 class=post-title>Understanding Variational Autoencoders and Implementation in Keras</h1><div class=post-meta-info><div class=post-author><span class=author-name>Deepak Baby</span></div><div class=post-date-time><svg width="16" height="16" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5A2 2 0 003 7v12a2 2 0 002 2z"/></svg>
<span>Tuesday, July 2, 2019</span>
<span class=meta-separator>•</span>
<svg width="16" height="16" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3A9 9 0 113 12a9 9 0 0118 0z"/></svg>
<span>10 min read</span></div></div><div class=post-tags-hero><a href=/tags/deep-learning class=tag-link>#deep learning</a>
<a href=/tags/vae class=tag-link>#vae</a>
<a href=/tags/variational-autoencoder class=tag-link>#variational autoencoder</a>
<a href=/tags/keras class=tag-link>#keras</a></div></div></section><div class=post-content-container><div class=post-content-grid><article class=post-article-content><div class=prose-content id=post-content><p>Variational Autoencoders (VAEs)<a href=https://arxiv.org/abs/1312.6114>[Kingma, et.al (2013)]</a> let us design complex generative models of data that can be trained on large datasets. This post is about understanding the VAE concepts, its loss functions and how we can implement it in keras.</p><h2 id=generating-data-from-a-latent-space>Generating data from a latent space</h2><p>VAEs, in terms of probabilistic terms, assume that the data-points in a large dataset are generated from a latent space. For e.g., let us assume we want to generate the image of an animal. First we imagine that it has four legs, a head and a tail. This is analogous to the latent space and from this set of characteristics that are defined in the latent space, the model will learn to generate the image of an animal.</p><p>Before we dive into the math and intuitions, let us define some notations:</p><ol><li>$\mathbf{X}$: The type of data we want to generate (say, a large dataset containing images of animals)</li><li>$z$: The latent variable, the set of characteristics we want in the image</li><li>$\mathbb{P}(\mathbf{X})$: probability distribution of the data</li><li>$\mathbb{P}(z)$: probability distribution of the latent space</li><li>$\mathbb{P}(\mathbf{X} \vert z)$: probability distribution of generating data from the latent variable</li></ol><p>We assume that every data-point $x$ is a random sample from the <em>unknown underlying process</em> whose true distribution $\mathbb{P}(\mathbf{X})$ is unknown.
VAEs make use of a specific probability model that captures the joint probability between the data $\mathbf{X}$ and latent variables $z$. This joint probability can be written as $\mathbb{P}(\mathbf{X}, z) = \mathbb{P}(\mathbf{X} \vert z) \cdot \mathbf{P}(z)$. The generative model assumed in VAE can be described as:</p><ol><li>Draw one latent variable $ z_{i} \sim \mathbb{P}(z) $: similar to defining a set of characteristics that defines an animal</li><li>Generate the data-point such that $x \sim \mathbb{P}(\mathbf{X} \vert z) $: similar to generating the image of an animal that satisfies the characteristics specified in the latent variable</li></ol><h2 id=vae-formulation-and-cost-function>VAE formulation and cost function</h2><p>From the probability model perspective, the latent variables are drawn from a prior $\mathbb{P}(z)$ and the generated data $x$ has a likelihood of $\mathbb{P}(\mathbf{X} \vert z)$ that is conditioned on the latent variables $z$. The objective here is to model the data distribution $\mathbb{P}(\mathbf{X})$ by marginalizing out the latent variable $z$ from the joint-distribution $\mathbb{P}(\mathbf{X}, z)$.</p><p>$$\begin{equation}
\mathbb{P}(\mathbf{X}) = \int_{z} \mathbb{P}(\mathbf{X} \vert z) \mathbb{P}(z) ~ dz
\end{equation}$$</p><p>However, this integral is very difficult to compute as it requires to be computed over all possibilities of the latent variable $z$. In order to overcome this, VAEs first try to infer the distribution $\mathbb{P}(z)$ from the data using $\mathbb{P}(z \vert \mathbf{X})$. i.e., rather looking at all possibilities of $z$, we want to infer the distribution of $z$ that describes our data reasonably well. For example, if we want to generate an animal, we only need to specify the characteristics that describe an animal. We do not need to include things like glass, table, &mldr; as it is unlikely that those characteristics contribute to generating the image of an animal. Thus, this inference phase $\mathbb{P}(z \vert \mathbf{X})$ limits our imagination space to focus on characteristics that are required for generating the image of animal.</p><p>Again, we do not know the best set of characteristics $\mathbb{P}(z \vert \mathbf{X})$ yet. VAEs make use of variational inference to infer $\mathbb{P}(z \vert \mathbf{X})$. Variational inference approximate the true distribuation $\mathbb{P}(z \vert \mathbf{X})$ using a simpler distribution that is easy to evaluate. A popular choice is Gaussian distribution.</p><p>Further, a parametric inference model $\mathbb{Q}(z \vert \mathbf{X})$ that maps the data to the underlying latent space and the difference between $\mathbb{P}(z \vert \mathbf{X})$ and $\mathbb{Q}(z \vert \mathbf{X})$ is quantified using <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence>Kullback-Leibler divergence</a> between them.</p><p>$$\begin{align}
D_{KL} \left( \mathbb{Q}(z \vert \mathbf{X}) \mathrel{\Vert} \mathbb{P}(z \vert \mathbf{X}) \right) &= \sum \mathbb{Q}(z \vert \mathbf{X}) \log \dfrac{ \mathbb{Q}(z \vert \mathbf{X})}{ \mathbb{P}(z \vert \mathbf{X})} \\
&=\mathbb{E}\left[ \log \dfrac{ \mathbb{Q}(z \vert \mathbf{X})}{ \mathbb{P}(z \vert \mathbf{X})} \right] \\
&= \mathbb{E}\left[ \log\mathbb{Q}(z \vert \mathbf{X}) - \log \mathbb{P}(z \vert \mathbf{X}) \right]
\end{align}$$</p><p>where, $\mathbb{E}$ is the expectation with respect to $\mathbb{Q}(z \vert \mathbf{X})$. Using $\mathbb{P}(z \vert \mathbf{X}) = \dfrac{\mathbb{P}(\mathbf{X} \vert z) \mathbb{P}(z)}{\mathbb{P}(\mathbf{X})}$ we can rewrite the above expression as:</p><p>$$\begin{align}
D_{KL} \left( \mathbb{Q}(z \vert \mathbf{X}) \mathrel{\Vert} \mathbb{P}(z \vert \mathbf{X}) \right) &= \mathbb{E}\left[ \log\mathbb{Q}(z \vert \mathbf{X}) - \log \dfrac{\mathbb{P}(\mathbf{X} \vert z) \mathbb{P}(z)}{\mathbb{P}(\mathbf{X})} \right] \\
&= \mathbb{E}\left[ \log\mathbb{Q}(z \vert \mathbf{X}) - \log\mathbb{P}(\mathbf{X} \vert z) - \log \mathbb{P}(z) + \log \mathbb{P}(\mathbf{X}) \right]
\end{align}$$</p><p>Notice that $\mathbb{P}(\mathbf{X})$ does not depend on $z$ and hence it can be taken outside the expectation operation over $z$. We will denote $D_{KL}$ as $D$.</p><p>$$\begin{align}
D \left( \mathbb{Q}(z \vert \mathbf{X}) \mathrel{\Vert} \mathbb{P}(z \vert \mathbf{X}) \right) &= \mathbb{E}\left[ \log\mathbb{Q}(z \vert \mathbf{X}) - \log\mathbb{P}(\mathbf{X} \vert z) - \log \mathbb{P}(z) \right] + \log \mathbb{P}(\mathbf{X}) \\
\implies \quad \log \mathbb{P}(\mathbf{X}) - D \left( \mathbb{Q}(z \vert \mathbf{X}) \mathrel{\Vert} \mathbb{P}(z \vert \mathbf{X}) \right) &= \mathbb{E}\left[ \log\mathbb{P}(\mathbf{X} \vert z) \right] - \mathbb{E}\left[ \log\mathbb{Q}(z \vert \mathbf{X}) - \mathbb{P}(z) \right] \\
&= \mathbb{E}\left[ \log\mathbb{P}(\mathbf{X} \vert z) \right] - D\left( \mathbb{Q}(z \vert \mathbf{X}) \mathrel{\Vert} \mathbb{P}(z) \right)
\end{align}$$</p><p>The right-hand side of the above equation is the objective function used by VAEs. What it says is that, we are trying to model our data which is described by $\log \mathbb{P}(\mathbf{X})$ with some error $D \left( \mathbb{Q}(z \vert \mathbf{X}) \mathrel{\Vert} \mathbb{P}(z \vert \mathbf{X}) \right)$. Since $D_{KL}$ is always positive, we can write the above equation as:</p><p>$$\begin{equation}
\log \mathbb{P}(\mathbf{X}) \geq \mathbb{E}\left[ \log\mathbb{P}(\mathbf{X} \vert z) \right] - D\left( \mathbb{Q}(z \vert \mathbf{X}) \mathrel{\Vert} \mathbb{P}(z) \right)
\end{equation}$$</p><p>Thus, the right-hand side (RHS) of the above inequality is the lower bound for $\log \mathbb{P}(\mathbf{X})$ which we are trying to maximize. This is known as the evidence lower bound (ELBO). Maximizing the RHS is also the same as minimizing its negative. The negative of the RHS is therefore used as a cost function to be minimized while training VAEs.</p><p>At this point, what we have is:</p><ol><li>$\mathbb{P}(\mathbf{X} \vert z)$: Generating data from the given latent variable (the <strong>decoder</strong>)</li><li>$\mathbb{Q}(z \vert \mathbf{X})$: Infering the latent code given the data (the <strong>encoder</strong>)</li><li>$D\left(\mathbb{Q}(z \vert \mathbf{X}) \mathrel{\Vert} \mathbb{P}(z) \right)$: Making sure that the encoded representation resembles a simpler, tractable distribuation (e.g., Gaussian).</li></ol><p>Thus a VAE first encodes the data into some latent space (mapping $x$ to $z$) and then generates (decodes: mapping $z$ to $x$) data based on samples from that latent space, and hence called variational autoencoder.</p><h2 id=vae-cost-function-and-neural-networks>VAE cost function and neural networks</h2><p>The VAE cost function can be seen as adding an additional cost term on the traditional autoencoders. The first term is the reconstruction loss at the output, which is the same as used in an autoencoder. The second term forces the encoder to map the input data to a pre-defined tractable distribution.</p><p>Why do we need $\mathbb{P}(z)$ to be a simple distribution? Since VAE is a generative model, we would like to generate new data-points by sampling $\mathbb{P}(z)$. The easiest choice for this is a standard normal distribution $\mathcal{N}(0,1)$.</p><p>The mappings $\mathbb{P}(\mathbf{X} \vert z)$ and $\mathbb{Q}(z \vert \mathbf{X})$ are realized using deep neural networks (DNNs). Thus VAEs are designed using two DNNs: an encoder and a decoder. The cost function is to minimize the negative of the ELBO obtained above.</p><h2 id=implementing-vae-cost-in-keras>Implementing VAE cost in keras</h2><p>As detailed before, the first term of the cost function is the reconstruction loss. We can use any popular loss, say mean-squared error, for this purpose. Computing the KL divergence cost term requires assuming $\mathbb{Q}(z \vert \mathbf{X})$ to be also Gaussian with parameters $\mu (\mathbf{X})$ and $\Sigma (\mathbf{X})$. This assumption enables us to compute the KL divergence between $\mathbb{Q}(z \vert \mathbf{X}) = \mathcal{N}(\mu (\mathbf{X}), \Sigma (\mathbf{X}))$ and $\mathbb{P}(z) = \mathcal{N}(0,1)$ in closed form as:</p><p>$$\begin{align}
D\left[ \mathcal{N}(\mu (\mathbf{X}), \Sigma (\mathbf{X}))~\Vert ~ \mathcal{N}(0,1) \right] = \dfrac{1}{2} \left[ tr\left( \Sigma (\mathbf{X}) \right) + \mu (\mathbf{X})^{T} \mu (\mathbf{X}) - k - \log det\left( \Sigma (\mathbf{X}) \right) \right]
\end{align}$$
where, $tr$ and $det$ are the trace and determinant of the covariance matrix $\Sigma (\mathbf{X})$ and $k$ is the dimension of the Gaussian distribution. For details on the calculation of the above divergence, refer to this <a href=https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians>page</a>. We also assume that the covariance matrix is diagonal, we can compute the determinant by simpy multiplying its diagonal elements. In addition, we can also implement $\Sigma (\mathbf{X})$ as a vector since it is a diagonal matrix.</p><p>$$\begin{align}
D\left[ \mathcal{N}(\mu (\mathbf{X}), \Sigma (\mathbf{X}))~\Vert ~ \mathcal{N}(0,1) \right] &= \dfrac{1}{2} \left[ \sum \Sigma (\mathbf{X}) + \sum \mu^{2} (\mathbf{X}) - \sum 1 - \log \prod \Sigma (\mathbf{X}) \right] \\\
&= \dfrac{1}{2} \left[ \sum \Sigma (\mathbf{X}) + \sum \mu^{2} (\mathbf{X}) - \sum 1 - \sum \log \Sigma (\mathbf{X}) \right] \\\
&= \dfrac{1}{2} \sum \left[ \Sigma (\mathbf{X}) + \mu^{2} (\mathbf{X}) - 1 - \log \Sigma (\mathbf{X}) \right]
\end{align}$$</p><p>In addition, typically we model the logarithm of $\Sigma (\mathbf{X})$ for numerical stability. Thus the final loss term becomes:</p><p>$$\begin{equation}
D\left[ \mathcal{N}(\mu (\mathbf{X}), \Sigma (\mathbf{X}))~\Vert ~ \mathcal{N}(0,1) \right] = \dfrac{1}{2} \sum \left[ \exp(\Sigma (\mathbf{X})) + \mu^{2} (\mathbf{X}) - 1 - \Sigma (\mathbf{X}) \right]
\end{equation}$$</p><h2 id=keras-implementation>Keras implementation</h2><p>This is mostly a copy of the example provided in <a href=https://keras.io/examples/variational_autoencoder/>Keras VAE example</a>, but with some edits and added comments. This post does not discuss the reparameterization trick involved in training a VAE as it is discussed in many other pages.</p><p><strong>Even though the example below works really well, in practice, we will need to somehow adjust the reconstruction loss and the KL loss. The insights I gained and the tricks I used to overcome the issues will be described in the upcoming post.</strong></p><p><a href=https://deepakbaby.github.io/post/vae-insights/>Implementing Variational Autoencoders: Some insights and tricks</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> keras.layers <span style=color:#f92672>import</span> Lambda, Input, Dense
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> keras.models <span style=color:#f92672>import</span> Model
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> keras.datasets <span style=color:#f92672>import</span> mnist
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> keras.losses <span style=color:#f92672>import</span> mse, binary_crossentropy
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> keras.utils <span style=color:#f92672>import</span> plot_model
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> keras <span style=color:#f92672>import</span> backend <span style=color:#66d9ef>as</span> K
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># reparameterization trick</span>
</span></span><span style=display:flex><span><span style=color:#75715e># instead of sampling from Q(z|X), sample eps = N(0,I)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># z = z_mean + sqrt(var)*eps</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sampling</span>(args):
</span></span><span style=display:flex><span>    z_mean, z_log_var <span style=color:#f92672>=</span> args
</span></span><span style=display:flex><span>    batch <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>shape(z_mean)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    dim <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>int_shape(z_mean)[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    <span style=color:#75715e># by default, random_normal has mean=0 and std=1.0</span>
</span></span><span style=display:flex><span>    epsilon <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>random_normal(shape<span style=color:#f92672>=</span>(batch, dim))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> z_mean <span style=color:#f92672>+</span> K<span style=color:#f92672>.</span>exp(<span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> z_log_var) <span style=color:#f92672>*</span> epsilon
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_results</span>(models,
</span></span><span style=display:flex><span>                 data,
</span></span><span style=display:flex><span>                 batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>,
</span></span><span style=display:flex><span>                 model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;vae_mnist&#34;</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Plots labels and MNIST digits as function of 2-dim latent vector
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Arguments
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        models (tuple): encoder and decoder models
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        data (tuple): test data and label
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        batch_size (int): prediction batch size
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model_name (string): which model is using this function
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    encoder, decoder <span style=color:#f92672>=</span> models
</span></span><span style=display:flex><span>    x_test, y_test <span style=color:#f92672>=</span> data
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>makedirs(model_name, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    filename <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(model_name, <span style=color:#e6db74>&#34;vae_mean.png&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># display a 2D plot of the digit classes in the latent space</span>
</span></span><span style=display:flex><span>    z_mean, _, _ <span style=color:#f92672>=</span> encoder<span style=color:#f92672>.</span>predict(x_test,
</span></span><span style=display:flex><span>                                   batch_size<span style=color:#f92672>=</span>batch_size)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>scatter(z_mean[:, <span style=color:#ae81ff>0</span>], z_mean[:, <span style=color:#ae81ff>1</span>], c<span style=color:#f92672>=</span>y_test)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>colorbar()
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;z[0]&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;z[1]&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>savefig(filename)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    filename <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(model_name, <span style=color:#e6db74>&#34;digits_over_latent.png&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># display a 30x30 2D manifold of digits</span>
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> <span style=color:#ae81ff>30</span>
</span></span><span style=display:flex><span>    digit_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>28</span>
</span></span><span style=display:flex><span>    figure <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((digit_size <span style=color:#f92672>*</span> n, digit_size <span style=color:#f92672>*</span> n))
</span></span><span style=display:flex><span>    <span style=color:#75715e># linearly spaced coordinates corresponding to the 2D plot</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># of digit classes in the latent space</span>
</span></span><span style=display:flex><span>    grid_x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, n)
</span></span><span style=display:flex><span>    grid_y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, n)[::<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, yi <span style=color:#f92672>in</span> enumerate(grid_y):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j, xi <span style=color:#f92672>in</span> enumerate(grid_x):
</span></span><span style=display:flex><span>            z_sample <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[xi, yi]])
</span></span><span style=display:flex><span>            x_decoded <span style=color:#f92672>=</span> decoder<span style=color:#f92672>.</span>predict(z_sample)
</span></span><span style=display:flex><span>            digit <span style=color:#f92672>=</span> x_decoded[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>reshape(digit_size, digit_size)
</span></span><span style=display:flex><span>            figure[i <span style=color:#f92672>*</span> digit_size: (i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> digit_size,
</span></span><span style=display:flex><span>                   j <span style=color:#f92672>*</span> digit_size: (j <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> digit_size] <span style=color:#f92672>=</span> digit
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span>    start_range <span style=color:#f92672>=</span> digit_size <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    end_range <span style=color:#f92672>=</span> n <span style=color:#f92672>*</span> digit_size <span style=color:#f92672>+</span> start_range <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    pixel_range <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(start_range, end_range, digit_size)
</span></span><span style=display:flex><span>    sample_range_x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>round(grid_x, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    sample_range_y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>round(grid_y, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xticks(pixel_range, sample_range_x)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>yticks(pixel_range, sample_range_y)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;z[0]&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;z[1]&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>imshow(figure, cmap<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Greys_r&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>savefig(filename)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># MNIST dataset</span>
</span></span><span style=display:flex><span>(x_train, y_train), (x_test, y_test) <span style=color:#f92672>=</span> mnist<span style=color:#f92672>.</span>load_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>image_size <span style=color:#f92672>=</span> x_train<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>original_dim <span style=color:#f92672>=</span> image_size <span style=color:#f92672>*</span> image_size
</span></span><span style=display:flex><span>x_train <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>reshape(x_train, [<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, original_dim])
</span></span><span style=display:flex><span>x_test <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>reshape(x_test, [<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, original_dim])
</span></span><span style=display:flex><span>x_train <span style=color:#f92672>=</span> x_train<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#39;float32&#39;</span>) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255</span>
</span></span><span style=display:flex><span>x_test <span style=color:#f92672>=</span> x_test<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#39;float32&#39;</span>) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># network parameters</span>
</span></span><span style=display:flex><span>input_shape <span style=color:#f92672>=</span> (original_dim, )
</span></span><span style=display:flex><span>intermediate_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>latent_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>50</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># VAE model = encoder + decoder</span>
</span></span><span style=display:flex><span><span style=color:#75715e># build encoder model</span>
</span></span><span style=display:flex><span>inputs <span style=color:#f92672>=</span> Input(shape<span style=color:#f92672>=</span>input_shape, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;encoder_input&#39;</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> Dense(intermediate_dim, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)(inputs)
</span></span><span style=display:flex><span>z_mean <span style=color:#f92672>=</span> Dense(latent_dim, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;z_mean&#39;</span>)(x)
</span></span><span style=display:flex><span>z_log_var <span style=color:#f92672>=</span> Dense(latent_dim, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;z_log_var&#39;</span>)(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># use reparameterization trick to push the sampling out as input</span>
</span></span><span style=display:flex><span><span style=color:#75715e># note that &#34;output_shape&#34; isn&#39;t necessary with the TensorFlow backend</span>
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> Lambda(sampling, output_shape<span style=color:#f92672>=</span>(latent_dim,), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;z&#39;</span>)([z_mean, z_log_var])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># instantiate encoder model</span>
</span></span><span style=display:flex><span>encoder <span style=color:#f92672>=</span> Model(inputs, [z_mean, z_log_var, z], name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;encoder&#39;</span>)
</span></span><span style=display:flex><span>encoder<span style=color:#f92672>.</span>summary()
</span></span><span style=display:flex><span>plot_model(encoder, to_file<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;vae_mlp_encoder.png&#39;</span>, show_shapes<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># build decoder model</span>
</span></span><span style=display:flex><span>latent_inputs <span style=color:#f92672>=</span> Input(shape<span style=color:#f92672>=</span>(latent_dim,), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;z_sampling&#39;</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> Dense(intermediate_dim, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)(latent_inputs)
</span></span><span style=display:flex><span>outputs <span style=color:#f92672>=</span> Dense(original_dim, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>)(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># instantiate decoder model</span>
</span></span><span style=display:flex><span>decoder <span style=color:#f92672>=</span> Model(latent_inputs, outputs, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;decoder&#39;</span>)
</span></span><span style=display:flex><span>decoder<span style=color:#f92672>.</span>summary()
</span></span><span style=display:flex><span>plot_model(decoder, to_file<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;vae_mlp_decoder.png&#39;</span>, show_shapes<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># instantiate VAE model</span>
</span></span><span style=display:flex><span>outputs <span style=color:#f92672>=</span> decoder(encoder(inputs)[<span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>vae <span style=color:#f92672>=</span> Model(inputs, outputs, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;vae_mlp&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    models <span style=color:#f92672>=</span> (encoder, decoder)
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> (x_test, y_test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>vae_loss</span>(y_true, y_pred):
</span></span><span style=display:flex><span>        reconstruction_loss <span style=color:#f92672>=</span> mse(y_true, y_pred)
</span></span><span style=display:flex><span>        reconstruction_loss <span style=color:#f92672>*=</span> original_dim
</span></span><span style=display:flex><span>        z_mean <span style=color:#f92672>=</span> vae<span style=color:#f92672>.</span>get_layer(<span style=color:#e6db74>&#39;encoder&#39;</span>)<span style=color:#f92672>.</span>get_layer(<span style=color:#e6db74>&#39;z_mean&#39;</span>)<span style=color:#f92672>.</span>output
</span></span><span style=display:flex><span>        z_log_var <span style=color:#f92672>=</span> vae<span style=color:#f92672>.</span>get_layer(<span style=color:#e6db74>&#39;encoder&#39;</span>)<span style=color:#f92672>.</span>get_layer(<span style=color:#e6db74>&#39;z_log_var&#39;</span>)<span style=color:#f92672>.</span>output
</span></span><span style=display:flex><span>        kl_loss <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> z_log_var <span style=color:#f92672>-</span> K<span style=color:#f92672>.</span>square(z_mean) <span style=color:#f92672>-</span> K<span style=color:#f92672>.</span>exp(z_log_var)
</span></span><span style=display:flex><span>        kl_loss <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>sum(kl_loss, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        kl_loss <span style=color:#f92672>*=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> K<span style=color:#f92672>.</span>mean(reconstruction_loss <span style=color:#f92672>+</span> kl_loss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    vae<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;adam&#39;</span>, loss<span style=color:#f92672>=</span>vae_loss)
</span></span><span style=display:flex><span>    vae<span style=color:#f92672>.</span>summary()
</span></span><span style=display:flex><span>    plot_model(vae,
</span></span><span style=display:flex><span>               to_file<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;vae_mlp.png&#39;</span>,
</span></span><span style=display:flex><span>               show_shapes<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># train the autoencoder</span>
</span></span><span style=display:flex><span>    vae<span style=color:#f92672>.</span>fit(x_train,
</span></span><span style=display:flex><span>        epochs<span style=color:#f92672>=</span>epochs,
</span></span><span style=display:flex><span>        batch_size<span style=color:#f92672>=</span>batch_size,
</span></span><span style=display:flex><span>        validation_data<span style=color:#f92672>=</span>(x_test, <span style=color:#66d9ef>None</span>))
</span></span><span style=display:flex><span>    vae<span style=color:#f92672>.</span>save_weights(<span style=color:#e6db74>&#39;vae_mlp_mnist.h5&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    plot_results(models,
</span></span><span style=display:flex><span>                 data,
</span></span><span style=display:flex><span>                 batch_size<span style=color:#f92672>=</span>batch_size,
</span></span><span style=display:flex><span>                 model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;vae_mlp&#34;</span>)
</span></span></code></pre></div></div><div class=share-section><h4 class=share-title>Share this post</h4><div class=share-buttons><a class="share-btn share-facebook" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdeepakbaby.in%2fposts%2fvae-keras%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook-f"></i>
</a><a class="share-btn share-twitter" href="https://twitter.com/share?url=https%3a%2f%2fdeepakbaby.in%2fposts%2fvae-keras%2f&text=Understanding%20Variational%20Autoencoders%20and%20Implementation%20in%20Keras" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a class="share-btn share-linkedin" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fdeepakbaby.in%2fposts%2fvae-keras%2f&title=Understanding%20Variational%20Autoencoders%20and%20Implementation%20in%20Keras" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin-in"></i>
</a><a class="share-btn share-reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdeepakbaby.in%2fposts%2fvae-keras%2f&title=Understanding%20Variational%20Autoencoders%20and%20Implementation%20in%20Keras" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit-alien"></i>
</a><a class="share-btn share-whatsapp" href="https://api.whatsapp.com/send?text=Understanding%20Variational%20Autoencoders%20and%20Implementation%20in%20Keras https%3a%2f%2fdeepakbaby.in%2fposts%2fvae-keras%2f" target=_blank rel=noopener aria-label="Share on WhatsApp"><i class="fab fa-whatsapp"></i>
</a><a class="share-btn share-email" href="mailto:?subject=Understanding%20Variational%20Autoencoders%20and%20Implementation%20in%20Keras&body=https%3a%2f%2fdeepakbaby.in%2fposts%2fvae-keras%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></div><div class=improve-page-section><a href=https://github.com/deepakbaby/deepakbaby-site/edit/main/content/posts/vae-keras/index.md class=improve-link target=_blank rel=noopener><i class="fas fa-code-branch"></i>
<span>Improve this page</span></a></div><nav class=post-navigation><a href=/posts/kaldi-mkl/ class="nav-card nav-prev"><span class=nav-label>← Previous</span>
<span class=nav-title>Installing Kaldi with MKL support without root access</span>
</a><a href=/posts/vae-insights/ class="nav-card nav-next"><span class=nav-label>Next →</span>
<span class=nav-title>Implementing Variational Autoencoders: Insights and some tricks</span></a></nav><div class=comments-section><h2 class=comments-heading><i class="fas fa-comments"></i>
Discussion</h2><div class=comments-content><div id=comments-container><script src=https://utteranc.es/client.js repo=deepakbaby/deepakbaby-site issue-term=title theme=github-light crossorigin=anonymous async></script></div></div></div></article></div></div><button id=scroll-to-top class=scroll-top-btn aria-label="Scroll to top">
<i class="fas fa-chevron-up"></i></button></div><style>@import 'https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap';.modern-post-wrapper{font-family:inter,-apple-system,BlinkMacSystemFont,sans-serif;background:var(--bg-primary);color:var(--text-primary);min-height:100vh;padding-top:5rem;width:100vw;max-width:100vw;overflow-x:hidden;margin:0;padding-left:0;padding-right:0}.post-hero-section{position:relative;min-height:60vh;display:flex;align-items:center;justify-content:center;text-align:center;padding:4rem 2rem;overflow:hidden;background:linear-gradient(135deg,#0f172a 0%,#1e293b 100%)}[data-theme=light] .post-hero-section{background:linear-gradient(135deg,#f8fafc 0%,#e2e8f0 100%)}.post-hero-image{position:absolute;inset:0;background-size:cover;background-position:50%;opacity:.15;z-index:0}[data-theme=light] .post-hero-image{opacity:.08}.post-hero-overlay{position:absolute;inset:0;background:radial-gradient(circle at 30% 50%,rgba(59,130,246,.1) 0%,transparent 50%),radial-gradient(circle at 70% 50%,rgba(139,92,246,.1) 0%,transparent 50%);z-index:1}.post-hero-content{position:relative;z-index:2;max-width:900px;margin:0 auto}.post-meta-badges{margin-bottom:1.5rem}.post-category-badge{display:inline-block;padding:.5rem 1.25rem;background:rgba(59,130,246,.15);border:1px solid rgba(59,130,246,.3);border-radius:50px;color:#60a5fa;font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em}[data-theme=light] .post-category-badge{background:rgba(59,130,246,.1);color:#3b82f6}.post-title{font-size:clamp(2rem,5vw,3.5rem);font-weight:400;line-height:1.1;margin:0 0 2rem;color:#fff;letter-spacing:-.02em}[data-theme=light] .post-title{color:#1e293b}.post-meta-info{display:flex;align-items:center;justify-content:center;gap:1.5rem;flex-wrap:wrap;margin-bottom:1.5rem;color:rgba(255,255,255,.8);font-size:.95rem}[data-theme=light] .post-meta-info{color:#64748b}.post-author{display:flex;align-items:center;gap:.75rem}.author-name{font-weight:600}.post-date-time{display:flex;align-items:center;gap:.5rem;color:rgba(255,255,255,.8)}[data-theme=light] .post-date-time{color:rgba(0,0,0,.7)}.post-date-time svg{width:16px;height:16px;opacity:.7}.meta-separator{opacity:.5}.post-tags-hero{display:flex;align-items:center;justify-content:center;gap:.75rem;flex-wrap:wrap}.tag-link{padding:.375rem 1rem;background:rgba(255,255,255,5%);border:1px solid rgba(255,255,255,.1);border-radius:50px;color:rgba(255,255,255,.8);font-size:.85rem;font-weight:500;text-decoration:none;transition:all .3s}.tag-link:hover{background:rgba(59,130,246,.2);border-color:rgba(59,130,246,.4);color:#60a5fa}[data-theme=light] .tag-link{background:rgba(0,0,0,4%);border-color:rgba(0,0,0,8%);color:#64748b}[data-theme=light] .tag-link:hover{background:rgba(59,130,246,.1);border-color:rgba(59,130,246,.3);color:#3b82f6}.post-content-container{max-width:1400px;margin:0 auto;padding:4rem 2rem}.post-content-grid{display:grid;grid-template-columns:1fr;gap:4rem;align-items:start;width:90%;margin:0 auto}.post-article-content{min-width:0}.prose-content{font-family:inter,-apple-system,BlinkMacSystemFont,segoe ui,system-ui,sans-serif;line-height:1.8;font-size:1.0625rem;font-weight:400}.prose-content h1,.prose-content h2,.prose-content h3,.prose-content h4,.prose-content h5,.prose-content h6{font-family:inter,-apple-system,BlinkMacSystemFont,segoe ui,system-ui,sans-serif!important;font-weight:400!important;line-height:1.4!important;margin-top:2.5rem!important;margin-bottom:1rem!important;color:var(--text-primary)!important;letter-spacing:-.01em!important;border:none!important;border-bottom:none!important;border-top:none!important;text-decoration:none!important;box-shadow:none!important;pointer-events:none!important;cursor:text!important}.prose-content h1::before,.prose-content h2::before,.prose-content h3::before,.prose-content h4::before,.prose-content h5::before,.prose-content h6::before,.prose-content h1::after,.prose-content h2::after,.prose-content h3::after,.prose-content h4::after,.prose-content h5::after,.prose-content h6::after{content:none!important;border:none!important;border-bottom:none!important;text-decoration:none!important}.prose-content h1:hover,.prose-content h2:hover,.prose-content h3:hover,.prose-content h4:hover,.prose-content h5:hover,.prose-content h6:hover,.prose-content h1:target,.prose-content h2:target,.prose-content h3:target,.prose-content h4:target,.prose-content h5:target,.prose-content h6:target,.prose-content h1[id],.prose-content h2[id],.prose-content h3[id],.prose-content h4[id],.prose-content h5[id],.prose-content h6[id]{text-decoration:none!important;border-bottom:none!important;cursor:text!important}.prose-content h1{font-size:2.25rem}.prose-content h2{font-size:1.875rem}.prose-content h3{font-size:1.5rem}.prose-content h4{font-size:1.25rem}.prose-content h5{font-size:1.125rem}.prose-content h6{font-size:1rem}.prose-content p{margin-bottom:1.5rem;color:var(--text-primary)}.prose-content a{color:#3b82f6;text-decoration:underline;text-underline-offset:3px;transition:color .2s}.prose-content a:hover{color:#60a5fa}.prose-content ul,.prose-content ol{margin-bottom:1.5rem;padding-left:1.75rem}.prose-content li{margin-bottom:.5rem}.prose-content blockquote{border-left:4px solid #3b82f6;padding-left:1.5rem;margin:1.5rem 0;font-style:italic;color:var(--text-secondary)}.prose-content code{background:rgba(59,130,246,.1);padding:.2em .4em;border-radius:4px;font-size:.9em;font-family:courier new,monospace;color:#3b82f6}[data-theme=light] .prose-content code{background:rgba(59,130,246,8%)}.prose-content pre{background:rgba(0,0,0,.3);padding:1.5rem;border-radius:12px;overflow-x:auto;margin:1.5rem 0;border:1px solid rgba(255,255,255,.1)}[data-theme=light] .prose-content pre{background:#f8fafc;border-color:rgba(0,0,0,8%)}.prose-content pre code{background:0 0;padding:0;color:inherit}.prose-content img{max-width:100%;height:auto;border-radius:12px;margin:2rem 0}.prose-content table{width:100%;border-collapse:collapse;margin:1.5rem 0}.prose-content th,.prose-content td{border:1px solid rgba(255,255,255,.1);padding:.75rem;text-align:left}[data-theme=light] .prose-content th,[data-theme=light] .prose-content td{border-color:rgba(0,0,0,8%)}.prose-content th{background:rgba(59,130,246,.1);font-weight:600}.prose-content hr{border:none;border-top:1px solid rgba(255,255,255,.1);margin:3rem 0}[data-theme=light] .prose-content hr{border-top-color:rgba(0,0,0,8%)}.prose-content .katex{font-size:1.1em}.prose-content .katex-display{overflow-x:auto;overflow-y:hidden;padding:1rem 0}.share-section{margin-top:4rem;padding-top:2rem;border-top:1px solid rgba(255,255,255,.1)}[data-theme=light] .share-section{border-top-color:rgba(0,0,0,8%)}.share-title{font-size:1.125rem;font-weight:600;margin-bottom:1rem;color:var(--text-primary)}.share-buttons{display:flex;gap:.75rem;flex-wrap:wrap}.share-btn{display:flex;align-items:center;justify-content:center;width:44px;height:44px;border-radius:50%;background:rgba(255,255,255,5%);border:1px solid rgba(255,255,255,.1);color:var(--text-secondary);text-decoration:none;transition:all .3s}.share-btn:hover{transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}.share-facebook:hover{background:#1877f2;border-color:#1877f2;color:#fff}.share-twitter:hover{background:#1da1f2;border-color:#1da1f2;color:#fff}.share-linkedin:hover{background:#0a66c2;border-color:#0a66c2;color:#fff}.share-reddit:hover{background:#ff4500;border-color:#ff4500;color:#fff}.share-whatsapp:hover{background:#25d366;border-color:#25d366;color:#fff}.share-email:hover{background:#6b7280;border-color:#6b7280;color:#fff}.improve-page-section{margin-top:3rem;text-align:center}.improve-link{display:inline-flex;align-items:center;gap:.5rem;padding:.75rem 1.5rem;background:rgba(255,255,255,5%);border:1px solid rgba(255,255,255,.1);border-radius:50px;color:var(--text-secondary);text-decoration:none;font-weight:500;transition:all .3s}.improve-link:hover{background:rgba(59,130,246,.1);border-color:rgba(59,130,246,.3);color:#3b82f6}.post-navigation{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:1.5rem;margin-top:4rem}.nav-card{padding:1.5rem;background:rgba(255,255,255,3%);border:1px solid rgba(255,255,255,8%);border-radius:12px;text-decoration:none;transition:all .3s;display:flex;flex-direction:column;gap:.5rem}[data-theme=light] .nav-card{background:rgba(255,255,255,.6);border-color:rgba(0,0,0,8%)}.nav-card:hover{border-color:rgba(59,130,246,.3);transform:translateY(-2px);box-shadow:0 8px 20px rgba(59,130,246,.1)}.nav-label{font-size:.875rem;color:var(--text-muted);font-weight:500}.nav-title{font-size:1.125rem;font-weight:600;color:var(--text-primary)}.comments-section{margin-top:5rem;padding-top:3rem;border-top:2px solid rgba(255,255,255,.1)}[data-theme=light] .comments-section{border-top-color:rgba(0,0,0,8%)}.comments-heading{font-size:1.75rem;font-weight:700;margin-bottom:2rem;color:var(--text-primary);display:flex;align-items:center;gap:.75rem}.comments-heading i{color:#3b82f6}.scroll-top-btn{position:fixed;bottom:2rem;right:2rem;width:50px;height:50px;border-radius:50%;background:rgba(59,130,246,.9);border:none;color:#fff;font-size:1.25rem;cursor:pointer;opacity:0;visibility:hidden;transition:all .3s;box-shadow:0 4px 12px rgba(59,130,246,.3);z-index:100}.scroll-top-btn.visible{opacity:1;visibility:visible}.scroll-top-btn:hover{background:#3b82f6;transform:translateY(-4px);box-shadow:0 8px 20px rgba(59,130,246,.4)}@media(max-width:768px){.modern-post-wrapper{padding-top:4rem}.post-hero-section{min-height:50vh;padding:3rem 1.5rem}.post-title{font-size:2rem}.post-meta-info{flex-direction:column;gap:1rem}.post-content-container{padding:2rem 1.5rem}.post-content-grid{width:90%}.prose-content{font-size:1rem}.prose-content h1{font-size:1.875rem}.prose-content h2{font-size:1.5rem}.prose-content h3{font-size:1.25rem}.scroll-top-btn{bottom:1.5rem;right:1.5rem;width:44px;height:44px}}</style><script>(function(){"use strict";const e=document.getElementById("scroll-to-top");window.addEventListener("scroll",()=>{window.pageYOffset>300?e.classList.add("visible"):e.classList.remove("visible")}),e.addEventListener("click",()=>{window.scrollTo({top:0,behavior:"smooth"})})})()</script></div><footer class=modern-footer><div class=footer-glow></div><div class=footer-container><div class=footer-content><div class=footer-brand><span class=brand-name>Deepak Baby</span><p class=brand-tagline>Senior Data Scientist at KBC Bank & Verzekering</p></div><div class=footer-social><a href=mailto:deepakbabycet@gmail.com class=social-icon aria-label=Email title=Email><i class="fas fa-envelope"></i>
</a><a href=https://www.github.com/deepakbaby class=social-icon target=_blank rel="noopener noreferrer" aria-label=Github title=Github><i class="fab fa-github"></i>
</a><a href=https://www.linkedin.com/in/deepakbaby/ class=social-icon target=_blank rel="noopener noreferrer" aria-label=LinkedIn title=LinkedIn><i class="fab fa-linkedin"></i>
</a><a href="https://scholar.google.com/citations?user=69q7FOYAAAAJ&amp;hl=en" class=social-icon target=_blank rel="noopener noreferrer" aria-label="Google Scholar" title="Google Scholar"><i class="fa-brands fa-google-scholar"></i></a></div><div class=footer-links><a href=#hero>Home</a>
<a href=#about>About</a>
<a href=#experiences>Experience</a>
<a href=#posts>Blog</a>
<a href=#publications>Publications</a></div></div><div class=footer-bottom><p class=copyright>© 2025 Deepak Baby. All rights reserved.</p><p class=credits>Made with <span class=heart>❤</span>, Hugo and Claude</p></div></div></footer><style>.modern-footer{position:relative;background:#050506;padding:4rem 0 2rem;overflow:hidden}.footer-glow{position:absolute;top:0;left:50%;transform:translateX(-50%);width:600px;height:2px;background:linear-gradient(90deg,transparent 0%,#3b82f6 25%,#1d4ed8 50%,#3b82f6 75%,transparent 100%);filter:blur(1px)}.footer-container{max-width:1200px;margin:0 auto;padding:0 2rem}.footer-content{display:flex;flex-direction:column;align-items:center;text-align:center;gap:2rem;margin-bottom:3rem}.footer-brand{margin-bottom:.5rem}.brand-name{font-size:1.5rem;font-weight:700;background:linear-gradient(135deg,#fff 0%,#a78bfa 100%);-webkit-background-clip:text;-webkit-text-fill-color:transparent;background-clip:text}.brand-tagline{font-size:.9rem;color:#6b7280;margin:.5rem 0 0}.footer-social{display:flex;gap:1rem}.social-icon{width:44px;height:44px;border-radius:50%;background:rgba(255,255,255,5%);border:1px solid rgba(255,255,255,8%);display:flex;align-items:center;justify-content:center;color:#9ca3af;font-size:1.1rem;text-decoration:none;transition:all .3s ease}.social-icon:hover{background:linear-gradient(135deg,#8b5cf6 0%,#3b82f6 100%);border-color:transparent;color:#fff;transform:translateY(-4px);box-shadow:0 8px 25px rgba(139,92,246,.3)}.footer-links{display:flex;flex-wrap:wrap;justify-content:center;gap:1.5rem}.footer-links a{color:#6b7280;text-decoration:none;font-size:.9rem;transition:color .3s}.footer-links a:hover{color:#a78bfa}.footer-bottom{border-top:1px solid rgba(255,255,255,5%);padding-top:2rem;display:flex;flex-direction:column;align-items:center;gap:.5rem}.copyright{font-size:.875rem;color:#4b5563;margin:0}.credits{font-size:.75rem;color:#374151;margin:0}.heart{color:#ef4444;animation:heartbeat 1.5s infinite}@keyframes heartbeat{0%,100%{transform:scale(1)}50%{transform:scale(1.2)}}@media(min-width:768px){.footer-content{flex-direction:row;justify-content:space-between;text-align:left}.footer-bottom{flex-direction:row;justify-content:space-between}}</style><script src=/application.f6959e0d840b2a0cb8ebdc7b64f82c76ed540bafda2e2f31ece1d91e812ba4d1.js integrity="sha256-9pWeDYQLKgy469x7ZPgsdu1UC6/aLi8x7OHZHoErpNE=" defer></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1})'></script><script src=/js/collapsible-sidebars.js></script></body></html>