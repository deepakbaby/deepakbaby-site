<!doctype html><html lang=en><head><title>Distributed Training Techniques: DDP, Pipeline Parallelism, and FSDP</title><style>:root{--bg-primary:#0f172a;--bg-secondary:#1e293b;--bg-card:rgba(255, 255, 255, 0.03);--text-primary:#fff;--text-secondary:#9ca3af;--text-muted:#6b7280;--border-color:rgba(255, 255, 255, 0.08);--orb-opacity:0.5;--toggle-bg:rgba(255, 255, 255, 0.1);--toggle-border:rgba(255, 255, 255, 0.2);--social-bg:rgba(255, 255, 255, 0.05);--social-border:rgba(255, 255, 255, 0.1);--mouse-border:rgba(255, 255, 255, 0.2);--wheel-bg:rgba(255, 255, 255, 0.4);--card-hover-border:rgba(59, 130, 246, 0.3);--card-hover-shadow:rgba(59, 130, 246, 0.15)}[data-theme=light]{--bg-primary:#fafbfc;--bg-secondary:#ffffff;--bg-card:transparent;--text-primary:#1e293b;--text-secondary:#475569;--text-muted:#94a3b8;--border-color:rgba(0, 0, 0, 0.06);--orb-opacity:0.1;--toggle-bg:rgba(0, 0, 0, 0.04);--toggle-border:rgba(0, 0, 0, 0.08);--social-bg:transparent;--social-border:rgba(0, 0, 0, 0.1);--mouse-border:rgba(0, 0, 0, 0.12);--wheel-bg:rgba(0, 0, 0, 0.2);--card-hover-border:rgba(59, 130, 246, 0.3);--card-hover-shadow:rgba(59, 130, 246, 0.08);--card-bg:transparent;--card-border:rgba(0, 0, 0, 0.04);--timeline-color:#3b82f6;--section-bg:#ffffff;--glow-color:rgba(59, 130, 246, 0.08)}body{background-color:var(--bg-primary);color:var(--text-primary)}</style><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.c783c6e6b679af484325eb2b13483dfca8aa9b848c0b6e81bd4022ece7a9a0ef.css integrity="sha256-x4PG5rZ5r0hDJesrE0g9/Kiqm4SMC26BvUAi7OepoO8="><link rel=icon type=image/png href=/images/site/favicon_hu_be34366aac4a405.png><meta property="og:title" content="Deepak Baby"><meta property="og:type" content="website"><meta property="og:description" content="Personal website of Deepak Baby."><meta property="og:image" content="/images/author/deepak.jpeg"><meta property="og:url" content="https://deepakbaby.in"><meta name=description content="A comprehensive guide to distributed training techniques in ML - understanding Data Distributed Parallel (DDP), Pipeline Parallelism, and Fully Sharded Data Parallel (FSDP) with their pros and cons."><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=stylesheet href=/css/collapsible-sidebars.css><link rel=stylesheet href=/css/comments.css><script integrity="sha256-DO4ugzEwhTW1Id1UIWn0gUJWaebCYOypeTit6LW4QB4=">let theme=localStorage.getItem("theme-scheme")||localStorage.getItem("darkmode:color-scheme")||"light";theme==="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid wrapper"><button class=theme-toggle-fixed id=themeToggle aria-label="Toggle theme"><svg class="sun-icon" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><svg class="moon-icon" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg></button><nav class=scroll-navbar id=scrollNavbar><div class=navbar-container><a href=https://deepakbaby.in/ class=navbar-brand><img src=/images/site/favicon.png alt class=navbar-favicon>
Deepak Baby</a><div class=navbar-links><a href=https://deepakbaby.in/#about class=navbar-link>About</a>
<a href=https://deepakbaby.in/#experiences class=navbar-link>Experience</a>
<a href=/posts class=navbar-link>Blog</a>
<a href=https://deepakbaby.in/publications class=navbar-link>Publications</a></div><button class=theme-toggle-navbar id=themeToggleNav aria-label="Toggle theme"><svg class="sun-icon" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><svg class="moon-icon" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg></button></div></nav><style>.theme-toggle-fixed{position:fixed;top:1.5rem;right:1.5rem;width:44px;height:44px;border-radius:50%;background:var(--toggle-bg,rgba(255,255,255,.1));border:1px solid var(--toggle-border,rgba(255,255,255,.2));display:flex;align-items:center;justify-content:center;cursor:pointer;transition:all .3s;color:var(--text-primary,#fff);z-index:1000}.theme-toggle-fixed:hover{background:var(--toggle-hover-bg,rgba(255,255,255,.2));transform:scale(1.05)}.theme-toggle-fixed svg{width:22px;height:22px}.scroll-navbar.visible~.theme-toggle-fixed,.theme-toggle-fixed.hidden{opacity:0;pointer-events:none}.scroll-navbar.visible~.section-wrapper .theme-toggle-fixed,body:has(.scroll-navbar.visible) .theme-toggle-fixed{opacity:0;pointer-events:none}.scroll-navbar{position:fixed;top:0;left:0;right:0;background:var(--navbar-bg,rgba(15,23,42,.95));backdrop-filter:blur(20px);border-bottom:1px solid var(--border-color,rgba(255,255,255,8%));z-index:999;transform:translateY(-100%);transition:transform .3s ease}.scroll-navbar.visible{transform:translateY(0)}.navbar-container{max-width:1200px;margin:0 auto;padding:1rem 2rem;display:flex;align-items:center;justify-content:space-between}.navbar-brand{display:flex;align-items:center;gap:.75rem;font-family:inter,sans-serif;font-size:1.125rem;font-weight:500;color:var(--text-primary,#fff);text-decoration:none!important;transition:transform .3s ease,color .3s}.navbar-brand:hover,.navbar-brand:focus,.navbar-brand:active{transform:translateY(-2px);color:#3b82f6;text-decoration:none!important}.navbar-favicon{width:28px;height:28px;border-radius:6px}.navbar-links{display:flex;gap:2rem}.navbar-link{font-family:inter,sans-serif;font-size:.875rem;font-weight:400;color:var(--text-secondary,rgba(255,255,255,.7));text-decoration:none!important;transition:transform .3s ease,color .3s;display:inline-block}.navbar-link:hover,.navbar-link:focus,.navbar-link:active{transform:translateY(-2px);color:var(--text-primary,#fff);text-decoration:none!important}.theme-toggle-navbar{width:36px;height:36px;border-radius:50%;background:var(--toggle-bg,rgba(255,255,255,.1));border:1px solid var(--toggle-border,rgba(255,255,255,.2));display:flex;align-items:center;justify-content:center;cursor:pointer;transition:all .3s;color:var(--text-primary,#fff)}.theme-toggle-navbar:hover{background:var(--toggle-hover-bg,rgba(255,255,255,.2))}.theme-toggle-navbar svg{width:18px;height:18px}.sun-icon{display:none}.moon-icon{display:block}[data-theme=light] .sun-icon{display:block}[data-theme=light] .moon-icon{display:none}[data-theme=light] .scroll-navbar{--navbar-bg:rgba(255, 255, 255, 0.98);box-shadow:0 1px 8px rgba(0,0,0,5%);border-bottom-color:transparent}@media(max-width:768px){.theme-toggle-fixed{top:1rem;right:1rem;width:40px;height:40px}.navbar-links{display:none}}</style><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("themeToggle"),a=document.getElementById("themeToggleNav"),t=document.getElementById("scrollNavbar"),n=document.documentElement,r=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: light)").matches?"light":"dark");n.setAttribute("data-theme",r);function s(){const t=n.getAttribute("data-theme"),e=t==="dark"?"light":"dark";n.setAttribute("data-theme",e),localStorage.setItem("theme",e)}e?.addEventListener("click",s),a?.addEventListener("click",s);const o=document.querySelector(".hero-section"),i=o?o.offsetHeight*.6:100;window.addEventListener("scroll",()=>{const n=window.pageYOffset;n>i?(t?.classList.add("visible"),e?.classList.add("hidden")):(t?.classList.remove("visible"),e?.classList.remove("hidden"))}),window.pageYOffset>i&&(t?.classList.add("visible"),e?.classList.add("hidden"))})</script><style>.type-posts .container-fluid.wrapper{max-width:none!important;width:100%!important;padding:0!important;margin:0!important;overflow-x:hidden!important}body.type-posts{overflow-x:hidden!important;width:100vw!important;max-width:100vw!important}html{overflow-x:hidden!important}</style><div class=modern-post-wrapper><section class=post-hero-section><div class=post-hero-overlay></div><div class=post-hero-image style=background-image:url(/posts/distributed-training/dt.png)></div><div class=post-hero-content><div class=post-meta-badges><span class=post-category-badge>deep learning</span></div><h1 class=post-title>Distributed Training Techniques: DDP, Pipeline Parallelism, and FSDP</h1><div class=post-meta-info><div class=post-author><span class=author-name>Deepak Baby</span></div><div class=post-date-time><svg width="16" height="16" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5A2 2 0 003 7v12a2 2 0 002 2z"/></svg>
<span>Monday, December 1, 2025</span>
<span class=meta-separator>•</span>
<svg width="16" height="16" fill="none" stroke="currentcolor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3A9 9 0 113 12a9 9 0 0118 0z"/></svg>
<span>8 min read</span></div></div><div class=post-tags-hero><a href=/tags/deep-learning class=tag-link>#deep learning</a>
<a href=/tags/distributed-training class=tag-link>#distributed training</a>
<a href=/tags/ddp class=tag-link>#DDP</a>
<a href=/tags/fsdp class=tag-link>#FSDP</a>
<a href=/tags/pipeline-parallelism class=tag-link>#pipeline parallelism</a>
<a href=/tags/pytorch class=tag-link>#PyTorch</a></div></div></section><div class=post-content-container><div class=post-content-grid><article class=post-article-content><div class=prose-content id=post-content><p>Modern deep learning models have grown exponentially in size and complexity. GPT-4 has over a trillion parameters, and even &ldquo;smaller&rdquo; models like LLaMA-70B require substantial computational resources. Training or fine-tuning such models on a single GPU is often impossible; not just because of time constraints, but because the model itself may not fit in the memory of a single device. This is where <strong>distributed training</strong> becomes essential.</p><h2 id=why-do-we-need-distributed-training>Why Do We Need Distributed Training?</h2><h3 id=the-memory-wall-problem>The Memory Wall Problem</h3><p>A modern GPU like the NVIDIA A100 has 80GB of memory. Sounds like a lot? Let&rsquo;s do some math:</p><ul><li>A model with 7 billion parameters in FP32 requires: $7B \times 4 \text{ bytes} = 28\text{GB}$</li><li>But during training, we also need:<ul><li>Gradients: another 28GB</li><li>Optimizer states (Adam has 2 momentum terms): 56GB more</li><li>Activations for backpropagation: varies, but often substantial</li></ul></li></ul><p>A 7B parameter model can easily require 150GB+ during training, far exceeding what a single GPU can handle.</p><h3 id=the-time-constraint>The Time Constraint</h3><p>Even if a model fits in memory, training on a single GPU can take prohibitively long. Consider:</p><ul><li>Training GPT-3 on a single V100 GPU would take approximately <strong>355 years</strong></li><li>With distributed training across thousands of GPUs, this was reduced to <strong>weeks</strong></li></ul><h3 id=types-of-parallelism>Types of Parallelism</h3><p>Distributed training employs different parallelism strategies:</p><table><thead><tr><th>Strategy</th><th>What&rsquo;s Parallelized</th><th>When to Use</th></tr></thead><tbody><tr><td><strong>Data Parallelism</strong></td><td>Training data across replicas</td><td>Large datasets, model fits in single GPU</td></tr><tr><td><strong>Model/Tensor Parallelism</strong></td><td>Model layers across devices</td><td>Very large layers (e.g., attention in transformers)</td></tr><tr><td><strong>Pipeline Parallelism</strong></td><td>Model stages across devices</td><td>Deep models with many sequential layers</td></tr><tr><td><strong>Hybrid (3D Parallelism)</strong></td><td>Combination of above</td><td>Extremely large models (100B+ parameters)</td></tr></tbody></table><p><div style=margin-top:2rem></div>This post dives deep into three fundamental approaches: <strong>DDP</strong>, <strong>Pipeline Parallelism</strong>, and <strong>FSDP</strong>.</p><hr><h2 id=distributed-data-parallel-ddp>Distributed Data Parallel (DDP)</h2><p>DDP is the most straightforward and commonly used approach for distributed training. The core idea is simple: replicate the entire model on each GPU, split the data batch across GPUs, and synchronize gradients.</p><h3 id=how-ddp-works>How DDP Works</h3><p>Click the Play button to see a visualization of how DDP works.</p><style>.embedded-html-wrapper{position:relative;margin:2rem 0;border-radius:14px;overflow:hidden;border:1px solid rgba(255,255,255,8%);background:radial-gradient(circle at top right,rgba(255,255,255,5%),rgba(10,10,10,.85));box-shadow:0 12px 35px rgba(0,0,0,.45)}.embedded-html-toolbar{display:flex;align-items:center;justify-content:flex-end;gap:.5rem;padding:.75rem .9rem;background:rgba(4,4,4,.95);border-bottom:1px solid rgba(255,255,255,5%);font-family:-apple-system,BlinkMacSystemFont,segoe ui,Roboto,sans-serif;color:rgba(255,255,255,.9);backdrop-filter:blur(8px)}.embedded-html-toolbar-label{margin-right:auto;font-size:.9rem;letter-spacing:.02em;text-transform:uppercase;color:#a7a7a7}.embedded-html-toolbar-button{background:rgba(255,255,255,8%);color:rgba(255,255,255,.9);border:1px solid rgba(255,255,255,.15);border-radius:8px;padding:.35rem .7rem;display:inline-flex;align-items:center;justify-content:center;gap:.25rem;font-size:.9rem;font-weight:600;cursor:pointer;transition:transform .2s ease,background .2s ease,box-shadow .2s ease;line-height:1;position:relative;backdrop-filter:blur(6px)}.embedded-html-toolbar-button:hover{background:rgba(255,255,255,.16);transform:translateY(-1px);box-shadow:0 6px 14px rgba(0,0,0,.35)}.embedded-html-toolbar-button:active{transform:translateY(0)}.embedded-html-toolbar-button svg{width:18px;height:18px;fill:currentColor}.embedded-html-toolbar-button[data-active=true]{background:rgba(255,255,255,.14);border-color:rgba(255,255,255,.3);box-shadow:0 0 0 2px rgba(255,255,255,.2)}.sr-only{position:absolute;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;clip:rect(0,0,0,0);white-space:nowrap;border:0}.embedded-html-wrapper iframe{width:100%;border:none;display:block;aspect-ratio:16/9;min-height:400px;background:#000}@media(max-width:768px){.embedded-html-toolbar{flex-wrap:wrap;gap:.65rem}.embedded-html-toolbar-label{width:100%;margin:0;text-align:center}}</style><div class=embedded-html-wrapper data-src=/visualizations/ddp.html><div class=embedded-html-toolbar><span class=embedded-html-toolbar-label>DDP visualization</span>
<button type=button class=embedded-html-toolbar-button data-action=open-new-tab aria-label="Open the DDP visualization in a new tab">
<svg viewBox="0 0 24 24" aria-hidden="true"><path d="M14 3h7v7h-2V6.414l-9.293 9.293-1.414-1.414L17.586 5H14V3zM5 5h5v2H6v11h11v-4h2v5H5V5z"/></svg>
<span class=sr-only>Open DDP visualization in new tab</span>
</button>
<button type=button class=embedded-html-toolbar-button data-action=fullscreen data-label-base="Enter fullscreen view for the DDP visualization" data-label-active="Exit fullscreen view for the DDP visualization" aria-label="Enter fullscreen view for the DDP visualization">
<svg viewBox="0 0 24 24" aria-hidden="true"><path d="M4 4h6v2H6v4H4V4zm10 0h6v6h-2V6h-4V4zm6 10v6h-6v-2h4v-4h2zm-10 6H4v-6h2v4h4v2z"/></svg>
<span class=sr-only>Toggle fullscreen for DDP visualization</span></button></div><iframe src=/visualizations/ddp.html loading=lazy allowfullscreen frameborder=0 scrolling=no title="Distributed Data Parallel visualization"></iframe></div><ol><li><strong>Model Replication</strong>: Each GPU gets a complete copy of the model with identical initial weights</li><li><strong>Data Sharding</strong>: The training batch is split equally among all GPUs</li><li><strong>Forward Pass</strong>: Each GPU processes its data shard independently</li><li><strong>Backward Pass</strong>: Each GPU computes gradients for its local data</li><li><strong>Gradient Synchronization</strong>: All GPUs synchronize their gradients using <strong>AllReduce</strong></li><li><strong>Weight Update</strong>: Each GPU applies the synchronized gradients to update its local model</li></ol><p>The magic happens in the <strong>AllReduce</strong> operation, which efficiently computes the average of gradients across all GPUs and distributes the result back to each GPU.</p><h3 id=allreduce-the-heart-of-ddp>AllReduce: The Heart of DDP</h3><p>AllReduce is a collective communication operation that:</p><ol><li>Takes input tensors from all processes</li><li>Applies a reduction operation (typically sum or average)</li><li>Distributes the result to all processes</li></ol><h3 id=pros-of-ddp>Pros of DDP</h3><table><thead><tr><th>Advantage</th><th>Description</th></tr></thead><tbody><tr><td>✅ <strong>Simple Implementation</strong></td><td>Minimal code changes required; wrap model in <code>DistributedDataParallel</code></td></tr><tr><td>✅ <strong>Linear Scaling</strong></td><td>Near-linear speedup with more GPUs for communication-bound scenarios</td></tr><tr><td>✅ <strong>No Model Changes</strong></td><td>Works with any model architecture without modifications</td></tr><tr><td>✅ <strong>Fault Tolerance</strong></td><td>Easy to checkpoint and resume training</td></tr><tr><td>✅ <strong>Overlapping Communication</strong></td><td>Gradient sync overlaps with backward computation</td></tr></tbody></table><h3 id=cons-of-ddp>Cons of DDP</h3><table><thead><tr><th>Disadvantage</th><th>Description</th></tr></thead><tbody><tr><td>❌ <strong>Memory Redundancy</strong></td><td>Full model replicated on each GPU</td></tr><tr><td>❌ <strong>Model Size Limit</strong></td><td>Model must fit entirely in single GPU memory</td></tr><tr><td>❌ <strong>Communication Overhead</strong></td><td>AllReduce scales with model size</td></tr><tr><td>❌ <strong>Synchronization Barrier</strong></td><td>All GPUs must wait for slowest one (stragglers)</td></tr></tbody></table><h3 id=when-to-use-ddp>When to Use DDP</h3><p>DDP is ideal when:</p><ul><li>Your model fits comfortably in a single GPU</li><li>You want simple, robust distributed training</li><li>You&rsquo;re scaling across multiple machines with fast interconnects</li></ul><hr><h2 id=pipeline-parallelism>Pipeline Parallelism</h2><p>When models are too large to fit on a single GPU, we need to partition them across devices. Pipeline Parallelism splits the model into <strong>stages</strong>, where each stage runs on a different GPU.</p><h3 id=how-pipeline-parallelism-works>How Pipeline Parallelism Works</h3><p>Click the Play button.</p><div class=embedded-html-wrapper data-src=/visualizations/pipeline.html><div class=embedded-html-toolbar><span class=embedded-html-toolbar-label>Pipeline parallelism</span>
<button type=button class=embedded-html-toolbar-button data-action=open-new-tab aria-label="Open the pipeline parallelism visualization in a new tab">
<svg viewBox="0 0 24 24" aria-hidden="true"><path d="M14 3h7v7h-2V6.414l-9.293 9.293-1.414-1.414L17.586 5H14V3zM5 5h5v2H6v11h11v-4h2v5H5V5z"/></svg>
<span class=sr-only>Open pipeline parallelism visualization in new tab</span>
</button>
<button type=button class=embedded-html-toolbar-button data-action=fullscreen data-label-base="Enter fullscreen view for the pipeline parallelism visualization" data-label-active="Exit fullscreen view for the pipeline parallelism visualization" aria-label="Enter fullscreen view for the pipeline parallelism visualization">
<svg viewBox="0 0 24 24" aria-hidden="true"><path d="M4 4h6v2H6v4H4V4zm10 0h6v6h-2V6h-4V4zm6 10v6h-6v-2h4v-4h2zm-10 6H4v-6h2v4h4v2z"/></svg>
<span class=sr-only>Toggle fullscreen for pipeline parallelism visualization</span></button></div><iframe src=/visualizations/pipeline.html loading=lazy allowfullscreen frameborder=0 scrolling=no title="Pipeline parallelism visualization"></iframe></div><ol><li><strong>Model Partitioning</strong>: Split model into N sequential stages</li><li><strong>Stage Assignment</strong>: Each GPU handles one or more stages</li><li><strong>Micro-batching</strong>: Split input batch into smaller micro-batches</li><li><strong>Pipeline Execution</strong>: Process micro-batches in a pipelined fashion</li></ol><h4 id=the-bubble-problem>The Bubble Problem</h4><p>Naive pipeline parallelism has a significant issue, <strong>pipeline bubbles</strong>. Notice the <code>Device Utilization Timeline</code> in the above animation as training progresses. It shows &ldquo;bubbles&rdquo; where GPUs sit idle, waiting for data from previous stages, leading to wasted compute resources.</p><h4 id=reducing-bubbles-with-micro-batching>Reducing Bubbles with Micro-batching</h4><p>The key optimization is to use <strong>many micro-batches</strong>:</p><p>$$\text{Bubble Fraction} = \frac{p - 1}{m}$$</p><p>Where $p$ is the number of pipeline stages and $m$ is the number of micro-batches. With more micro-batches, the bubble overhead becomes negligible.</p><h3 id=pipeline-schedules>Pipeline Schedules</h3><p>Different scheduling strategies minimize bubbles:</p><table><thead><tr><th>Schedule</th><th>Description</th><th>Memory</th><th>Bubble Ratio</th></tr></thead><tbody><tr><td><strong>GPipe</strong></td><td>All forward, then all backward</td><td>High (stores activations)</td><td>$(p-1)/m$</td></tr><tr><td><strong>1F1B</strong></td><td>Alternates forward/backward</td><td>Lower</td><td>$(p-1)/m$</td></tr><tr><td><strong>Interleaved 1F1B</strong></td><td>Virtual stages</td><td>Lowest</td><td>$(p-1)/(m \cdot v)$</td></tr></tbody></table><h3 id=pros-of-pipeline-parallelism>Pros of Pipeline Parallelism</h3><table><thead><tr><th>Advantage</th><th>Description</th></tr></thead><tbody><tr><td>✅ <strong>Scales Model Size</strong></td><td>Train models larger than single GPU memory</td></tr><tr><td>✅ <strong>Lower Communication</strong></td><td>Only activations transferred between stages</td></tr><tr><td>✅ <strong>Works with Sequential Models</strong></td><td>Natural fit for transformer layers</td></tr><tr><td>✅ <strong>Memory Efficient</strong></td><td>Each GPU only holds subset of model</td></tr></tbody></table><h3 id=cons-of-pipeline-parallelism>Cons of Pipeline Parallelism</h3><table><thead><tr><th>Disadvantage</th><th>Description</th></tr></thead><tbody><tr><td>❌ <strong>Pipeline Bubbles</strong></td><td>Idle time reduces GPU utilization</td></tr><tr><td>❌ <strong>Complex Implementation</strong></td><td>Requires careful model partitioning</td></tr><tr><td>❌ <strong>Load Balancing</strong></td><td>Stages must have similar compute cost</td></tr><tr><td>❌ <strong>Increased Latency</strong></td><td>Forward pass must traverse all stages</td></tr><tr><td>❌ <strong>Gradient Staleness</strong></td><td>Some schedules have delayed gradient updates</td></tr></tbody></table><h3 id=when-to-use-pipeline-parallelism>When to Use Pipeline Parallelism</h3><p>Pipeline parallelism shines when:</p><ul><li>Model doesn&rsquo;t fit on a single GPU but isn&rsquo;t excessively large</li><li>Model has clear sequential structure (e.g., transformer blocks)</li><li>You have limited inter-GPU bandwidth</li><li>Combined with data parallelism for better scaling</li></ul><hr><h2 id=fully-sharded-data-parallel-fsdp>Fully Sharded Data Parallel (FSDP)</h2><p>FSDP represents a paradigm shift in distributed training. Instead of replicating the entire model on each GPU (like DDP), FSDP <strong>shards</strong> the model parameters, gradients, and optimizer states across all GPUs.</p><h3 id=how-fsdp-works>How FSDP Works</h3><div class=embedded-html-wrapper data-src=/visualizations/fsdp.html><div class=embedded-html-toolbar><span class=embedded-html-toolbar-label>FSDP visualization</span>
<button type=button class=embedded-html-toolbar-button data-action=open-new-tab aria-label="Open the FSDP visualization in a new tab">
<svg viewBox="0 0 24 24" aria-hidden="true"><path d="M14 3h7v7h-2V6.414l-9.293 9.293-1.414-1.414L17.586 5H14V3zM5 5h5v2H6v11h11v-4h2v5H5V5z"/></svg>
<span class=sr-only>Open FSDP visualization in new tab</span>
</button>
<button type=button class=embedded-html-toolbar-button data-action=fullscreen data-label-base="Enter fullscreen view for the FSDP visualization" data-label-active="Exit fullscreen view for the FSDP visualization" aria-label="Enter fullscreen view for the FSDP visualization">
<svg viewBox="0 0 24 24" aria-hidden="true"><path d="M4 4h6v2H6v4H4V4zm10 0h6v6h-2V6h-4V4zm6 10v6h-6v-2h4v-4h2zm-10 6H4v-6h2v4h4v2z"/></svg>
<span class=sr-only>Toggle fullscreen for FSDP visualization</span></button></div><iframe src=/visualizations/fsdp.html loading=lazy allowfullscreen frameborder=0 scrolling=no title="Fully Sharded Data Parallel visualization"></iframe></div><p>FSDP follows a <strong>gather-compute-scatter</strong> pattern:</p><ol><li><strong>Sharding</strong>: Model parameters are partitioned across all GPUs</li><li><strong>AllGather</strong>: Before forward pass, gather full parameters for current layer</li><li><strong>Forward Compute</strong>: Execute layer with full parameters</li><li><strong>Discard</strong>: After forward, discard non-local parameters to save memory</li><li><strong>Repeat</strong> for backward pass: AllGather → Compute gradients → ReduceScatter</li><li><strong>ReduceScatter</strong>: Distribute and reduce gradients back to shards</li></ol><h4 id=memory-savings>Memory Savings</h4><p>The memory savings with FSDP are dramatic:</p><table><thead><tr><th>Component</th><th>DDP Memory</th><th>FSDP Memory</th></tr></thead><tbody><tr><td>Parameters</td><td>$\Phi$ per GPU</td><td>$\Phi / N$ per GPU</td></tr><tr><td>Gradients</td><td>$\Phi$ per GPU</td><td>$\Phi / N$ per GPU</td></tr><tr><td>Optimizer States</td><td>$2\Phi$ per GPU (Adam)</td><td>$2\Phi / N$ per GPU</td></tr><tr><td><strong>Total</strong></td><td>$4\Phi$</td><td>$4\Phi / N$</td></tr></tbody></table><p>Where $\Phi$ is model size and $N$ is number of GPUs.</p><p>For a 7B model on 8 GPUs:</p><ul><li><strong>DDP</strong>: 28GB × 4 = 112GB per GPU (doesn&rsquo;t fit on 80GB A100!)</li><li><strong>FSDP</strong>: 112GB / 8 = 14GB per GPU ✓</li></ul><h4 id=sharding-strategies>Sharding Strategies</h4><p>FSDP offers flexible sharding strategies:</p><table><thead><tr><th>Strategy</th><th>What&rsquo;s Sharded</th><th>Memory</th><th>Communication</th></tr></thead><tbody><tr><td><strong>FULL_SHARD</strong></td><td>Params, Grads, Optimizer</td><td>Minimum</td><td>Maximum</td></tr><tr><td><strong>SHARD_GRAD_OP</strong></td><td>Grads, Optimizer</td><td>Medium</td><td>Medium</td></tr><tr><td><strong>NO_SHARD</strong></td><td>Nothing (like DDP)</td><td>Maximum</td><td>Minimum</td></tr></tbody></table><h3 id=pros-of-fsdp>Pros of FSDP</h3><table><thead><tr><th>Advantage</th><th>Description</th></tr></thead><tbody><tr><td>✅ <strong>Massive Memory Savings</strong></td><td>Linear reduction in memory with GPU count</td></tr><tr><td>✅ <strong>Train Huge Models</strong></td><td>Enable training of models that don&rsquo;t fit on single GPU</td></tr><tr><td>✅ <strong>Flexible Sharding</strong></td><td>Choose tradeoff between memory and communication</td></tr><tr><td>✅ <strong>Native PyTorch</strong></td><td>Well-integrated into PyTorch ecosystem</td></tr><tr><td>✅ <strong>Mixed Precision</strong></td><td>Works seamlessly with AMP/BF16</td></tr><tr><td>✅ <strong>Activation Checkpointing</strong></td><td>Combines well with gradient checkpointing</td></tr></tbody></table><h3 id=cons-of-fsdp>Cons of FSDP</h3><table><thead><tr><th>Disadvantage</th><th>Description</th></tr></thead><tbody><tr><td>❌ <strong>Communication Overhead</strong></td><td>More collective operations than DDP</td></tr><tr><td>❌ <strong>Complexity</strong></td><td>More configuration options to tune</td></tr><tr><td>❌ <strong>Debugging Difficulty</strong></td><td>Harder to debug distributed sharded state</td></tr><tr><td>❌ <strong>Checkpoint Complexity</strong></td><td>Saving/loading requires special handling</td></tr><tr><td>❌ <strong>Latency</strong></td><td>AllGather adds latency before each layer</td></tr></tbody></table><h3 id=when-to-use-fsdp>When to Use FSDP</h3><p>FSDP is the right choice when:</p><ul><li>Model doesn&rsquo;t fit on single GPU even with mixed precision</li><li>You need to train models with billions of parameters</li><li>You have fast GPU interconnects (NVLink, InfiniBand)</li><li>Memory is the primary bottleneck</li></ul><hr><h2 id=comparison-summary>Comparison Summary</h2><table><thead><tr><th>Aspect</th><th>DDP</th><th>Pipeline Parallel</th><th>FSDP</th></tr></thead><tbody><tr><td><strong>Memory per GPU</strong></td><td>Full model</td><td>Model / stages</td><td>Model / GPUs</td></tr><tr><td><strong>Communication</strong></td><td>AllReduce</td><td>Point-to-point</td><td>AllGather + ReduceScatter</td></tr><tr><td><strong>Complexity</strong></td><td>Low</td><td>Medium</td><td>Medium-High</td></tr><tr><td><strong>Model Size Limit</strong></td><td>Single GPU</td><td>Total GPU memory</td><td>Total GPU memory</td></tr><tr><td><strong>GPU Utilization</strong></td><td>High</td><td>Medium (bubbles)</td><td>High</td></tr><tr><td><strong>Best For</strong></td><td>Small-Medium models</td><td>Sequential models</td><td>Large models</td></tr></tbody></table><h2 id=choosing-the-right-strategy>Choosing the Right Strategy</h2><div class=mermaid align=center style=background-color:none;border-radius:5px>graph LR;
A[Model] --> B{Fit on single GPU?}
B --> |YES| G[Use DDP]
B --> |NO| C{Do you have fast interconnect?}
C --> |YES| E[Use FSDP]
C --> |NO| F[Use Pipeline Parallel]</div><h2 id=practical-recommendations>Practical Recommendations</h2><ol><li><p><strong>Start with DDP</strong> if your model fits on a single GPU. DDP is the simplest and most efficient.</p></li><li><p><strong>Switch to FSDP</strong> when memory becomes the bottleneck. Start with <code>SHARD_GRAD_OP</code> for less communication overhead.</p></li><li><p><strong>Add Pipeline Parallelism</strong> for very deep models, especially when combined with FSDP for each pipeline stage.</p></li><li><p><strong>Use 3D Parallelism</strong> (Data + Pipeline + Tensor) for even bigger models (100B+ parameters).</p></li><li><p><strong>Profile and measure</strong>: Use tools like PyTorch Profiler to identify bottlenecks.</p></li></ol><h2 id=conclusion>Conclusion</h2><p>Distributed training is essential for working with state-of-the-art neural network models. Understanding these three fundamental approaches gives you the tools to train models of any size.</p><ul><li><strong>DDP</strong> for simplicity and efficiency with smaller models</li><li><strong>Pipeline Parallelism</strong> for scaling deep sequential models</li><li><strong>FSDP</strong> for massive models that exceed single-GPU memory</li></ul><hr><h2 id=further-reading>Further Reading</h2><ul><li><a href=https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>PyTorch DDP Documentation</a></li><li><a href=https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>PyTorch FSDP Tutorial</a></li><li><a href=https://arxiv.org/abs/1811.06965>GPipe Paper</a></li><li><a href=https://arxiv.org/abs/1910.02054>ZeRO: Memory Optimization Toward Training Trillion Parameter Models</a></li><li><a href=https://arxiv.org/abs/1909.08053>Megatron-LM: Training Multi-Billion Parameter Language Models</a></li></ul><script>(function(){function e(){const t=document.querySelectorAll('.embedded-html-wrapper [data-action="fullscreen"]'),n=document.querySelectorAll('.embedded-html-wrapper [data-action="open-new-tab"]');if(!t.length&&!n.length)return;function s(){return document.fullscreenElement||document.webkitFullscreenElement||document.msFullscreenElement||null}function o(e){return e?e.requestFullscreen?e.requestFullscreen():e.webkitRequestFullscreen?e.webkitRequestFullscreen():e.msRequestFullscreen?e.msRequestFullscreen():Promise.resolve():Promise.resolve()}function i(){return document.exitFullscreen?document.exitFullscreen():document.webkitExitFullscreen?document.webkitExitFullscreen():document.msExitFullscreen?document.msExitFullscreen():Promise.resolve()}function e(){const e=s();t.forEach(t=>{const n=t.closest(".embedded-html-wrapper"),s=n&&e===n,i=t.dataset.labelBase||t.getAttribute("aria-label")||"Toggle fullscreen",o=t.dataset.labelActive;t.dataset.active=s?"true":"false",t.setAttribute("aria-label",s&&o?o:i)})}t.forEach(e=>{e.addEventListener("click",()=>{const t=e.closest(".embedded-html-wrapper");if(!t)return;const n=s();n&&n!==t?i().finally(()=>o(t)):n===t?i():o(t)})}),n.forEach(e=>{e.addEventListener("click",()=>{const n=e.closest(".embedded-html-wrapper"),t=n?.dataset.src;if(!t)return;window.open(t,"_blank","noopener")})}),document.addEventListener("fullscreenchange",e),document.addEventListener("webkitfullscreenchange",e),document.addEventListener("msfullscreenchange",e),e()}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",e):e()})()</script></div><div class=share-section><h4 class=share-title>Share this post</h4><div class=share-buttons><a class="share-btn share-facebook" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdeepakbaby.in%2fposts%2fdistributed-training%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook-f"></i>
</a><a class="share-btn share-twitter" href="https://twitter.com/share?url=https%3a%2f%2fdeepakbaby.in%2fposts%2fdistributed-training%2f&text=Distributed%20Training%20Techniques%3a%20DDP%2c%20Pipeline%20Parallelism%2c%20and%20FSDP" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a class="share-btn share-linkedin" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fdeepakbaby.in%2fposts%2fdistributed-training%2f&title=Distributed%20Training%20Techniques%3a%20DDP%2c%20Pipeline%20Parallelism%2c%20and%20FSDP" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin-in"></i>
</a><a class="share-btn share-reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdeepakbaby.in%2fposts%2fdistributed-training%2f&title=Distributed%20Training%20Techniques%3a%20DDP%2c%20Pipeline%20Parallelism%2c%20and%20FSDP" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit-alien"></i>
</a><a class="share-btn share-whatsapp" href="https://api.whatsapp.com/send?text=Distributed%20Training%20Techniques%3a%20DDP%2c%20Pipeline%20Parallelism%2c%20and%20FSDP https%3a%2f%2fdeepakbaby.in%2fposts%2fdistributed-training%2f" target=_blank rel=noopener aria-label="Share on WhatsApp"><i class="fab fa-whatsapp"></i>
</a><a class="share-btn share-email" href="mailto:?subject=Distributed%20Training%20Techniques%3a%20DDP%2c%20Pipeline%20Parallelism%2c%20and%20FSDP&body=https%3a%2f%2fdeepakbaby.in%2fposts%2fdistributed-training%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></div><div class=improve-page-section><a href=https://github.com/deepakbaby/deepakbaby-site/edit/main/content/posts/distributed-training/index.md class=improve-link target=_blank rel=noopener><i class="fas fa-code-branch"></i>
<span>Improve this page</span></a></div><nav class=post-navigation><a href=/posts/nice-keras/ class="nav-card nav-prev"><span class=nav-label>← Previous</span>
<span class=nav-title>NICE- Non-linear Independent Components Estimation: Insights and Implementation in Keras</span>
</a><a href=/posts/distributed-training-presentation/ class="nav-card nav-next"><span class=nav-label>Next →</span>
<span class=nav-title>Slides: Distributed Training for ML</span></a></nav><div class=comments-section><h2 class=comments-heading><i class="fas fa-comments"></i>
Discussion</h2><div class=comments-content><div id=comments-container><script src=https://utteranc.es/client.js repo=deepakbaby/deepakbaby-site issue-term=title theme=github-light crossorigin=anonymous async></script></div></div></div></article></div></div><button id=scroll-to-top class=scroll-top-btn aria-label="Scroll to top">
<i class="fas fa-chevron-up"></i></button></div><style>@import 'https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap';.modern-post-wrapper{font-family:inter,-apple-system,BlinkMacSystemFont,sans-serif;background:var(--bg-primary);color:var(--text-primary);min-height:100vh;padding-top:5rem;width:100vw;max-width:100vw;overflow-x:hidden;margin:0;padding-left:0;padding-right:0}.post-hero-section{position:relative;min-height:60vh;display:flex;align-items:center;justify-content:center;text-align:center;padding:4rem 2rem;overflow:hidden;background:linear-gradient(135deg,#0f172a 0%,#1e293b 100%)}[data-theme=light] .post-hero-section{background:linear-gradient(135deg,#f8fafc 0%,#e2e8f0 100%)}.post-hero-image{position:absolute;inset:0;background-size:cover;background-position:50%;opacity:.15;z-index:0}[data-theme=light] .post-hero-image{opacity:.08}.post-hero-overlay{position:absolute;inset:0;background:radial-gradient(circle at 30% 50%,rgba(59,130,246,.1) 0%,transparent 50%),radial-gradient(circle at 70% 50%,rgba(139,92,246,.1) 0%,transparent 50%);z-index:1}.post-hero-content{position:relative;z-index:2;max-width:900px;margin:0 auto}.post-meta-badges{margin-bottom:1.5rem}.post-category-badge{display:inline-block;padding:.5rem 1.25rem;background:rgba(59,130,246,.15);border:1px solid rgba(59,130,246,.3);border-radius:50px;color:#60a5fa;font-size:.85rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em}[data-theme=light] .post-category-badge{background:rgba(59,130,246,.1);color:#3b82f6}.post-title{font-size:clamp(2rem,5vw,3.5rem);font-weight:400;line-height:1.1;margin:0 0 2rem;color:#fff;letter-spacing:-.02em}[data-theme=light] .post-title{color:#1e293b}.post-meta-info{display:flex;align-items:center;justify-content:center;gap:1.5rem;flex-wrap:wrap;margin-bottom:1.5rem;color:rgba(255,255,255,.8);font-size:.95rem}[data-theme=light] .post-meta-info{color:#64748b}.post-author{display:flex;align-items:center;gap:.75rem}.author-name{font-weight:600}.post-date-time{display:flex;align-items:center;gap:.5rem;color:rgba(255,255,255,.8)}[data-theme=light] .post-date-time{color:rgba(0,0,0,.7)}.post-date-time svg{width:16px;height:16px;opacity:.7}.meta-separator{opacity:.5}.post-tags-hero{display:flex;align-items:center;justify-content:center;gap:.75rem;flex-wrap:wrap}.tag-link{padding:.375rem 1rem;background:rgba(255,255,255,5%);border:1px solid rgba(255,255,255,.1);border-radius:50px;color:rgba(255,255,255,.8);font-size:.85rem;font-weight:500;text-decoration:none;transition:all .3s}.tag-link:hover{background:rgba(59,130,246,.2);border-color:rgba(59,130,246,.4);color:#60a5fa}[data-theme=light] .tag-link{background:rgba(0,0,0,4%);border-color:rgba(0,0,0,8%);color:#64748b}[data-theme=light] .tag-link:hover{background:rgba(59,130,246,.1);border-color:rgba(59,130,246,.3);color:#3b82f6}.post-content-container{max-width:1400px;margin:0 auto;padding:4rem 2rem}.post-content-grid{display:grid;grid-template-columns:1fr;gap:4rem;align-items:start;width:90%;margin:0 auto}.post-article-content{min-width:0}.prose-content{font-family:inter,-apple-system,BlinkMacSystemFont,segoe ui,system-ui,sans-serif;line-height:1.8;font-size:1.0625rem;font-weight:400}.prose-content h1,.prose-content h2,.prose-content h3,.prose-content h4,.prose-content h5,.prose-content h6{font-family:inter,-apple-system,BlinkMacSystemFont,segoe ui,system-ui,sans-serif!important;font-weight:400!important;line-height:1.4!important;margin-top:2.5rem!important;margin-bottom:1rem!important;color:var(--text-primary)!important;letter-spacing:-.01em!important;border:none!important;border-bottom:none!important;border-top:none!important;text-decoration:none!important;box-shadow:none!important;pointer-events:none!important;cursor:text!important}.prose-content h1::before,.prose-content h2::before,.prose-content h3::before,.prose-content h4::before,.prose-content h5::before,.prose-content h6::before,.prose-content h1::after,.prose-content h2::after,.prose-content h3::after,.prose-content h4::after,.prose-content h5::after,.prose-content h6::after{content:none!important;border:none!important;border-bottom:none!important;text-decoration:none!important}.prose-content h1:hover,.prose-content h2:hover,.prose-content h3:hover,.prose-content h4:hover,.prose-content h5:hover,.prose-content h6:hover,.prose-content h1:target,.prose-content h2:target,.prose-content h3:target,.prose-content h4:target,.prose-content h5:target,.prose-content h6:target,.prose-content h1[id],.prose-content h2[id],.prose-content h3[id],.prose-content h4[id],.prose-content h5[id],.prose-content h6[id]{text-decoration:none!important;border-bottom:none!important;cursor:text!important}.prose-content h1{font-size:2.25rem}.prose-content h2{font-size:1.875rem}.prose-content h3{font-size:1.5rem}.prose-content h4{font-size:1.25rem}.prose-content h5{font-size:1.125rem}.prose-content h6{font-size:1rem}.prose-content p{margin-bottom:1.5rem;color:var(--text-primary)}.prose-content a{color:#3b82f6;text-decoration:underline;text-underline-offset:3px;transition:color .2s}.prose-content a:hover{color:#60a5fa}.prose-content ul,.prose-content ol{margin-bottom:1.5rem;padding-left:1.75rem}.prose-content li{margin-bottom:.5rem}.prose-content blockquote{border-left:4px solid #3b82f6;padding-left:1.5rem;margin:1.5rem 0;font-style:italic;color:var(--text-secondary)}.prose-content code{background:rgba(59,130,246,.1);padding:.2em .4em;border-radius:4px;font-size:.9em;font-family:courier new,monospace;color:#3b82f6}[data-theme=light] .prose-content code{background:rgba(59,130,246,8%)}.prose-content pre{background:rgba(0,0,0,.3);padding:1.5rem;border-radius:12px;overflow-x:auto;margin:1.5rem 0;border:1px solid rgba(255,255,255,.1)}[data-theme=light] .prose-content pre{background:#f8fafc;border-color:rgba(0,0,0,8%)}.prose-content pre code{background:0 0;padding:0;color:inherit}.prose-content img{max-width:100%;height:auto;border-radius:12px;margin:2rem 0}.prose-content table{width:100%;border-collapse:collapse;margin:1.5rem 0}.prose-content th,.prose-content td{border:1px solid rgba(255,255,255,.1);padding:.75rem;text-align:left}[data-theme=light] .prose-content th,[data-theme=light] .prose-content td{border-color:rgba(0,0,0,8%)}.prose-content th{background:rgba(59,130,246,.1);font-weight:600}.prose-content hr{border:none;border-top:1px solid rgba(255,255,255,.1);margin:3rem 0}[data-theme=light] .prose-content hr{border-top-color:rgba(0,0,0,8%)}.prose-content .katex{font-size:1.1em}.prose-content .katex-display{overflow-x:auto;overflow-y:hidden;padding:1rem 0}.share-section{margin-top:4rem;padding-top:2rem;border-top:1px solid rgba(255,255,255,.1)}[data-theme=light] .share-section{border-top-color:rgba(0,0,0,8%)}.share-title{font-size:1.125rem;font-weight:600;margin-bottom:1rem;color:var(--text-primary)}.share-buttons{display:flex;gap:.75rem;flex-wrap:wrap}.share-btn{display:flex;align-items:center;justify-content:center;width:44px;height:44px;border-radius:50%;background:rgba(255,255,255,5%);border:1px solid rgba(255,255,255,.1);color:var(--text-secondary);text-decoration:none;transition:all .3s}.share-btn:hover{transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.2)}.share-facebook:hover{background:#1877f2;border-color:#1877f2;color:#fff}.share-twitter:hover{background:#1da1f2;border-color:#1da1f2;color:#fff}.share-linkedin:hover{background:#0a66c2;border-color:#0a66c2;color:#fff}.share-reddit:hover{background:#ff4500;border-color:#ff4500;color:#fff}.share-whatsapp:hover{background:#25d366;border-color:#25d366;color:#fff}.share-email:hover{background:#6b7280;border-color:#6b7280;color:#fff}.improve-page-section{margin-top:3rem;text-align:center}.improve-link{display:inline-flex;align-items:center;gap:.5rem;padding:.75rem 1.5rem;background:rgba(255,255,255,5%);border:1px solid rgba(255,255,255,.1);border-radius:50px;color:var(--text-secondary);text-decoration:none;font-weight:500;transition:all .3s}.improve-link:hover{background:rgba(59,130,246,.1);border-color:rgba(59,130,246,.3);color:#3b82f6}.post-navigation{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:1.5rem;margin-top:4rem}.nav-card{padding:1.5rem;background:rgba(255,255,255,3%);border:1px solid rgba(255,255,255,8%);border-radius:12px;text-decoration:none;transition:all .3s;display:flex;flex-direction:column;gap:.5rem}[data-theme=light] .nav-card{background:rgba(255,255,255,.6);border-color:rgba(0,0,0,8%)}.nav-card:hover{border-color:rgba(59,130,246,.3);transform:translateY(-2px);box-shadow:0 8px 20px rgba(59,130,246,.1)}.nav-label{font-size:.875rem;color:var(--text-muted);font-weight:500}.nav-title{font-size:1.125rem;font-weight:600;color:var(--text-primary)}.comments-section{margin-top:5rem;padding-top:3rem;border-top:2px solid rgba(255,255,255,.1)}[data-theme=light] .comments-section{border-top-color:rgba(0,0,0,8%)}.comments-heading{font-size:1.75rem;font-weight:700;margin-bottom:2rem;color:var(--text-primary);display:flex;align-items:center;gap:.75rem}.comments-heading i{color:#3b82f6}.scroll-top-btn{position:fixed;bottom:2rem;right:2rem;width:50px;height:50px;border-radius:50%;background:rgba(59,130,246,.9);border:none;color:#fff;font-size:1.25rem;cursor:pointer;opacity:0;visibility:hidden;transition:all .3s;box-shadow:0 4px 12px rgba(59,130,246,.3);z-index:100}.scroll-top-btn.visible{opacity:1;visibility:visible}.scroll-top-btn:hover{background:#3b82f6;transform:translateY(-4px);box-shadow:0 8px 20px rgba(59,130,246,.4)}@media(max-width:768px){.modern-post-wrapper{padding-top:4rem}.post-hero-section{min-height:50vh;padding:3rem 1.5rem}.post-title{font-size:2rem}.post-meta-info{flex-direction:column;gap:1rem}.post-content-container{padding:2rem 1.5rem}.post-content-grid{width:90%}.prose-content{font-size:1rem}.prose-content h1{font-size:1.875rem}.prose-content h2{font-size:1.5rem}.prose-content h3{font-size:1.25rem}.scroll-top-btn{bottom:1.5rem;right:1.5rem;width:44px;height:44px}}</style><script>(function(){"use strict";const e=document.getElementById("scroll-to-top");window.addEventListener("scroll",()=>{window.pageYOffset>300?e.classList.add("visible"):e.classList.remove("visible")}),e.addEventListener("click",()=>{window.scrollTo({top:0,behavior:"smooth"})})})()</script></div><footer class=modern-footer><div class=footer-glow></div><div class=footer-container><div class=footer-content><div class=footer-brand><span class=brand-name>Deepak Baby</span><p class=brand-tagline>Senior Data Scientist at KBC Bank & Verzekering</p></div><div class=footer-social><a href=mailto:deepakbabycet@gmail.com class=social-icon aria-label=Email title=Email><i class="fas fa-envelope"></i>
</a><a href=https://www.github.com/deepakbaby class=social-icon target=_blank rel="noopener noreferrer" aria-label=Github title=Github><i class="fab fa-github"></i>
</a><a href=https://www.linkedin.com/in/deepakbaby/ class=social-icon target=_blank rel="noopener noreferrer" aria-label=LinkedIn title=LinkedIn><i class="fab fa-linkedin"></i>
</a><a href="https://scholar.google.com/citations?user=69q7FOYAAAAJ&amp;hl=en" class=social-icon target=_blank rel="noopener noreferrer" aria-label="Google Scholar" title="Google Scholar"><i class="fa-brands fa-google-scholar"></i></a></div><div class=footer-links><a href=#hero>Home</a>
<a href=#about>About</a>
<a href=#experiences>Experience</a>
<a href=#posts>Blog</a>
<a href=#publications>Publications</a></div></div><div class=footer-bottom><p class=copyright>© 2025 Deepak Baby. All rights reserved.</p><p class=credits>Made with <span class=heart>❤</span>, Hugo and Claude</p></div></div></footer><style>.modern-footer{position:relative;background:#050506;padding:4rem 0 2rem;overflow:hidden}.footer-glow{position:absolute;top:0;left:50%;transform:translateX(-50%);width:600px;height:2px;background:linear-gradient(90deg,transparent 0%,#3b82f6 25%,#1d4ed8 50%,#3b82f6 75%,transparent 100%);filter:blur(1px)}.footer-container{max-width:1200px;margin:0 auto;padding:0 2rem}.footer-content{display:flex;flex-direction:column;align-items:center;text-align:center;gap:2rem;margin-bottom:3rem}.footer-brand{margin-bottom:.5rem}.brand-name{font-size:1.5rem;font-weight:700;background:linear-gradient(135deg,#fff 0%,#a78bfa 100%);-webkit-background-clip:text;-webkit-text-fill-color:transparent;background-clip:text}.brand-tagline{font-size:.9rem;color:#6b7280;margin:.5rem 0 0}.footer-social{display:flex;gap:1rem}.social-icon{width:44px;height:44px;border-radius:50%;background:rgba(255,255,255,5%);border:1px solid rgba(255,255,255,8%);display:flex;align-items:center;justify-content:center;color:#9ca3af;font-size:1.1rem;text-decoration:none;transition:all .3s ease}.social-icon:hover{background:linear-gradient(135deg,#8b5cf6 0%,#3b82f6 100%);border-color:transparent;color:#fff;transform:translateY(-4px);box-shadow:0 8px 25px rgba(139,92,246,.3)}.footer-links{display:flex;flex-wrap:wrap;justify-content:center;gap:1.5rem}.footer-links a{color:#6b7280;text-decoration:none;font-size:.9rem;transition:color .3s}.footer-links a:hover{color:#a78bfa}.footer-bottom{border-top:1px solid rgba(255,255,255,5%);padding-top:2rem;display:flex;flex-direction:column;align-items:center;gap:.5rem}.copyright{font-size:.875rem;color:#4b5563;margin:0}.credits{font-size:.75rem;color:#374151;margin:0}.heart{color:#ef4444;animation:heartbeat 1.5s infinite}@keyframes heartbeat{0%,100%{transform:scale(1)}50%{transform:scale(1.2)}}@media(min-width:768px){.footer-content{flex-direction:row;justify-content:space-between;text-align:left}.footer-bottom{flex-direction:row;justify-content:space-between}}</style><script src=/application.f6959e0d840b2a0cb8ebdc7b64f82c76ed540bafda2e2f31ece1d91e812ba4d1.js integrity="sha256-9pWeDYQLKgy469x7ZPgsdu1UC6/aLi8x7OHZHoErpNE=" defer></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1})'></script><script src=/js/collapsible-sidebars.js></script></body></html>