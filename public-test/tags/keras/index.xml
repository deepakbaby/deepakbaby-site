<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Keras on Deepak Baby</title><link>https://deepakbaby.in/tags/keras/</link><description>Recent content in Keras on Deepak Baby</description><generator>Hugo</generator><language>en</language><lastBuildDate>Tue, 03 Dec 2019 12:55:51 +0100</lastBuildDate><atom:link href="https://deepakbaby.in/tags/keras/index.xml" rel="self" type="application/rss+xml"/><item><title>NICE- Non-linear Independent Components Estimation: Insights and Implementation in Keras</title><link>https://deepakbaby.in/posts/nice-keras/</link><pubDate>Tue, 03 Dec 2019 12:55:51 +0100</pubDate><guid>https://deepakbaby.in/posts/nice-keras/</guid><description>&lt;p>Keras implementation can be found &lt;a href="https://deepakbaby.github.io/post/nice-keras/">here&lt;/a>.&lt;/p>
&lt;p>Flow-based deep generative models have not gained much attention in the research community when compared to &lt;a href="https://arxiv.org/abs/1406.2661">GANs&lt;/a> or &lt;a href="https://arxiv.org/abs/1312.6114">VAEs&lt;/a>. This post discusses a flow-based model called &lt;a href="https://arxiv.org/abs/1410.8516">NICE&lt;/a>, its advantages over the other generative models and finally an implementation in Keras.&lt;/p>
&lt;p>While VAEs use an encoder that finds only an approximation of the latent variable corresponding to a datapoint, GANs doesnt even have an encoder to infer latents. In flow-based models, the latent variables can be infered exactly without any approximation. Flow-based models make use of reversible architecture (which will be explained below) which enables accurate inference, in addition to providing optimization over the exact log-likelihood of the data instead of a lower bound of it.&lt;/p></description></item><item><title>Understanding Variational Autoencoders and Implementation in Keras</title><link>https://deepakbaby.in/posts/vae-keras/</link><pubDate>Tue, 02 Jul 2019 16:44:25 +0200</pubDate><guid>https://deepakbaby.in/posts/vae-keras/</guid><description>&lt;p>Variational Autoencoders (VAEs)&lt;a href="https://arxiv.org/abs/1312.6114">[Kingma, et.al (2013)]&lt;/a> let us design complex generative models of data that can be trained on large datasets. This post is about understanding the VAE concepts, its loss functions and how we can implement it in keras.&lt;/p>
&lt;h2 id="generating-data-from-a-latent-space">Generating data from a latent space&lt;/h2>
&lt;p>VAEs, in terms of probabilistic terms, assume that the data-points in a large dataset are generated from a latent space. For e.g., let us assume we want to generate the image of an animal. First we imagine that it has four legs, a head and a tail. This is analogous to the latent space and from this set of characteristics that are defined in the latent space, the model will learn to generate the image of an animal.&lt;/p></description></item><item><title>Tracking Multiple Losses with Keras</title><link>https://deepakbaby.in/posts/keras-multiple-losses/</link><pubDate>Mon, 04 Mar 2019 11:59:56 +0100</pubDate><guid>https://deepakbaby.in/posts/keras-multiple-losses/</guid><description>&lt;p>Often we deal with networks that are optimized for multiple losses (e.g., VAE). In such scenarios, it is useful to keep track of each loss independently, for fine-tuning its contribution to the overall loss. This post details an example on how to do this with keras.&lt;/p>
&lt;p>Let us look at an example model which needs to trained to minimize the sum of two losses, say mean square error (MSE) and mean absolute error (MAE). Let $\lambda_{mse}$ be the hyperparameter that controls the contribution of MSE to the toal loss. i.e., the total loss is MAE + $\lambda_{mse}$ * MSE. This loss can be implemented using:&lt;/p></description></item></channel></rss>