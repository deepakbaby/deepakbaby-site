<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Presentation on Deepak Baby</title><link>https://deepakbaby.in/tags/presentation/</link><description>Recent content in Presentation on Deepak Baby</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 03 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deepakbaby.in/tags/presentation/index.xml" rel="self" type="application/rss+xml"/><item><title>Slides: Distributed Training for ML</title><link>https://deepakbaby.in/posts/distributed-training-presentation/</link><pubDate>Wed, 03 Dec 2025 00:00:00 +0000</pubDate><guid>https://deepakbaby.in/posts/distributed-training-presentation/</guid><description>&lt;p>Explore distributed training techniques through this interactive presentation. Navigate through the slides using arrow keys or the navigation controls.&lt;/p>
&lt;h2 id="topics-covered">Topics Covered&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Back to Basics&lt;/strong>: Understanding neural network fundamentals&lt;/li>
&lt;li>&lt;strong>Why Distributed Training&lt;/strong>: Memory constraints and scaling challenges&lt;/li>
&lt;li>&lt;strong>DDP (Data Distributed Parallel)&lt;/strong>: Replicating models across GPUs&lt;/li>
&lt;li>&lt;strong>Pipeline Parallelism&lt;/strong>: Splitting models across devices&lt;/li>
&lt;li>&lt;strong>FSDP (Fully Sharded Data Parallel)&lt;/strong>: Advanced sharding techniques&lt;/li>
&lt;/ul>
&lt;h2 id="slides">Slides&lt;/h2>
&lt;p>Use the arrow keys (← →) or click the navigation arrows to move between slides. Some slides include animations that you can step through using the animation controls at the bottom.&lt;/p></description></item></channel></rss>