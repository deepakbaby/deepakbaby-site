<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on Deepak Baby</title><link>https://deepakbaby.in/tags/deep-learning/</link><description>Recent content in Deep Learning on Deepak Baby</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 03 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deepakbaby.in/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Slides: Distributed Training for ML</title><link>https://deepakbaby.in/posts/distributed-training-presentation/</link><pubDate>Wed, 03 Dec 2025 00:00:00 +0000</pubDate><guid>https://deepakbaby.in/posts/distributed-training-presentation/</guid><description>&lt;p>Explore distributed training techniques through this interactive presentation. Navigate through the slides using arrow keys or the navigation controls.&lt;/p>
&lt;h2 id="topics-covered">Topics Covered&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Back to Basics&lt;/strong>: Understanding neural network fundamentals&lt;/li>
&lt;li>&lt;strong>Why Distributed Training&lt;/strong>: Memory constraints and scaling challenges&lt;/li>
&lt;li>&lt;strong>DDP (Data Distributed Parallel)&lt;/strong>: Replicating models across GPUs&lt;/li>
&lt;li>&lt;strong>Pipeline Parallelism&lt;/strong>: Splitting models across devices&lt;/li>
&lt;li>&lt;strong>FSDP (Fully Sharded Data Parallel)&lt;/strong>: Advanced sharding techniques&lt;/li>
&lt;/ul>
&lt;h2 id="slides">Slides&lt;/h2>
&lt;p>Use the arrow keys (← →) or click the navigation arrows to move between slides. Some slides include animations that you can step through using the animation controls at the bottom.&lt;/p></description></item><item><title>Distributed Training Techniques: DDP, Pipeline Parallelism, and FSDP</title><link>https://deepakbaby.in/posts/distributed-training/</link><pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate><guid>https://deepakbaby.in/posts/distributed-training/</guid><description>&lt;p>Modern deep learning models have grown exponentially in size and complexity. GPT-4 has over a trillion parameters, and even &amp;ldquo;smaller&amp;rdquo; models like LLaMA-70B require substantial computational resources. Training or fine-tuning such models on a single GPU is often impossible; not just because of time constraints, but because the model itself may not fit in the memory of a single device. This is where &lt;strong>distributed training&lt;/strong> becomes essential.&lt;/p>
&lt;h2 id="why-do-we-need-distributed-training">Why Do We Need Distributed Training?&lt;/h2>
&lt;h3 id="the-memory-wall-problem">The Memory Wall Problem&lt;/h3>
&lt;p>A modern GPU like the NVIDIA A100 has 80GB of memory. Sounds like a lot? Let&amp;rsquo;s do some math:&lt;/p></description></item><item><title>NICE- Non-linear Independent Components Estimation: Insights and Implementation in Keras</title><link>https://deepakbaby.in/posts/nice-keras/</link><pubDate>Tue, 03 Dec 2019 12:55:51 +0100</pubDate><guid>https://deepakbaby.in/posts/nice-keras/</guid><description>&lt;p>Keras implementation can be found &lt;a href="https://deepakbaby.github.io/post/nice-keras/">here&lt;/a>.&lt;/p>
&lt;p>Flow-based deep generative models have not gained much attention in the research community when compared to &lt;a href="https://arxiv.org/abs/1406.2661">GANs&lt;/a> or &lt;a href="https://arxiv.org/abs/1312.6114">VAEs&lt;/a>. This post discusses a flow-based model called &lt;a href="https://arxiv.org/abs/1410.8516">NICE&lt;/a>, its advantages over the other generative models and finally an implementation in Keras.&lt;/p>
&lt;p>While VAEs use an encoder that finds only an approximation of the latent variable corresponding to a datapoint, GANs doesnt even have an encoder to infer latents. In flow-based models, the latent variables can be infered exactly without any approximation. Flow-based models make use of reversible architecture (which will be explained below) which enables accurate inference, in addition to providing optimization over the exact log-likelihood of the data instead of a lower bound of it.&lt;/p></description></item><item><title>Understanding Variational Autoencoders and Implementation in Keras</title><link>https://deepakbaby.in/posts/vae-keras/</link><pubDate>Tue, 02 Jul 2019 16:44:25 +0200</pubDate><guid>https://deepakbaby.in/posts/vae-keras/</guid><description>&lt;p>Variational Autoencoders (VAEs)&lt;a href="https://arxiv.org/abs/1312.6114">[Kingma, et.al (2013)]&lt;/a> let us design complex generative models of data that can be trained on large datasets. This post is about understanding the VAE concepts, its loss functions and how we can implement it in keras.&lt;/p>
&lt;h2 id="generating-data-from-a-latent-space">Generating data from a latent space&lt;/h2>
&lt;p>VAEs, in terms of probabilistic terms, assume that the data-points in a large dataset are generated from a latent space. For e.g., let us assume we want to generate the image of an animal. First we imagine that it has four legs, a head and a tail. This is analogous to the latent space and from this set of characteristics that are defined in the latent space, the model will learn to generate the image of an animal.&lt;/p></description></item><item><title>Tracking Multiple Losses with Keras</title><link>https://deepakbaby.in/posts/keras-multiple-losses/</link><pubDate>Mon, 04 Mar 2019 11:59:56 +0100</pubDate><guid>https://deepakbaby.in/posts/keras-multiple-losses/</guid><description>&lt;p>Often we deal with networks that are optimized for multiple losses (e.g., VAE). In such scenarios, it is useful to keep track of each loss independently, for fine-tuning its contribution to the overall loss. This post details an example on how to do this with keras.&lt;/p>
&lt;p>Let us look at an example model which needs to trained to minimize the sum of two losses, say mean square error (MSE) and mean absolute error (MAE). Let $\lambda_{mse}$ be the hyperparameter that controls the contribution of MSE to the toal loss. i.e., the total loss is MAE + $\lambda_{mse}$ * MSE. This loss can be implemented using:&lt;/p></description></item></channel></rss>